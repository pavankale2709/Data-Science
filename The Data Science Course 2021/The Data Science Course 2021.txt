-Statistics -> Data Mining -> Predictive Analysis -> Data Science

-Analysis
	-Analyses on things that have already happened in the past. Ee do analyses to explain how and or why something happened.
	-Types:
		-Qualitative Analysis: Explain how and why something happened
		-Quantitative Analysis: data + how sales decreases last summer
-Analytics
	-Analytics is essentially the application of logical and computational reasoning to the component parts obtained in an analysis. And in doing this, you are looking for patterns and exploring what you could do with them in the future.
	-Types:
		-Qualitative Analytics (intuition + analysis) - Based on experience, market knowledge
		-Quantitative Analytics (formulas + algorithms)
-Business Analysis include: Things that has been already happened
	-Qualitative
		-Business case studies
	-Quantitative
		-Preliminary data report
		-Reporting with visuals
		-Creating dashboards
-Business Analytics includes: Things that will happen in future based on analysis
	-Ideally
		-Qualitative
			-Qualitative analytics
		-Quantitative
			-Sales forecasting
	-Actually according to industry this include
		-Business case studies
		-Preliminary data report
		-Reporting with visuals
		-Creating dashboards
		-Qualitative analytics
		-Sales forecasting
-Data Analysis includes: Past data analysis
	-Preliminary data report
	-Reporting with visuals
	-Creating dashboards
-Data analytics include: Data processing in past or in future
	-Ideally
		-Sales forecasting
	-Actually according to industry this include
		-Preliminary data report
		-Reporting with visuals
		-Creating dashboards
		-Sales forecasting
-Data Science
	-This include:
		-Preliminary data report
		-Reporting with visuals
		-Creating dashboards
		-Sales forecasting
	-Data science is a discipline reliant on data availability, while business analytics does not completely rely on data.
	-Data science incorporates part of data analytics, mostly the part that uses complex mathematical, statistical and programming tools.
	-Data science can be used to improve the accuracy of predictions based on data extracted from various activities typical for drilling efficiency. And that certainly isn't business analytics.
-What is left is to think of something that involves data analytics, but neither data science nor business analytics, can we find an example here as well?Digital signal is used to represent data in the form of discrete values, which is an example of numeric data. Therefore, data analytics can be applied to digital signal in order to produce a higher quality signal. And that's what digital signal processing is all about.
-Business Intelligence
	-This include
		-Reporting with visuals
		-Creating dashboards
	-BI is the process of analyzing and reporting historical business data. 
	-After reports and dashboards have been prepared, they can be used to make informed strategic and tactical business decisions by end users such as the general manager. Concisely put, business intelligence aims to explain past events using business data.
	-Concisely put, business intelligence aims to explain past events using business data.
-Machine learning
	-The ability of machines to predict outcomes without being explicitly programmed to do so is regarded as machine learning.
	-Creating and implementing algorithms that let machines receive data and use this data to make predictions, analyze patterns and give recommendations on their own.
	-This include:
		-Creating Real-time dashboards
		-Client Retention
		-Fraud Protection
		-Speech Recognition
		-Image Recognition
-Artificial Intelligence
	-It is about simulating human knowledge and decision making with computers.
	-This include:
		-Symbolic Reasoning
		-Creating Real-time dashboards
		-Client Retention
		-Fraud Protection
		-Speech Recognition
		-Image Recognition
-Advanced Analytics
	-Include above all analysis and analytics


-Data
	-Data is defined as information stored in a digital format, which can then be used as a base for performing analyses and decision making.
	-Data types
		-Traditional data
			-Data in the form of tables containing numeric or text values, data that is structured and stored in databases which can be managed from one computer.
			-Traditional data types:
				-Raw data (raw facts/primary data): raw data -> processing -> information 
					-cannot analyzed strainght way
					-untouched data you have accumulated and stored on server
					-Data collection
						-Serveys
						-Cookies
				-Processed data
				-Information
			-Data preprocessing
				-This is a group of operations that will basically convert your raw data into a format that is more understandable and hence useful for further processing.
				-It attempts to fix the problems that can inevitably occur with data gathering. Mark the incorrect entries as invalid or correct them.
				-Raw data pre-processing techniques:
					-Class labelling
						-Involves labeling the data point to the correct data type or arranging data by category (numerical, categorical).
					-Data Cleansing (Data cleaning or data scrubbing)
						-The goal of data cleansing is to deal with inconsistent data (mis-spelled data, missing values).
				-Balancing
				-Data Shuffling: shuffling is a way to randomize data
					-prevents unwanted patterns
					-improves predictive performance
					-helps avoid misleading results
			-Visualization techniques
				-Entity Relationship diagram
				-Relational Schema
			-Example
				-Basic Customer data
				-Historical stock price data
		-Big data
			-Extremely large data, and it is not just humongous in terms of volume. This data could be in various formats. It can be structured, sami-structure or unstructured.
			-11 V's of big data
				-vision you have about big data
				-value big data carries
				-visualization tools you use
				-variability in the consistency of big data
			-Its size as measured in terabytes, petabytes and even exabytes.
			-Variety here, we are not talking just about numbers and text, big data often implies dealing with images, audio files, mobile data and others.
			-Velocity, when working with big data, one's goal is to make extracting patterns from it as quickly as possible.
			-Examples of big data can be text data, digital image data, digital video data, digital audio data and more.
			-Text data mining
				-Text data mining represents the process of deriving valuable, unstructured data from a text.
			-Data masking
				-Must preserve confidential information.
				-Analyze the information without compromising private details like data shuffling.
				-Confidentiality preserving data mining techniques
	
-Data Science
	-Tools involved:
		-statistical
		-mathematical
		-programming
		-problem-solving
		-data-management
	-Segments of data science
		-Business intelligence
			-BI includes technology driven tools involved in the process of analyzing, understanding and reporting available past data.
			-This will result in you having reports or dashboards and will help you on your way to making informed strategic and tactical business decisions.
			-This part of the process is worth your time, you can extract insights and ideas about your business that will help it grow and give you an edge of your competitors giving you added stability.
			-Business intelligence means understanding how your cells grew and why? Did competitors lose market share? Was there an increase in the price of your products or did you sell a mix of more expensive products? How did your profitability margins behave in the same time frame of a previous year? Were their client accounts that were more profitable?
			-Understanding past business performance in order to improve future performance.
			-How do we measure business performance?
				-Collecting observations
					-monthly revenue
					-sales volume
					-customers
			-Quantification is the process of representing observations as numbers.
			-Measure is the accumulation of observations to show some information.
			-A metric refers to a value that derives from the measures you have obtained and aims at gauging business performance or progress.
				-To compare if a measure is related to something like simple descriptive statistics of past performance, a metric has a business meaning attached.
			-In a nutshell, your observations lead to measures and your measures are used to create your metrics. Your metrics have business applications.
			-Key Performance Indicators (KPIs)
				-KPIs are the metrics that are tightly aligned with your business objectives.
				-Key because they are related to your main business goals.
				-Performance because they show how successfully you have performed within a specified time frame.
				-Indicators, because they are values or metrics that indicate something related to your business performance
			-General managers will want dashboards and reports with graphs, diagrams, maps and other easily digestible visuals supported by the most relevant numbers, filtering out the boring metrics and turning the interesting and informative KPIs into easily understood and comparable visualizations is an important part of the business intelligence analysts job.
			-Example:	
				-Price Optimization
				-Inventory Management
		-Predictive Analysis		
			-Traditional methods
				-Traditional methods, according to our framework, are set of methods that are derived mainly from statistics and are adapted for business. There is no denying that these conventional data science tools are absolutely applicable today, they are perfect for forecasting future performance with great accuracy, to give you an idea.
				-Here are some instances, regression analysis, cluster analysis and factor analysis, all of which are prime examples of traditional methods.
				-Traditional data science methods:
					-Regression (Linear)
						-In business and statistics, a regression is a model used for quantifying causal relationships among the different variables included in your analysis.
					-Logistic regression
						-In logistic regression, the values on the vertical line won't be arbitrary integers. They'll be one's or zero's only. Such a model is useful during the decision making process.
						-Companies apply logistic regression algorithms to filter job candidates during their screening process. If the algorithm estimates the probability that a prospective candidate will perform well and the company is above 50 percent, it would predict one or a successful application. Otherwise, it will predict zero.
					-Cluster analysis
						-When the data is divided into a few groups called clusters, you can apply cluster analysis.
					-Factor analysis
						-Clustering is about grouping observations together and fact analysis is about grouping explanatory variables together.
					-Time-series analysis
				-Example:
					-User experience
					-Sales forecasting
			-Machine learning
				-In contrast to traditional methods, the responsibility is left for the machine through mathematics, a significant amount of computer power and applying A.I.
				-The machine is given the ability to predict outcomes from data without being explicitly programmed to.
				-ML is all about creating algorithms that let machines receive data, perform calculations and apply statistical analysis in order to make predictions with unprecedented accuracy.
				-The core of machine learning is, creating an algorithm which a computer then uses to find a model that fits the data as best as possible and makes very accurate predictions based on that. 
				-We don't give the machine instructions on how to find that model. We provide it with algorithms which give the machine directions on how to learn on its own.
				-A machine learning algorithm is like a trial and error process. But the special thing about it is that each consecutive trial is at least as good as the previous one. There are four ingredients: Data, Model, objective function and optimization algorithm.
				-Machine learning is not about robots, what people use it for is to improve complex computational models that can find infinite applications in our daily lives, especially in the business world. And that complex computational models we are talking about step on the fundaments of regressions and cluster analyses models.
				-Types of ML
					-Supervised learning
						-Its name derives from the fact that training an algorithm resembles a teacher supervising her students.
						-Here we have labelled data.
						-Approaches:
							-Support Vector Machines (SVM)
							-Neural Networks (NNs)
							-Deep learning
							-Random forests
							-Bayesian networks
					-Unsupervised learning
						-Here we have unlabelled data
						-Approaches:
							-k-means
							-Deep learning
					-Reinforcement learning
						-Similar to supervised learning, but instead of minimizing the loss, we maximizes the rewards.
						-In reinforcement learning, a reward system is being used to improve the machine learning model at hand. The idea of using this reward system is to: maximize the objective function.
						-Approaches
							-Deep learning
				-Example:
					-Fraud prevention
					-Client retention


-Programming languages and Softwares used in Data Science
	-Programming languages: 
		-Traditional Data: R, Python, SQL, MATLAB
		-Big data: R, Python, Java, Scala
		-Business Intelligence: R, Python, SQL, MATLAB
		-Traditional methods: R, Python, MATLAB
		-Machine Learning: R, Python, MATLAB, Java, JS, C, Scala, C++
		-R, Python
			-suitable for mathematical and statistical computations
			-they are adaptable
			-not able to address problem specific to some domains, Ex: RDMS
		-MATLAB
			-ideal for working with mathematical functions and matrix manipulations
		-Java / Scala
			-very useful when combining data from multiple sources
	-Softwares 
		-Traditional Data: Excel, IBM SPSS
		-Big data: Apache hadoop, Apache HBASE, MongoDB
		-Business Intelligence: Excel, Power BI, sas, QlikQ, ableau
		-Traditional methods: Excel, IBM SPSS, EViews (econometric time-series data), STATA (academic statistical and econometric research)
		-Machine Learning: Microsoft Azure, rapidminor
		-Excel
			-it is able to do relatively complex computations and good visualization quickly
		-IBM SPSS
			-for working with traditional data and applying statistical analysis

-SWOT analysis is a famous type of qualitative analysis contributing to the strategic decision making of a company. It points out the strengths and weaknesses of running a particular business so a SWOT analysis can improve the firm's strategy. But it is not a data driven analysis. 

----------------------------------------------------------------------------------------------------------------------------------------
-Probability

-Basic Probability Formula
	-Chance of something happening
	-The likelihood of an event occurring.
	-An event is a specific outcome or a combination of several outcomes.
	-When dealing with uncertain events, we are seldom satisfied by simply knowing whether an event is likely or unlikely.
	-Having a probability of one expresses absolute certainty of the event occurring and a probability of zero expresses absolute certainty of the event not occurring.
	-P(A) -> Probability of an event A
		-Theoretical probability, P(A) = Number of preferred Outcomes / Total number of possible outcomes
			-Preferred outcomes = outcomes that we want to see happen  = favourable
			-Total number of possible outcomes = sample space
	-The probability of two independent events occurring at the same time is equal to the product of all the probabilities of the individual events.
		-P(A and B) = P(A) * P(B)
-Computing Expected values
	-Expected values, E(A)
		-Expected value means the average outcome we expect if we run an experiment many times OR The outcome we expect to occur when we run an experiment.
	-Experimental probability
		-Trial = flip a coin and record outcome
		-Experiment = Multiple trials (toss coin 50 times and record 20 outcomes)
		-The probabilities we get after conducting experiments are called experimental probabilities, whereas the ones we introduced earlier were theoretical or true probabilities.
		-P(A) = successful trials / all trials
	-Expected value
		-The expected value of an event A, E(A), is the outcome we expect to occur when we run an experiment.
		-Expected value for categorical outcomes
				-E(A) = P(A) * n (number of trials)
		-Expected value for Numerical outcomes
			-E(A) = We take the value for every element in the sample space and multiply it by its probability, then we add all of those up to get the expected value.
			-E(A) = A * P(A) + B * P(B) + C * P(C)
	-We can use expected values to make predictions about the future based on past data. We frequently make predictions using intervals instead of specific values due to the uncertainty the future brings.
	-Meteorologists often use these when forecasting the weather, they do not know exactly how much snow, rain or wind there's going to be, so they provide us with likely intervals instead.
-Probability frequency distribution (PDF)
	-A probability frequency distribution is a collection of the probabilities for each possible outcome.
	-Ee can express this probability frequency distribution through a table or a graph.
	-If we want the interval with the highest probability, we should construct it around the expected value.
-Events and their complements
	-Complement of an event is Everything the event is not.
	-Complement of event A id denoted as A'.
	-(A')' = A
	-P(A') = 1 - P(A)

	

-Probability - Combinatorics

-Fundamentals of Combinatorics
	-Combinatorics deals with combinations of objects from a specific finite set.
	-We also need to consider certain restrictions (repetition, order, a different criterion) that can be applied to form combinations.
	-The three integral parts of combinatorics: permutations, variations and combinations.
-Permutations and How to use them
	-Permutations represents the number of different possible ways we can arrange a set of elements. These elements can be digits, letters, objects or even people.
	-How to compute the number of permutations for a finite set of any size and in different situations?
		-P(n) = n * (n-1) * (n-1) * ..... * 1 = n!
-Simple operations with Factorials
		-n! = the product of the natural numbers from 1 to n
		-n! = n * (n-1)!
		-(n+1)! = (n+1) * n! 
		-(n+k)! = n! * (n+1) * (n+2) * .... * (n+k)
		-(n-k)! = n! / ( (n-k+1) * (n-k+2) * ..... * n )
		-If n > k then n!/k! = (k+1) * (k+2) * .... * n
-Solving Variations with Repetition
	-Variations express the total number of ways we can pick and arrange some elements of a given set.
	-V(n, p) = n ^ p = The number of variations with repetition when picking up p-many elements out of n elements is equal to n to the power of P.
		-n = the total number of elements, we have available
		-p = the number of positions we need to fill
-Solving Variation without repetition
	-V(n, p) = n! / (n-p)! = The number of variations without repetition when arranging p elements out of a total of n is equal to n! / (n-p)!.
	-We use Variations when we have to first pick and then arrange some (but not all) elements of the sample space.
		-Variations don't take into account double couting of elements (same combination of elements in different order)
-Solving Combinations
	-Combinations represent the number of different ways we can pick certain elements of a set.
	-Variations don't take into account double couting of elements, that is where combinations come in.
	-All the different permutations of a single combination are different variations.
	-The number of combinations = the number of variations / the number of permutations
		-C(n, p) = V(n, p) / P(p) = n! / p! (n-p)!
-Symmetry of combinations
	-We can pick p-many elements in as many ways as we can pick (n - p) many elements. This shows us that when it comes to combinations, the number of possible ways in which p-many elements can be selected is symmetric with respect to n over two.
		-C(n, p) = C(n, n-p)
	-When P is greater than n/2, (n - p) would be smaller that p, in such instances, we can apply symmetry to avoid calculating factorial of large numbers. Generally, we use symmetry to simplify the calculations we need to make.
		-p > n/2 > (n-p)
-Solving Combinations with separate sample space
	-The way of calculating the total number of combinations for these kinds of questions (separate sample space for each event) is by simply multiplying the number of options available for each individual event (that is, number of elements in each sample space).
	-This allows project managers to determine the appropriate amount of time it would take for such (Combinations with separate sample space) a task to be completed.
-Combinatorics in real life: The Lottery
	-The likelihood of two independent events occurring simultaneously equals the product of their individual probabilities.
-Summary of Permutation, Variation, Combinations
	-We use "permutations with variations" when we must arrange a set of objects. In such cases, the order in which we pick them is crucial. 
	-The major difference between permutations and variations is that, in permutations, you always arrange the entire set of elements in the sample space, where as, in variations, you arrange some (p) element in the sample space (n).
	-If we want to only want to pick p elements out of n elements and we don't care about their arrangement then we are dealing with combinations. 
	-Without repetition
		-C = V / P = Variations / Permutations
		-This is because, we count all the permutations of a given set of numbers as a single combination, but as separate variations.
		-P(n) = n!
		-V(n, p) = n! / (n-p)!
		-C(n, p) = n! / p! (n-p)!
	-With repetition
		-V(n, p) = n ^ p
		-C(n, p) = (n+p-1)! / p! (n-1)!
	-Combinations are symmetric.
		-C(n, p) = C(n, (n-p))



-Probability - Bayesian Inference

-Sets and Events
	-A set is a collection of elements, which hold certain values. 
	-Every event has a set of outcomes (favourable outcomes) that satisfy it.
	-Set is denoted by uppercase letter and elements by lowercase.
	-Empty set = null set
	-Non-empty set can be of type: Finite, Infinite
	-∀x∈A:x is even -> for all x in A, such that, x is even
	-Subset: A set that is fully contained in another set
		-Every set contains at least 2 sets: itself and a null set
-Ways sets can interact
	-Independent: The two events can never happen simultaneously
	-Intersecting: The two events can occur at the same time.
	-Completely overlap/Sub-set: One event can only ever occur if the other one does as well.
-Intersection of Sets (A ∩ B)
	-All the outcomes that are favorable for both Event A and Event B simultaneously.
-Union of Sets (A U B)
	-The union of two sets is a combination of all outcomes preferred for either A or B.
	-A U B = A + B - (A ∩ B)
-Mutually exclusive sets
	-Mutually exclusive sets are sets which are not allowed to have any overlapping elements.
	-Mutually exclusive sets have the empty set as their intersection. Therefore, if the intersection of any number of sets is the empty set, then they must be mutually exclusive and vice versa.
	-If some sets are mutually exclusive, their union is simply the sum of all separate individual sets.
	-Complements:	
		-Sets have complements. Such compliments consists of all values that are parts of the sample space, but not part of the set.
		-The compliments is that they are always mutually exclusive. However, not all mutually exclusive sets are compliments.
-Dependence and Independence of sets
	-Independent events
		-Events whose theoretical probability remains unaffected by other events.
	-Dependent events
		-Events whose probabilities very as conditions change.
		-The probability of an event changes depending on the information we have.
	-P(A|B) - P of A given B: The probability of getting A if we are given that B has occurred - Conditionl Probability
		-Used to distinguish dependent events from independent events
-The Conditional probability Formula
	-The conditional probability is the likelihood of an event occurring, assuming a different one has already happened.
	-P(A|B) = Probability of getting A if we are given that B has occurred
	-For 2 independent events A and B 
		-P(A) = P(A|B)
		-P(A ∩ B) = P(A) * P(B)
	-For 2 dependent events A and B 
		-The conditional probability of an event A given an event B equals the probability of the intersection of A and B over the probability of event B occurring. This holds true if the probability of event B is greater than zero Only.
		-To satisfy the conditional probability, we need both events B and A to occur simultaneously. This suggests that the intersection of A and B would consist of all favorable outcomes for this probability. Secondly, the conditional probability requires that Event B occurs, so the sample space would simply be all outcomes where event B is satisfied.
		-P(A|B) = P(A ∩ B) / P(B)
		-P(A|B) != P(B|A)
-The Law of total probability
	-A = B1 ∪ B2 ∪ B3 ∪ ... ∪ Bn
		then P(A) = P(A|B1) * P(B1) + P(A|B2) * P(B2) + ...
-Additive law
	-A ∪ B = A + B - (A ∩ B)
	-The probability of the union of two sets is equal to the sum of the individual probabilities of each event, minus the probability of their intersection.
		-P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
-Multiplication rule
	-The probability of both events (A and B) happening equals the product of the likelihood of A occurring and the conditional probability that B occurs, given A has already occurred.
		-P(A ∩ B) = P(B|A) * P(A) = P(A|B) * P(B)
-Bayes' Law/Rule/Theorem
	-Conditional probability formula, P(A|B) = P(A ∩ B) / P(B)
	-Multiplication rule, P(A ∩ B) = P(B|A) * P(A), from these 2 equaltions
		-P(A|B) = P(B|A) * P(A) / P(B) -> Is called Bayes Theorem
	-Bayes theorem allows us to find a relationship between the different conditional probabilities of two events.
		


-Probability Distribution

-Fundamentals of probability distribution
	-A distribution shows the possible values a variable can take and how frequently they occur.
		-P(Y=y) = p(y) = The probability function
		-Y = Actual outcome of an event
		-y = One of the possible event
		-P(Y=y) = Likelihood of reaching particular outcome y 
	-Probability distributions or simply probabilities measure the likelihood of an outcome depending on how often it is featured in the sample space.
	-Regardless of whether we have a finite or infinite number of possibilities, we define distribution's using only two characteristics mean and variance.
	-The mean (µ) of the distribution is its average value.
	-Variance (σ2), is essentially how spread out the data is. 
		-We measure this spread by how far away from the mean all the values are. The more dispersed the data is, the higher its variance will be.
	-Standard deviation (σ) is simply the positive square root of variance.
	-Types of data
		-Population data: All the data
			-Mean (µ)
			-Variance (σ2)
			-SD (σ)
		-Sample data: Part of the data
			-Sample mean (x̄)
			-Sample Variance (s^2)
			-Sample SD (s)
	-A constant relationship exists between mean and variance for any distribution. By definition, the variance equals the expected value of the squared difference from the mean for any value.
		-σ2 = E((Y-µ)^2) = E(Y^2) - µ^2
-Types of probability distribution
	-X ~ N (µ, σ2) -> Varible -> Type of distribution (characteristics)
	-Discret distribution
		-Characteristics of discret distribution
			-Finitely many distinct outcomes, so we can express the entire probability distribution with either a table, a graph or a formula. To do so, we need to ensure that every unique outcome has a probability assigned to it.
			-The probability of the intervals is simply the total likelihood of any of the values within the interval occurring.
		-Types of Discret distribution
			-Uniform distribution (U) (Equiprobable)
				-X ~ U (a, b) -> Variable X follows a discrete uniform distribution ranging from a to b. 
				-Events which follow the uniform distribution are ones where all outcomes have equal probability.
				-One big drawback of uniform distributions is that the expected value provides us no relevant information because all outcomes have the same probability. Since the expected value does not bring any predictive power, we cannot construct an interval around it and be certain it will yield a high probability.
				-Each outcome is equally likely, so both the mean and the variance are uninterpretable and posses no predictive power
			-Bernoulli distribution (only 2 possible outcomes - True/False)
				-X ~ Bern(p) -> Variable X follows a Bernoulli distribution with a probability of success equal to p.
				-Any event where we only have one trial and two possible outcomes follows Bernoulli distribution.
				-Usually when dealing with a Bernoulli distribution, we either have the probabilities of one of the events occurring or have passed data indicating some experimental probability.
				-E(X) = p = The likelihood of favoured event
				-Variance, σ2 = p * (1-p)
			-Binomial distribution
				-X ~ B(n, p) -> Variable X follows a binomial distribution with n trials and a likelihood of p to succeed on each individual trial.
				-Binomial events are a sequence of identical Bernoulli events.
				-Just like the Bernoulli distribution, the outcomes for each iteration are two, but we have many iterations
				-A sequence of identical Bernoulli events
				-We can express a Bernoulli distribution as a binomial distribution with a single trial.
					-Bern(p) = B(1, p)
				-The expected value of the Bernoulli distribution suggests which outcome we expect for a single trial. Now, the expected value of the binomial distribution would suggest the number of times we expect to get a specific outcome. The graph of the binomial distribution represents the likelihood of attaining our desired outcome a specific number of times.
				-The probability function for a binomial distribution is the product of the number of combinations of picking y many elements out of n, times P to the power of y, times one minus P to the power of 1 minus y.
					-p(y)=C(n, y) * P^y * (1-y)^(n-y) 		-> The likelihood of getting a given outcome a precise number of times
				-Expected value of binomial distribution is equals the sum of all values in the sample space multiplied by their respective probabilities. 
					-E(X) = x0 * p(x0) + x1 * p(x1) + .... + xn * p(xn)
				-The expected value formula for a binomial event equals the probability of success for a given value multiplied by the number of trials we carry out
					-Y ~ B(n, p)   -> E(Y) = p * n
				-Variance, σ2 = E(y^2) - E(y)^2 = n * p * (1-p)
			-Poisson distribution
				-Y ~ Po(λ) -> Variable Y follows a poisson distribution with lambda equal to four (some number which will replace λ).
				-We used it when we want to test out how unusual an event frequency is for a given interval.
				-Poisson distribution deals with the frequency with which an event occurs within specific interval. Instead of the probability of an event, the Poisson distribution requires knowing how often it occurs for a specific period of time or distance.
				-The graph of the Poisson distribution plots, the number of instances the event occurs in a standard interval of time and the probability for each one, thus our graph would always start from zero, since no event can happen a negative amount of times.
				-Probability, P(Y) = (λ^y * e^(-λ)) / y!
					-e = Euler's number / Napier's constant =~ 2.72
					-e^(-λ) = 1 / e^λ
				-Expected value, E(y) = λ
				-µ = σ2 = λ = Elegant statistics
				-If we wish to compute the probability of an interval of a Poisson distribution, we find the joint probability of all individual elements within it.
	-Continuous distribution
		-Characteristics of Continuous distribution
			-Sample space is infinite, therefore, we cannot record the frequency of each distinct value, thus, we can no longer represent these distributions with a table. What we can do is represent them with a graph, Probability Density Function (PDF), f(y), where y is an element of sample space. The function depicts the associated probability for every possible value, y.
			-Since we are dealing with continuous outcomes, the probability distribution would be a smooth curve as opposed to unconnected individual bars, called Probability Distribution Curve(PDC), since it shows the likelihood of each outcome.
			-It is accepted that the probability for any individual value from a continuous distribution to be equal to zero.
				-P(X) = 0
				-P(x > X) = P(x >= X)
			-Cumulative Distribution Function (CDF) encompasses everything up to a certain value.
				-F(y) = P(Y <= y) = it represents probability of the random variable, Y being lower than or equal to a specific value, y.
				-F(-∞)=0 and F(∞)=1
			-CDF is used when we want to estimate Probability of some intervals.
				-Graphically, the area under the density curve would represent the chance of getting a value within that interval.
				-p(b > x > a) = ∫(a,b) p(x) dx
			-CDF represents the sum of all the PDF values up to that point.
			-PDF -> CDF (Integration): ∫(-∞,y) p(y) dy = F(y)   -> the CDF for a specific value y is equal to the integral of the density function over the interval from minus infinity to y.
			-CDF -> PDF (Derivative): p(y) = F(y) * (d/dy)   -> The PDF for any element of the sample space y equals the first derivative of the CDF with respect to Y.
			-Expected valye, E(y) = ∫(-∞,∞) y p(y) dy
			-Variance, Var(y) = E(y^2) - E(y)^2
		-Types of Continuous distribution
			-Normal distribution
				-The outcomes of many events in nature closely resembled this distribution, hence the name normal.
				-Extreme values are called outliers. The more data we gather, the lower part of the data they represent.
				-X ~ N(µ, σ2) -> Variable X follows normal distribution with mean, µ and variance, σ2.
				-The graph of a normal distribution is bell shaped, therefore, the majority of the data is centered around the mean, thus values further away from the mean are less likely to occur.
				-The graph is symmetric with regards to the mean. That suggests values equally far away in opposing directions would still be equally likely.
				-The expected value for a normal distribution equals its mean, whereas its variants (Var(X) = σ2) is usually given when we define the distribution. 
					-E(X) = µ
					-If variance is not given then computed as Var(X) = E(X^2)- E(X)^2
				-68, 95, 99.7% law
					-For any normally distributed event, 68% of all outcomes fall within one standard deviation (µ+-σ) away from the mean, 95% fall within two standard deviations (µ+-2σ) and 99.7% three.
				-Standard Normal distribution
					-A transformation is a way in which we can alter every element of a distribution to get a new distribution with similar characteristics. For normal distributions, we can use addition, subtraction, multiplication and division without changing the type of the distribution.
					-Standardizing is a special kind of transformation in which we make the expected value equal to zero (E(X) = 0) and the variance equal to one (Var(X) = 1). The distribution we get after standardizing any normal distribution is called a standard normal distribution.
					-Standard normal distribution table / Z-score table
						-Summarizes the most commonly used values for the CDF of a standard normal distribution.
					-How to achieve Standard Normal distribution?
						-Move the graph either to the left or to the right until its mean equals zero, this is done by subtracting the mean from every element.
						-To make standard deviation equal to one, divide every element of the newly obtained distribution by the value of the standard deviation.
						-Transformation: Z~N(0, 1) , Y~N(µ, σ2) => Z = (Y-µ)/σ
							-If we denote the standard normal distribution with Z, then for any normally distributed variable Y, Z equals Y minus µ over σ. This equation expresses the transformation we use when standardizing.
						-Every element of the non-standardized distribution is represented in the new distribution by the number of standard deviations, it is away from the mean.
				-A crucial factor about the normal distribution is that it requires a lot of data, if our sample is limited, we run the risk of outliers drastically affecting our analysis. In cases where we have less than 30 entries, we usually avoid assuming a normal distribution.
			-Student's-T distribution
				-Y ~ t(k) -> Variable Y follows student's-T distribution with k degree of freedom.
				-A small sample approximation of a normal distribution
				-The curve of the student's-t distribution is also bell shaped and symmetric. However, it has flater tales to accommodate the occurrence of values far away from the mean. That is because if such a value features in our limited data, it would be representing a bigger part of the total.
				-Another key difference between the student's-t distribution and the normal one is that apart from the mean and variance, we must also define the degrees of freedom for the distribution.
				-As long as, we have at least two degrees of freedom, the expected value of a T distribution is the mean µ.
					-if k>2, E(Y)=µ
				-Variance, Var(Y) = (S^2 * k) / (k-2)
				-Frequently used when conducting statistical analysis.
				-It plays a major role when we want to do hypothesis testing with limited data, since we also have a table (T-table/CDF table) summarizing the most important values of its CDF.
			-Chi-squared distribution
				-Y ~ X^2(k) -> Variable Y follows Chi-squared distribution with k degrees of freedom
				-Asymmetric continuous distribution with only non-negative values. Graphically, that means that the chi-squared distribution always starts from zero on the left. Depending on the average and maximum values within the set, the curve of the Chi-squared graph is typically skewed to the right.
				-Chi-squared distribution does not often mirror real life events. However, it is often used in hypothesis testing (to help determine goodness of fit) and for computing confidence intervals.
				-Elevating the student's-t distribution to the second power gives us the chi-squared and vice versa, finding the square root of the Chi-squared distribution gives us the student's-t.
					-Y ~ t(K) , Y^2 ~ X^2(k)
					-X ~ X^2(k), SqRoot(X) ~ t(k)
				-Chi-squared distribution also contains a table of known values, just like the normal more student's-t distribution.
				-The expected value for any chi-squared distribution is equal to its associated degrees of freedom k.
					-E(X) = k
				-The variance of chi-squared distribution is equal to two times the degrees of freedom
					-Var(X) = 2*k
			-Exponential distribution
				-X ~ Exp(λ) -> Variable X follows exponential distribution with the scale of λ.
				-The exponential distribution is usually present when we are dealing with events that are rapidly changing early on.
				-Example is how online news articles generate hits, they get most of their clicks when the topic is still fresh. The more time passes, the more irrelevant it becomes and interest dies off.
				-Variables which most closely follow an exponential distribution are ones with a probability that initially decreases before eventually plateauing. One such example is the number of views for a YouTube blog.
				-Graphically, the PDF of such a function would start off very high and sharply decrease within the first few time frames. The curve somewhat resembles a boomerang with each handle lining up with the X and Y axis. The CDF would also resemble a boomerang, however, this one has shifted 90 degrees to the right.
				-The PDF plateaus around the 0 mark, since the more y increases, the less likely it is to attain such a value, where as the CDF always has to reach the 1 mark.
				-The rate parameter, λ determines how fast the CDF curve reaches the point of plateauing and how spread out the graph is.
				-Expected value, E(Y) = 1/λ
				-Variance, Var(Y) = 1/λ^2
				-Generally, we can take the natural logarithm of every set of an exponential distribution and get a normal distribution. In statistics, we can use this new transform data to run linear regressions.
					-X ~ Exp(λ), X = ln(Y) => X ~ N(µ, σ2)
			-Logistic distribution
				-Y ~ Logistic(µ, S) -> Variable Y follows a logistic distribution with location (mean), µ and a scale of S.
				-It is useful in forecast analysis when we try to determine a cutoff point for a successful outcome.
				-We often use logistic distributions when trying to determine how continuous variable inputs can affect the probability of a binary outcome. This approach is commonly found in forecasting competitive sports events where there exist only two clear outcomes victory or defeat.
				-The graph of the PDF of the logistic distribution would look similarly to the normal distribution.
				-The graph of the logistic distribution is defined by two key features, its mean (µ) and its scale parameter (S). The former dictator at the center of the graph, while the latter shows how spread out the graph is going to be.
				-The CDF of logistic distribution is a curve that starts off slow, then picks up rather quickly before plateauing around the one mark. That is because once we reach values near the mean, the probability of converting the point drastically goes up. 
				-The scale dictate the shape of the graph. The smaller the scale, the later the graft starts to pick up, but the quicker it reaches values close to one.
				-Expected value, E(Y) = µ
				-Variance, Var(Y) = (S^2 * Pi^2) /3
		



-Probabilities in other field

-Finance
	-Option pricing
		-Option: Is an aggrement betweeen 2 parties for the price of a stock or item at a future point in time.
-Statistics
	-The field of statistics focused predominantly on samples and incomplete data. Doing so brings some uncertainty to any of the results we reach, this uncertainty is what leads us to rely on some of the most important concepts of probability, like expected values or prediction intervals.
	-Confidence interval
		-Confidence interval uses sample data to define a range within associated degree of certainty. These degrees of certainty are usually 90%, 95% or 99%, and expressed the likelihood of the population mean being within that interval.
		-Confidence intervals simply approximate some margins for the mean of the entire population based on a small sample.
		-Confidence interval can be computed using mean, variance and standard deviation. 
	-Hypothesis Testing 
		-Hypothesis is an idea that can be tested.
		-Mean, variance and type of distribution are crucial requirements for hypothesis testing.
	-Find type of distribution based on the shape of the curve and certain characteristics of the data.
	-After finding distribution, we create different models such as regressions. These models are computationally expensive, we use computer software to find the appropriate values. We call this entire process mathematical modeling. 
	-Regressions are essentially supervised machine learning.
	-What statisticians and mathematicians call mathematical modeling is what data scientists often refer to as supervised machine learning.
	-Data science serves as a natural expansion of statistics in the very same way statistics is the natural expansion of probability.
-Data Science
	-The more general the issues, the more we rely on the simpler concepts from probability and the more concrete our interests are, the more we need to delve into data science.
	-Data Analysis
		-In data analysis, we usually try to analyze past data and use some insight we find to make reasonable predictions about the future.
	-Mathematical modelling
		-In mathematical modeling, we often tend to run artificial simulations to see how well our predictions match up to various possible future outcomes.
		-On such approach is Monte Carlo simulation
		-Monte Carlo simulatio
			-We generate artificial data to test the predictive power of our mathematical models. Usually the data is not completely arbitrary, but rather follows certain restrictions we provide.
	-Data science 
		-Data Science relies on expected values much more than you would believe. Most machine learning is an extremely fast paced trial and error process where the computer adjusts its expectations as it goes through the data.
		-Even though machine and deep learning processes tend to yield very high predictive powers, they are still not 100 percent certain, each prediction has a probability of being incorrect. That is because there are unpredictable events that can occur in real life, earthquakes, volcanic eruptions or sudden scientific breakthroughs can completely change the anticipated course of events. Thus, even data scientists assign probabilities to their predictions, however accurate they may be.
	-Data science is an expansion of probability, statistics and programming that implements computational technology to solve more advanced questions.
	


-Statistics

-Population and Sample
	-Population
		-A population is the collection of all items of interest to our study, and is usually denoted with an uppercase N.
		-The numbers we've obtained when using a population are called parameters/characteristics.
		-Populations are hard to define and hard to observe in real life.
	-Sample
		-A sample is a subset of the population and is denoted with a lowercase n. 
		-The numbers we've obtained when working with the sample are called statistics.
		-A sample is much easier to gather, it is less time consuming and less costly.  Time and resources are the main reasons we prefer drawing samples compared to analyzing an entire population.
		-Sample have two defining characteristics randomness and representativeness.
			-A sample must be both random and representative for an insight to be precise.
			-A random sample is collected when each member of the sample is chosen from the population strictly by chance.
			-A representative sample is a subset of the population that accurately reflects the members of the entire population.
			-How to draw a sample that is both random and representative?



-Statistics - Descriptive Statistics
		
-Types of Data
	-Types of data
		-Categorical data
			-Describe categories or group.
			-Ex: Car brand, i.e. Mercedes, BMW, Audi etc
		-Numerical data
			-Represents number
			-Types of numerical data
				-Descrete data
					-Example: Number of children, grades, number of objects etc
				-Continuous data
					-Example: weigth of person, height, area, distance, time etc
-Levels of Measurement
	-Qualitative data
		-Nominal data
			-Example: Car brand, i.e. Mercedes, BMW, Audi etc
		-Ordinal data
			-Consists of groups and categories which follow a strict order.
			-Ex: Ratting your lunch and the options are disgusting, unappetizing, neutral, tasty and delicious
	-Quantitative data
		-Intervals and ratios are both represented by numbers, but have one major difference ratios have a true zero and intervals don't.
		-Types
			-Interval
				-Ex: Temperature
				-Variable shown in Kelvin's are ratios as we have a true zero, and we can make the claim that one temperature is two times more than another, Celsius and Fahrenheit have no true zero and are intervals.
			-Ratio
				-Ex: If I have two apples and you have six apples, you would have three times as many as I do.
				-Ex: number of objects in general, distance and time
-Categorical Variables - Visualization techniques
	-Visualizing data is the most intuitive way to interpret it, so it's an invaluable skill. It is much easier to visualize data if you know its type and measurement level.
	-Some of the most common ways to visualize categorical variables are frequency distribution tables, bar charts, pie charts and Pareto diagrams.
	-Frequency distribution table
		-It has two columns, the category itself and the corresponding frequency.
	-Bar charts/Column charts
		-The vertical axis shows the number (of units sold) while each bar represents a different category indicated on the horizontal axis.
	-Pie charts
		-In order to build pie chart, we need to calculate what percentage of the total each brand represents. In statistics, this is known as relative frequency.
		-Pie charts are especially useful when we want to not only compare items among each other, but also see their share of the total.
		-Ex: Market share
	-Pareto diagram
		-A Pareto diagram is nothing more than a special type of bar chart where categories are shown in descending order of frequency.
		-The cumulative frequency is the sum of the relative frequencies, it starts as the frequency of the first brand and then we add the second, the third and so on until it finishes at one hundred percent.
		-The pareto diagram combines the strong sides of the bar and the pie chart. It is easy to compare the data both between categories and as a part of the total.
		-Pareto Principle / 80-20 rule: It states that 80 percent of the effects come from 20 percent of the causes.
			-A real life example is a statement by Microsoft that by fixing 20 percent of its software bugs, they manage to solve 80 percent of the problems customers experience.
		-Pareto diagram is designed to show how sub-totals change with each additional category and provide us with a better understanding of our data.
-Numerical Variables - Frequency distribution table
	-Frequency distribution table is a table that contains intervals (for group of data) and corresponding frequencies.
	-The relative frequency is the frequency of a given interval as part of the total.
-The Histogram
-Cross tables and Scatter plots
	-Cross table or contingency table used for categorical data
		-Side-by-side bar chart
	-Scatter plots
		-It is used when representing two numerical variables.
-Measures of Central tendency - Mean, Median and Mode
	-Mean/Simple Average
		-For population: µ
		-For sample: X̄
		-The mean of a data set is addition of all of its components and then dividing them by their number.
		-Downside: Easily affected by outliers
	-Median
		-Median is the middle number in an ordered data set.
		-The median of the data set is the number at position (n+1)/2 in the ordered list where n is the number of observations.
		-Median is not affected by ouliers or extreme values
	-Mode
		-The mode is the value that occurs most often (it can be used for both numerical and categorical data)
		-There can be more than one mode, usually 2 or 3.
	-Which measure is best? The measures of central tendency should be used together rather than independently, therefore there is no best.
-Measures of Asymmetry -Skewness
	-Skewness indicates whether the observations in a data set are concentrated on one side.
	-Skewness tells us a lot about where the data is situated
	-When the mean is higher than the median, we have a positive or right skew
-Measures of variability 
	-Variance
		-Variance measures the dispersion of a set of data points around their mean value.
		-Population variance, (σ2) is equal to the sum of squared differences between the observed values and the population mean divided by the total number of observations.
		-Sample variance, S^2 is equal to the sum of squared differences between observed sample values and the sample mean divided by the number of sample observations, minus one.
		-Why do we elevate to the second degree? Squaring the differences has two main purposes. 
			-First, by squaring the numbers, we always get non-negative computations. It is intuitive that dispersion cannot be negative. If we calculate the difference and do not elevate to the second degree, we would obtain both positive and negative values that when some would cancel out, leaving us with no information about the dispersion.
			-Second, squaring amplifies the effect of large differences.
	-Standard deviation
		-Standard deviation is square root of variance.
	-Coefficient of variation/Relative Standard deviation
		-It is equal to the standard deviation divided by the mean.
		-Cv = σ / µ
		-Standard deviation is the most common measure of variability for a single data set.
		-Comparing the standard deviations of two different data sets is meaningless, but comparing coefficient of variation is not.
-Measures of relationship between variables
	-Covariance
		-If two variables are correlated, the main statistic to measure this correlation is called covariance.
		-Unlike variance, covariance may be positive, equal to zero or negative.
	-Linear Correlation Coefficient
		-The formulas for the correlation coefficient are the covariance divided by the product of the standard deviations of the two variables.
			-CC - Cov(x,y) / Stdev(x) * Stdev(y)
		- -1 <= Correlation COefficient <= 1
		-The correlation of one also known as perfect positive correlation means that the entire variability of one variable is explained by the other variable.
		-A correlation of zero between two variables means that they are absolutely independent from each other.
		-We can have a negative correlation coefficient, it can be perfect negative correlation of minus one or much more likely an imperfect negative correlation of a value between minus one and zero.
			-A company producing ice cream and a company selling umbrellas, ice cream tends to be sold more when the weather is very good and people buy umbrellas when it's rainy. Obviously, there is a negative correlation between the two and hence when one of the companies makes more money, the other won't.
		-Causality
			-It is important to understand the direction of causal relationships.
			-Correlation does not imply causation.
			-Causality is an asymmetric relation (x causes y is different from y causes x)
			
		
-Statistics - Inferential statistics

-Introduction
	-Inferential statistics refers to methods that rely on probability theory and distributions, in particular, to predict population values based on sample data.
-What is a distribution?
	-Distribution is nothing but probability distribution (normal/binomial/uniform).
	-A distribution is a function that shows the possible values for a variable and how often they occur.
	-The distribution of an event consists not only of the input values that can be observed, but is made up of all possible values.
	-Discret uniform distribution: all outcomes have an equal chance of occurring
	-Each probability distribution has a visual representation, it is a graph describing the likelihood of occurrence of every event. A distribution is defined by the underlying probabilities and not the graph. The graph is just a visual representation.
-Normal distribution
	-Normal and student's-t distribution 
		-Approximate a wide variety of random variables. 
		-Distributions of sample means with large enough sample sizes could be approximated to normal.
		-All computable statistics are elegant.
		-Decisions based on normal distribution insights have a good track record
	-Normal distribution = Gaussian distribution = Bell shaped curve
		-It is symmetrical and it's mean, median and mode are equal.
		-Graph has no skew. It is perfectly centered around its mean.
		-X ~ N(µ, σ2) 
		-The highest point is located at the mean because it coincides with the mode. The spread of the graph is determined by the standard deviation.
		-A lower standard deviation results in a lower dispersion. So more data in the middle and thinner tails. On the other hand, a higher standard deviation will cause the graph to flatten out with less points in the middle and more to the end or in statistics jargon, fat or tails.
-Standard Normal distribution
	-Standardization is the process of transforming variable to one with a mean of zero and a standard deviation of one.
		- X ~ N(µ, σ2) -> X ~ N(0, 1) = Z
	-When we stadardize a normal distribution, the result is Standard Normal distribution.
	-If we shift the mean by µ and the standard deviation by σ for any normal distribution, we will arrive at the standard normal distribution.
	-Standard Normal distribution is denoted as Z ~ N (0, 1)
	-z-Score, z= (x-µ)/σ
-Central limit Theorem
	-Sampling distribution
		-When we are referring to a distribution form by samples, we use the term a sampling distribution. We are dealing with a sampling distribution of the mean.
		-Each of these sample means are nothing but approximations of the population mean, the value they revolve around is actually the population mean itself.
		-X ~ N (µ, σ2/n)
	-Central limit theorem states that, No matter the distribution of the population, binomial, uniform, exponential or another one, the sampling distribution of the mean will approximate a normal distribution.
		-Not only that, but it's mean is the same as the population mean.
		-Variance = σ2/n, where σ2 = Population variance, n = sample size
		-Since the sample size is in the denominator, the bigger the sample size, the lower the variance or in other words, the closer the approximation we get.
		-Sampling distribution ~ N (µ, σ2/n), n > 30
	-The normal distribution has elegant statistics and an unmatched applicability in calculating confidence intervals and performing tests. The central limit theorem allows us to perform tests, solve problems and make inferences using the normal distribution even when the population is not normally distributed.
-Standard Error
	-The standard error is the standard deviation of the distribution form by the sample means, in other words, the standard deviation of the sampling distribution.
	-Standard Error = Standard deviation of sampling distribution = SQRT(σ2/n) = σ/SQRT(n)
	-Like a standard deviation, the standard error shows variability, in this case, it is the variability of the means of the different samples we extracted.
	-Standard Error is used for almost all statistical tests because it shows how well you approximated the true mean.
	-Standard error decreases as sample size increases
-Estimators and Estimates
	-Estimator of population paramater is an approximation depending solely on sample information. A specific value is called an estimate.
	-There are two types of estimates, point estimates and confidence interval estimates. 
		-Point estimate
			-Is a single number
			-The point estimate is located exactly in the middle of the confidence interval.
			-The sample mean, X̄ is a point estimate of the population mean, µ. The sample variance, S^2 is an estimate of the population variance, σ2.
			-There may be many estimators for the same variable, however, they all have two properties, bias and efficiency.
				-Bias
					-Estimates are like judges, we are always looking for the most efficient, unbiased estimates. An unbiased estimate has an expected value equal to the population parameter.
				-Efficiency
					-The most efficient estimates are the ones with the least variability of outcomes. That is, most efficient means the unbiased estimates are with the smallest variance.
			-The difference between estimates and statistics: The word statistic is the broader term. A point estimate is a statistic.
			-A point estimate is a single number given by an estimator. The estimator in this case is a point estimator and is the formula for the mean.
		-Confidence interval
			-Is an interval.
			-Confidence intervals provide much more information and our preferred when making inferences.
-What is Confidence Interval?
	-A confidence interval is a much more accurate representation of reality. However, there is still some uncertainty left, which we measure in levels of confidence.
	-A confidence interval is the range within which you expect the population parameter to be.
	-Level of confidence / Confidence level = (1 - α), where 0 <= α <= 1
	-Confidence Interval = [Point Estimate - Reliability Factor * Standard Error, Point Estimate + Reliability Factor * Standard Error]
						 = [X̄ - Reliability Factor * σ/SQRT(n), X̄ + Reliability Factor * σ/SQRT(n)]
-Confidence Intervals; Population Variance Known; z-score
	-A confidence interval is the range within which you expect the population parameter to be.
	-There can be two main situations when we calculate the confidence intervals for a population, when the population variance is known and when it is unknown.
	-Confidence intervals for a population mean with a known variance
		-An important assumption in this calculation is that the population is normally distributed. Even if it is not, you should use a large sample and let the central limit theorem do the normalization magic for you. If you work with a sample which is large enough, you can assume normality of sample means.
		-Confidence intervals for a population mean with a known variance = [X̄ - Z(α/2) * σ/SQRT(n), X̄ + Z(α/2) * σ/SQRT(n)]
			-Here 	-X̄ = The sample mean is the point estimate
					-Z ~ N(0,1) =  the standardized variable that has a standard normal distribution.
					-Z(α/2) = Reliability Factor
					-Confidence level = (1 - α), For a given confidence level (99%, 95%, 90%), we can compute α = (100 - (confidence level in %))/100
					-The Z(α) comes from the so-called standard normal distribution table - standard normal distribution critical values and corresponding (1 - α) 
					-Z = Critical value
			-There is a trade off between the level of confidence we chose and the estimation precision, a narrow confidence interval translates to higher uncertainty.
			-If we are trying to estimate the population mean and we are picking a larger interval, we're increasing our chances of having an interval that actually includes the mean and vice versa.
-Confidence Interval Clarifications
	-A 95 percent confidence interval would imply that we are ninety five percent confident that the true population mean falls within this interval.
	-So when α=0.05 or 5%, we have α/2= 2.5% chance that the true mean is on the left of the interval and 2.5% on the right.
	-When confience, (1-α) is lower, the CI is smaller. When confience, (1-α) is higher, the CI is larger.
	-There is a trade off between the level of confidence and the range of the interval. Ninety five percent is the accepted norm as we don't compromise with accuracy too much, but still get a relatively narrow interval.
-Student's-T distribution
	-The student's-T distribution is one of the biggest break-throughs in statistics as it allowed inference through small samples with an unknown population variance.
	-Visually, the student's-T distribution looks much like a normal distribution, but generally has fatter tails. Fatter tales allows for a higher dispersion of variables and there is more uncertainty.
	-The Student’s T distribution approximates the Normal distribution but has fatter tails. This means the probability of values being far away from the mean is bigger. For big enough samples, the Student’s T distribution coincides with the Normal distribution.
	-In the same way, z-statistic is related to the standard normal distribution, the t-statistic is related to the student's-T distribution.
		-t(n-1, α) = (X̄ - µ)/(S/SQRT(n))
			-The formula that allows us to calculate t-statistics is t with (n-1) degrees of freedom and a significant level of α equals the sample mean minus the population mean divided by the standard error of the sample.
	-The last characteristic of the student's-T statistic is that there are degrees of freedom. Usually for a sample of n, we have (n-1) degrees of freedom.
	-Like the standard normal distribution table, we also have a students-T table.
	-Please note that after the 30th row, the numbers don't vary that much, actually, after 30 degrees of freedom, the T-statistic table becomes almost the same as the Z-statistic.
	-As the degrees of freedom depend on the sample, in essence, the bigger the sample, the closer we get to the actual numbers. A common rule of thumb is that for a sample containing more than 50 observations, we use the Z-table instead of T-table.
-Confidence Interval; Population Variance unknown; t-score
	-Confidence intervals based on small samples from normally distributed populations are calculated with the t-statistic.
	-Confidence interval for the mean of a population with an unknown variance = [X̄ - t(n-1, α/2) * S/SQRT(n), X̄ + t(n-1, α/2) * S/SQRT(n)]
	-When population variance (σ) is known, population standard deviation (σ) goes with the z-statistic, z(α/2).
		-[X̄ - Z(α/2) * σ/SQRT(n), X̄ + Z(α/2) * σ/SQRT(n)]
	-When population variance is unknown, sample standard deviation (S) goes with the t-statistic, t(n-1, α/2).
		-[X̄ - t(n-1, α/2) * S/SQRT(n), X̄ + t(n-1, α/2) * S/SQRT(n)]
	-When we know the population variance, we get a narrower confidence interval, when we do not know the population variance, there is a higher uncertainty that is reflected by wider boundaries for our interval.
	-So, even when we do not know the population variance, we can still make predictions, but they will be less accurate.
	-The proper statistic for estimating the confidence interval when the population variance is unknown is the t-statistic and not z-statistic.
-Margin of Error (ME)
	-Confidence interval formulas are called Margin of error.
	-When the population variance is known, the margin of error = Z(α/2) * σ/SQRT(n)
	-When the population variance is unknown, the margin of error = t(n-1, α/2) * S/SQRT(n)
	-The true population mean falls in the interval defined by the sample mean plus minus the margin of error.
		-Confidence interval =X̄ +- ME
	-Getting a smaller margin of error means that the confidence interval would be narrower as we want a better prediction, it is in our interest to have the narrowest possible confidence interval.
	-We can control margin of error using standard deviation and the sample size.
		-Statistic and the standard deviation are in the numerator, so smaller statistics and smaller standard deviations will reduce the margin of error. How do we do that? A higher level of confidence increases the statistic, a higher statistic means a higher margin of error. This leads to a wider confidence interval.
		-If the standard deviation and the sample size are kept constant, a lower confidence level result in a narrower interval.
		-A lower standard deviation means that the data set is more concentrated around the mean, So we have a better chance to get it.
		-Higher sample sizes will decrease the margin of error. 
		-The more observations there are in the sample, the higher the chances of getting a good idea about the true mean of the entire population.
	-A higher statistic increases the ME. A higher standard deviation increases the ME. A higher sample size decreases the ME
-Confidence Interval; Two means; Dependent samples
	-When we consider 2 samples together, there are 2 situations
		-Dependent samples
			-Before and after situation
			-cause and effect
			-This statistical test is often used when developing medicine.
				-Ex: Testing magnesian level in blood after taking newly developed pill. As we will measure magnesian level before and after taking pill for same group of people, samples are dependent.
				-When dealing with biology, normality is so often observed that we immediately assume that such variables are normally distributed.
		-Independent samples
			-Population variation known
			-Population variation unknown but assumed to be equal
			-Population variation unknown but assumed to be different
-Confidence Interval; Two means; Independent samples; population variance known
	-Ex: Students from 2 different department (BE, MBA), we have number of students, their average marks and SD. We want to find a 95% confidence interval for the difference between the grades of the students from engineering and management.
	-σ2(diff) = σ2(e)/n(e) + σ2(m)/n(m)
		-The variance of the difference between the two means is equal to the variance of the grades received by engineering students divided by the sample size of engineering students, plus the variance of grades obtained by management students divided by the sample size of management students.
		-The underlying logic is that dispersion is additive, more variables means higher or equal variability.
	-CI = (X̄ - Ȳ) +- Z(α/2) * SQRT( σ2(x)/n(x) + σ2(y)/n(y) )
-Confidence Interval; Two means; Independent samples; population variance unknown but assumed to be equal
	-Ex: Price of apple in NY and LA, we have mean, SD and number of sample.
	-Sample variance = Pooled Sample Variance = S^2(P) = ( (n(x)-1)*S^2(x) + (n(y)-1)*S^2(y) ) / ( n(x) + n(y) - 2 )
	-CI = (X̄ - Ȳ) +- t(n(x)+n(y)-2, α/2) * SQRT( S^2(P)/n(x) + S^2(P)/n(y) )
		-The degrees of freedom (n(x)+n(y)-2) are equal to the total sample size, minus the number of variables.
-Confidence Interval; Two means; Independent samples; population variance unknown but assumed to be different
	-CI = (X̄ - Ȳ) +- t(v, α/2) * SQRT( S^2(x)/n(x) + S^2(y)/n(y) )
		-Difficult to remember formula to calculate degree of freedom, v.
			-v= ( S^2(x)/n(x) + S^2(y)/n(y) )^2 / ( ((S^2(x)/n(x))^2 / (n(x)-1)) + ((S^2(y)/n(y))^2 / (n(y)-1)) )  



-Hypothesis Testing

-Null vs Alternative Hypothesis
	-Confidence intervals provide us with an estimation of where the parameters are located. However, when you are making a decision, you need a yes or no answer. The correct approach in this case is to use a test. This is where hypothesis testing helps.
	-Four steps in data-driven decision making:
		-Formulate a hypothesis
		-Find the right test for your hypothesis
		-Execute the test
		-Make a decision based on the result
	-What is a hypothesis?  Hypothesis is an idea that can be tested.
	-There are two hypothesis that are made the Null hypothesis, H(0) and the alternative hypothesis, H(1) or H(A). 
	-The Null hypothesis is the one to be tested and the alternative is everything else.
	-The concept of the Null hypothesis is similar to innocent until proven guilty.
	-In statistics, the Null hypothesis is the statement we are trying to reject. Therefore, the Null hypothesis is the present state of affairs, while the alternative is our personal opinion.
-Rejection Region and Significance Level
	-Significance level, α is the probability of rejecting the null hypothesis if it is true.
	-Typical values for α are 0.01, 0.05 and 0.1. It is a value that you select based on the certainty you need.
	-Z-test, Z = (X̄ - µ) / (S/SQRT(n))
		-Z equals the sample mean minus the hypothesized mean divided by the standard error.
		-The idea is, we are standardizing or scaling the sample mean we got. If the sample mean is close enough to the hypothesized mean, then Z will be close to zero. Otherwise, it will be far away from it.
		-If the sample mean is exactly equal to the hypothesis mean, Z will be zero.
		-In all these cases, we would accept the null hypothesis.
	-How big the Z should be for us to reject the null hypothesis?
		-Calculate sample mean, X̄
		-Scale it, that is, compute Z = (X̄ - µ) / (S/SQRT(n))
		-For specified CI, compute α/2 and find value of Z from z-table.
		-Check if Z is in the rejection region
			-If Z is close to zero, then we cannot reject the null hypothesis, if it is far away from zero, then we reject the null hypothesis.
	-If the test value falls into the rejection region, you will reject the null hypothesis.
-Errors in hypothesis testing: Type I error and Type II error
	-Type I error is when you reject a true null hypothesis, it is also called a false positive. The probability of making this error is α, the level of significance.
		-Since you, the researcher, choose the α, the responsibility for making this error lies solely on you.
	-Type II error is when you accept a false null hypothesis, the probability of making this error is denoted by β. β depends mainly on sample size (n) and magnitude of the effect (σ).
		-Type II error is also known as false negative.
		-If your topic is difficult to test due to hard sampling or the effect you are looking for is almost negligible, it is more likely to make this type of error.
		-The probability of rejecting a false null hypothesis is equal to (1-β). This is the researchers goal to reject a false null hypothesis, therefore, (1-β) is called the power of the test.
		-Most often, researchers increase the power of a test by increasing the sample size.
-Test for the mean, Single population, population variance known
	-Z-score = Z = (X̄ - µ) / (σ/SQRT(n)) ~ N(0,1) = Standardizing variable by subtracting mean and deviding by SD ( in case of sample, SE) = Formula for standardization = Standardized variable associated with the test
	-z from z-table = Critical value
	-How does testing work? z ~ N(0,1) and Z ~ N(X̄ - µ(0), 1)
		-The lower case, z is normally distributed with a mean of 0 and standard deviation of one, the upper Case, Z is normally distributed with a mean of X̄ minus µ(0) and a standard deviation of one.
		-Standardization lets us compare the means, the closer the difference of X̄ and µ(0) to zero, the closer the Z score itself to zero. This implies a higher chance to accept the null hypothesis.
		-We compute Z using above formula. Then compute z for α/2 using z-table, where α is significance level. Compare standardized variable, Z with critical value, z value.
			-If Z-score > critical-value z, reject null hypothesis
			-If Z-score < critical-value z, accept null hypothesis
	-There is a special technique that allows us to see what the significance level is after which we will be unable to reject the null hypothesis.
-p-value
	-p-value is the smallest level of significance at which we can still reject the null hypothesis given the observed sample statistic.
	-If the p-value is lower than the level of significance, you should reject the null hypothesis.
	-For 1-sided test, p-value = (1 - number from z-table)
	-For 2-sided test, p-value = (1 - number from z-table) * 2
	-The closer to zero your p-value is, the more significant is the result you've obtained.
	-The p value is an extremely powerful measure as it works for all distributions, no matter if we are dealing with the normal, students-t, binomial or uniform distribution, whatever the test, the P value rationale holds.
-Test for the mean, Single Population, population variance unknown
	-Compute using t-statistic
	-T-score = (X̄ - µ) / (s/SQRT(n))
	-Compute t-score value for (n-1) degree of freedom and specified significance, α.
	-If T-score > critical-value t, reject null hypothesis
	-If T-score < critical-value t, accept null hypothesis
	-If p-value > α, accept null hypothesis
	-If p-value < α, reject null hypothesis
	-If we cannot reject a test at α level of significance, we could not reject that at smaller levels either.
-Test for the mean, Multiple population, Dependent samples, variance unknown
	-Hypothesized population mean difference, D(0) = µ(B) - µ(A) = µ(before) - µ(after)
	-Use t-statistics
	-T-score = (d̄ - µ(0)) / (s/SQRT(n)) 
	-If p-value > α, accept null hypothesis
	-If p-value < α, reject null hypothesis 
	-If 5% significance then α=0.05, if 1% significance then α=0.01.
-Test for the mean, Multiple population, Independent samples, known variance
	-Small samples + Unknown variances => t-statistic
	-Big samples + Known variances => z-statistic
	-Big samples + Unknown variances => Upto researcher but generally z-statistic
	-Z = (X̄ - µ(0)) / SQRT( σ2(e)/n(e) + σ2(m)/n(m) )
	-If p-value > α, accept null hypothesis
	-If p-value < α, reject null hypothesis 
-Test for the mean, Independent samples, unknown variance but assumed to be equal
	-Use Pooled Variance formula to compute Variance S^2(p)
	-Compute standard error of difference of mean, ST.error = SQRT( (S^2(p)/n(x)) +  (S^2(p)/n(y)) )
	-Use t-statistic
	-Degree of freedom = n(x) + n(y) - 2
	-T = (d̄ - µ(0)) / (ST.error) 
	-For common tests, a rule of thumb is to reject the null hypothesis when the test score is bigger than 2.
	-Generally for Z-score and T-score, a value that is higher than 4 is extremely significant.



-Introduction to Python

-Introduction to Programming
	-A program is a sequence of instructions that designate how to execute a computation
	-A programming is taking a task and writing it down in a programming language that the computer can understand and execute.
	-Computer science is about understanding what computers can do. Programming instead is the activity of telling computers to do something for us.
	-What skills are required to become good at programming?
		-Good problem solving skills
		-Well-developed abstract thinking
		-Mechanistic thinking
-Why Python?
	-It is a powerful computational tool when we have to solve complicated tasks in the field of finance, econometrics, economics, data science and machine learning.
	-It is an open-source, general-purpose, high-level programming language.
		-open-source: free and constantly updated
		-general-purpose: suitable for analysis of financial data. Also can be used for we programming through django framework, interoperability with other programming languages etc
		-high-level programming language: language easier to learn and implement
-Why Jupyter?
	-It is a server client application that allows you to edit your code through a Web browser.
	-The Jupyter installation always comes with an installed python kernel and the other kernels can be installed additionally.
	-The Jupiter server provides the environment where a client is matched with a corresponding languages kernel.
	-IPython Notebook document -> *.ipynb
	-It is well suited for demonstrations of programming concepts and training.
	-Solving a particular task could require coding in a few languages, say Python, R, Julia or PHP Instead of installing different interfaces for each language kernel you need, Jupyter allows you to use the same structure of the notebook type of file. Simply, each notebook you create will connect to the language kernel you request.
	-Anaconda: A software package that contains both the Python programming language and the Jupyter Notebook app.
-OOP
	-Each object belongs to some class defining the rules for creating that object and we can attach a certain number of attributes to it.
	-A method is a consequential logic sequence that can be applied to the object.
-Function vs Method
	-Function
		-can have many parameters
		-exists on its own
		-function()
	-Method
		-the object is one of its parameters
		-belongs to a class
		-object.method()
-Modules and packages
	-A module is a pre-written code containing definitions of variables, functions and classes. It can be loaded in all new programs, and we need not rewrite the code manually at the beginning of a new program.
	-A package is a collection or directory of related python modules. (Package = Library)
-Python Standard Library
	-Python Standard Library is a collection of modules available as soon as you install Python.
	-There are 4 ways to import the module:
		-import math
		 math.sqrt(16)
		-from math import sqrt
		 sqrt(16)
		-from math import sqrt as s
		 s(16)
		-import math as m
		 m.sqrt(16)
		-from math import * -> avoided for module conflict issues
		 sqrt(16)



-Introduction to Regression Analysis
	-Regression analysis is one of the most common methods of prediction. It is used whenever we have a causal (cause-effect) relationship between variables. It becomes extremely powerful when complemented by techniques like factor analysis.
	-Fundamentals of regression analysis are used in supervised machine learning.



-Advanced Statistical Methods - Linear Regression with StatsModels

-The Linear Regression Model
	-A linear regression is a linear approximation of a causal relationship between two or more variables.
	-Regression models are highly valuable as they are one of the most common ways to make inferences and predictions.
	-Process:
		-Get sample data
		-Design a model that explains the data/Design a model that works for that sample
		-Make predictions for the whole population based on the model you developed
	-There is a dependent variable (being predicted), Y and independent variables (predictors) x1, x2, ..., xk
		-Y = F(x1, x1, ..., xk)
		-The dependent variable, Y is a function of the independent variables x1 to xk and the regression model is a linear approximation of this function.
	-The simplest regression model is the simple linear regression.
		-Y = β(0) + β(1)*x1 + ε = Population formula
			Y = Dependent variable
			x = Independent variable
			β(1) = Quantifies the effect of x1 on Y
			β(0) = Constant
			ε = Error = The actual difference between the observed value and the value (Y) the regression predicted.
		-Simple Linear Regression Equation (Sample data)
			-y^ = b(0) + b(1)*x(1)
				y^ = Estimated/Predicted value
-Correlation and Regression
	-Correlation does not imply causation.
	-Correlation Analysis
		-Correlation measures the degree of relationship between two variables.
		-Correlation doesn't capture causality, but the degree of inter-relation between the two variables.
		-A property of correlation is that the correlation between X and Y is the same as between Y and X.
		-Correlation is a single point.
	-Regression Analysis
		-Regression analysis is about how one variable affects another or what changes it causes to the other.
		-Regression is based on causality, it shows no degree of connection but cause and effect.
		-Regressions of Y on X and X on Y yield different results.
		-Linear regression analysis is known for the best fitting line that goes through the data points and minimizes the distance between them.
-Geometrical Representation of the Linear Regression Model
	-When we plant the data points on an X-Y plane, the regression line is the best fitting line through the data points.
	-y^ = b(0) + b(1)*x(i)
		-b(0) = the intercept of the regression line with the y axis.
		-b(1) = the slope of the regression line.
		-The distance between the observed values and the regression line is the estimator of the error (ε), its point estimate is called residual.
-Python packages installation
	-Package needed
		-import numpy as np
			-Allow us to work with milti-dimentional arrays
		-import pandas as pd
			-Allows us to organize data in a tabular form and to attach descriptive labels to the rows and the columns of the table.
		-import scipy
			-NumPy, Pandas and matplotlib are part of large SciPy library. SciPy is a python ecosystem containing a lot of tools for scientific calculations suitable for the fields of mathematics, machine learning, engineering and more. 
		-import statsmodels.api as sm
			-A package built on top of Numpy and SciPy, which also integrates with pandas. SM provides very good summarization.
		-import matplotlib as plt
			-2D plotting library specially designed for a visualization of NumPy computations. 
			-In addition, it contains a large set of tools that can help you adjust a graph to your liking.
		-import seaborn as sns
			-Is a python visualisations library based on matplotlib. It provides a high level interface for drawing attractive statistical graphics.
		-import sklearn
			-one of the most widely used machine learning libraries. 
			-sklearn is the real deal, but statsmodels makes it easier to understand the real deal.
	-conda list
	-conda install sklearn    OR    pip install sklearn
-Decomposition of Variability
	-The determinants of a good regression | ANOVA framework
	-Sum of squares total (SST or TSS)
		-It is the squared differences between the observed dependent variable and its mean.
		-You could think of this as the dispersion of the observed variables around the mean.
		-It is a measure of the total variability of the data 
	-Sum of squares regression (SSR or ESS)
		-It is the sum of the differences between the predicted value and the mean of the dependent variable.
		-If this value of SSR is equal to the SST, it means your regression model captures all the observed variability and is perfect.
	-Sum of squares error (SSE or RSS)
		-The error is the difference between the observed value and the predicted value.
		-Also called residual sum of squares (RSS)
	-SST = SSR + SSE
		-The total variability of the data set is equal to the variability explained by the regression line, plus the unexplained variability known as error.
		-Given a constant total variability, a lower error will cause a better regression. Conversely, a higher error will cause a less powerful regression.
-What is the OLS?
	-OLS - Ordinary Least Squares
	-OLS is the most common method to estimate the linear regression equation.
	-Least Squares stands for the minimum squares error or SSE.
	-As we know, a lower error results in a better explanatory power of the regression model. So, this method aims to find the line which minimizes the sum of the squared errors.
		-S(b)=Summation(i=1->n) (y(i)-x(i)^T * b)^2 = (y - X*b)^T * (y - X*b)
		-S(b) is the OLS estimator of β for a simple linear regression
	-Other methods to determine regression line
		-Generalized least squares
		-Maximum likelihood estimation
		-Bayesian regression
		-Kernel regression
		-Gaussian process regression
-R-Squared
	-R^2 = It is equal to variability explained by the regression divided by total variability.
		-R^2 = SSR / SST
	-It is a relative measure and takes values ranging from zero to one. R squared of zero means your regression line explains none of the variability of the data and R-squared of one means your model explains the entire variability of the data.
	-Usually values varies from 0.2 to 0.9
	-In fields such as physics and chemistry, scientists are usually looking for regressions with R-squared between 0.7 and 0.99
	-However, in social sciences such as economics, finance and psychology and R-squared squared of 0.2 or 20% of the variability explained by the model could be fantastic.
	-The R-squared measures the goodness of fit of your model. The more factors you include in your regression, the higher the R-squared.
		-The R-squared shows how much of the total variability of the dataset is explained by your regression model. This may be expressed as: how well your model fits your data.

	

-Advanced Statistical Methods - Multiple Linear Regression with StatsModels

-Multiple Linear Regression
	-Population Multiple Regression Model
		-γ = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k) + ε
	-Multiple Regression Equation
		-y^ = b(0) + b(1)*x(1) + b(2)*x(2) + ... + b(k)*x(k)
			-y^ -> inferred value
			 b(0) -> intercept
			 x(1), x(2), ..., x(k) -> independent variables
			 b(1), b(2), ..., b(k) -> coefficients corresponds to independent variable
	-Multiple regressions is not about the best fitting line anymore, actually, it stops being two dimensional and when we have over three dimensions, there is no visual way to represent the data. So if it is not about the line, what is it about, it's about the best fitting model.
-Adjusted R-Squared (R̄)^2
	-We know
		-The R-squared measures how much of the total variability is explained by our model.
		-Multiple regressions are always better than simple ones, as with each additional variable that you add, the explanatory power may only increase or stay the same.
	-Like the R-squared, the adjusted R-squared measures how well your model fits the data. However, it penalizes the use of variables that are meaningless for the regression.
		-Considering the number of variables, the adjusted R-squared is always smaller than the R-squared as it penalizes excessive use of variables.
-Test for Significance of the model (F-Test)
	-Much like the Z-statistic that follows a normal distribution and the T-statistic that follows a student's-t distribution, the F-statistic follows an F distribution.
	-The F-Test is used for testing the overall significance of the model.
	-The null hypothesis is all the betas are equal to zero simultaneously (H0: β(1) = β(2) = ... = β(k) = 0 ). The alternative hypothesis is at least one β(i) differs from zero (H1: at least one β(i) != 0). What's the interpretation? If all betas are zero, then none of the independent variables matter. Therefore, our model has no merit.
	-The lower the F-statistic, the closer to a non-significant model.
-OLS assumptions
	-Linearity: γ = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k) + ε
	-No endogeneity of regressors: σxε = 0 : ∀ x,ε 
		-The covariance of the error and the x is zero for any error or x.
	-Normality and homoscedasticity of error term: ε ~ N(0, σ2)
		-Normality means the error term is normally distributed. The expected value of the error is zero, as we expect to have no errors on average.
		-Homoscedasticity means constant variance
	-No autocorrelation: σ(ε(i)ε(j)) = 0 : ∀ i != j
		-The covariance of any two error terms is zero. That's the assumption that would usually stop you from using a linear regression in your analysis.
	-No multicollinearity: ρ(x(i)x(j)) != 1 : ∀ i,j; i != j
		-Multicollinearity is observed when two or more variables have a high correlation between each other.
-OLS assumptions: Linearity
	-A linear regression is the simplest, non-trivial relationship. It is called linear because the equation is linear. Each independent variable is multiplied by a coefficient and summed up to predict the value of the dependent variable.
		-γ = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k) + ε
	-How can you verify if the relationship between two variables is linear? The easiest way is to choose an independent variable X1 and plotted against the dependent Y on a scatterplot, if the data points form a pattern that looks like a straight line, then a linear regression model is suitable.
	-You can transform non-linear regression to linear regression by applying exponential and logarithmic transformations
-OLS assumptions: No endogeneity
	-It refers to the prohibition of a link between the independent variables and the errors, mathematically expressed in the following way, σxε = 0 : ∀ x,ε
	-That is, the error (the difference between the observed values and the predicted values) is correlated with our independent values. This problem is referred to as a ommitted variable bias.
		-Ommitted variable bias is introduced to the model when you forget to include a relevant variable.
		-Omitted variable bias occurs when you forget to include a variable. This is reflected in the error term as the factor you forgot about is included in the error. In this way, the error is not random but includes a systematic part (the omitted variable).
		-As each independent variable explains why they move together and are somewhat correlated, similarly, why is also explained by the ommitted variable, so they are also correlated.
		-Chances are, the omitted variable is also correlated with at least one independent X. However, you forgot to include it as a regressor.
	-Omitted variable bias is hard to fix. Think of all the things you may have missed that led to this poor result.
	-The incorrect exclusion of a variable leads to biased and counterintuitive estimates that are toxic to your regression analysis.
	-An incorrect inclusion of a variable leads to inefficient estimates which don't bias the regression and you can immediately drop them.
-OLS assumptions: Normality and homoscedasticity
	-This comprises three parts: normality, zero mean and homoscedasticity of the error term.
	-Normality of error term
		-We assume the error term is normally distributed. Normal distribution is not required for creating the regression, but for making inferences.
		-t-tests and F-tests work because we assume normality of the error term.
		-What should we do if the error term is not normally distributed? -> Apply the central limit theorem
	-Zero mean of error term
		-If the mean is not expected to be zero, then the line is not the best fitting one. However, having an intercept solves that problem.
	-Homoscedasticity of the error term
		-The error term should have equal variance one with the other
			-σ2(ε(1)) = σ2(ε(2)) = .... = σ2(ε(k)) = σ2
		-Ex: A poor person may be forced to eat eggs or potatoes every day. A wealthy person, however, may go to a fancy restaurant one day and stay home and boil eggs the next day. The variability of his spending habits is tremendous. Therefore, we expect homoscedasticity. 
		-Is there a way to circumvent hetero homoscedasticity?
			-Check for ommitted variable bias
			-Look for outliers
			-Log transformation: for each observation in the dependent variable, calculate its natural log and then create a regression between the log(Y) and the independent Xs. Conversely, you can take the independent X that is causing you trouble and do the same.
			-Semi-log model
				-y^ = b(0) + b(1)*log(x(1)) -> As X increases by one unit, Y changes by b1 percent, such a relationship is known as elasticity
				-log(y^) = b(0) + b(1)*x(1)
			-Log-Log model
				-log(y^) = b(0) + b(1) * log(x(1))
-OLS assumptions: No autocorrelation
	-Also called no serial correlation.
	-σ(ε(i)ε(j)) = 0 : ∀ i != j -> Errors are assumed to be uncorrelated
	-Where can we observe serial correlation between errors? It is highly unlikely to find it in data taken at one moment of time known as cross-sectional data. However, it is very common in time series data.
		-There is a well-known phenomenon called the day of the Week effect, it consists in disproportionately high returns on Fridays and low returns on Mondays.
			-Investors don't have time to read all the news immediately, so they do it over the weekend. The first day to respond to negative information is on Mondays. Then during the week, their advisers give them new positive information and they start buying on Thursdays and Fridays.
			-Firms delay bad news for the weekends. So markets react on Mondays.
		-Whatever the reason, there is correlation of the errors when building regressions about stock prices.
	-So how does one detect autocorrelation?
		-A common way is to plot all the residuals on a graph and look for patterns. If you can't find any, you're safe.
		-Durban Watson test: Generally, its values fall between 0 and 4. 2 indicates no auto correlation, while values below 1 and above 3 are a cause for alarm.
	-Don't use the linear regression model when error terms are auto correlated. There are other types of regressions that deal with Time series data.
		-Autoregressive model
		-Moving average model
		-Autoregressive moving average model
		-Autoregressive integrated moving average model
-OLS assumptions: No multicollinearity
	-ρ(x(i)x(j)) != 1 : ∀ i,j; i != j
	-We observed multicollinearity when two or more variables have a high correlation.
	-Multicollinearity is a big problem, but is also the easiest to notice. Before creating the regression, find the correlation between each two pairs of independent variables and you will know if a multiple linearity problem may arise.
-Dealing with Categorical data - Dummy variables
	-In regression analysis, a dummy is a variable that is used to include categorical data into a regression model.
-Making Predictions with the Linear Regression
	-There is a StatsModels method, predict() which takes a data frame organized in a same way as X, and then make predictions.
		new_data=pd.DataFrame({'const':1,'SAT':[1700,1670],'Attendance':[0,1]})
		new_data=new_data[['const','SAT','Attendance']]
		predictions=results.predict(new_data)
		
---------------------------------------------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns
sns.set()

data=pd.read_csv('C:/Users/pakale/Downloads/real_estate_price_size.csv')
data.describe()

#Categorical variable
data['Attendance']=data['Attendance'].map({'Yes':1, 'No':0})

y=data['GPA']
x1=data[['SAT','Rand 1,2,3']]

x=sm.add_constant(x1)
results = sm.OLS(y,x).fit()
results.summary()

plt.scatter(data['SAT'],y,c=data['Attendance'],cmap='RdYlGn_r')
yhat_no=0.6439+0.0014*data['SAT']
yhat_yes=0.8665+0.0014*data['SAT']
yhat=0.275+0.0017*data['SAT']
fig=plt.plot(data['SAT'],yhat_no,lw=2,c='#006837',label='regression line 1')
fig=plt.plot(data['SAT'],yhat_yes,lw=2,c='#a50026',label='regression line 2')
fig=plt.plot(data['SAT'],yhat,lw=2,c='#4C72B0',label='regression line 2')
plt.xlabel('SAT',fontsize=20)
plt.ylabel('GPA',fontsize=20)
plt.show()
----------------------------------------------------------------	
				
-Advanced Statistical Methods - Linear Regression with sklearn

-What is sklearn and how it is different from Other packages?
	-sklearn is build on top of NumPy, SciPy and mayplotlib
	-sklearn is a machine learning package
	-sklearn features
		-sklearn is very fast and efficient and prefers working with arrays.
		-sklearn provides incredible documentation
		-Variety of features it provides: Regression, Classification, Clustering, Support vector machines, Dimensionality reduction etc
		-Numerically stable
	-Lagging in deep learning; TensorFlow, Kiros and PiTalk are much better alternative 
-Simple Linear Regression with sklearn
	-Dependent variable = output = target
	-Independent variable = input = feature
	-Standardization: the process of substracting the means and dividing by the standard deviation 
	-Normalization: we subtract the mean but divide by the L2-norm of the inputs
	-Reshape: x_matrix=x.values.reshape(-1,1)
	-LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
	-R-squared: reg.score(x_matric,y)
	-Coefficients: reg.coef_
	-Intercept: reg.intercept_
	-Making predictions: reg.predict(new_input)
-Multiple Linear Regression with sklearn
	-reg=LinearRegression()
	 reg.fit(x,y)
	 r2=reg.score(x,y)
	 n=x.shape[0]
	 p=x.shape[1]
	 adjusted_r2=1-(1-r2)*(n-1)/(n-p-1)
	 adjusted_r2
-Feature Selection (F-regression)
	-How to detect the variables which are unneeded in a model? There's actually a whole process created for that purpose. It is called feature selection.
	-Feature selection is a very important procedure in machine learning as it simplifies models, which makes them much easier to interpret by data scientists.
	-Through this process, we gain improved speed and often prevent a series of other unwanted issues arising from having too many features.
	-If a variable has a p-value > 0.05, we can disregard it. There is no builtin method in sklearn to get it. A very close concept is available with the feature_selection module from sklearn, called f_regression.
	-F-regression creates simple linear regressions of each feature (independent variables) and the dependent variable. Then the method would calculate the F-statistic for each of those regressions and return the respective p-values. If there were 50 features, 50 simple regressions would be created. Note that for a simple linear regression, the p-value of the F-statistic coincides with the p-value of the only independent variable.
	-from sklearn.feature_selection import f_regression
	 f_values=f_regression(x,y)[0]
     p_values=f_regression(x,y)[1]
	 p_values.round(3)
	-Note: these are the univariate p-values or the p-values reach from simple linear models. They do not reflect the interconnection of the features that are multiple linear regression.
-Creating a summary table with p-values
	-reg_summary=pd.DataFrame(data=x.columns.values, columns=['Features'])
	 reg_summary['Coefficient']=reg.coef_
     reg_summary['p-values']=p_values.round(3)
	 reg_summary
-Feature Scaling (Standardization)
	-Standardization = Feature Scaling: Is the process of transforming the data (we are working with) into a standard scale. This translates to subtracting the mean and dividing by the standard deviation. In this way, regardless of the data set, we will always obtain a distribution with a mean of 0 and a standard deviation of 1.
	-StandardScaler() module from sklearn is a preprocessing model used to standardize data.
		-scaler = StandardScaler()
		 scaler.fit(x)
		 x_scaled = scaler.transform(x)
-Feature Selection through Standardization of Weights
	-Coefficients = Weights in ML
	-Intercept = Biad in ML
	-reg=LinearRegression()
     reg.fit(x_scaled,y)
	 reg_summary=pd.DataFrame(data=[['Bias'],['SAT'], ['Rand 1,2,3']], columns=['Features'])
	 reg_summary['Weights']=reg.intercept_, reg.coef_[0], reg.coef_[1]
	-What about the interpretation of these weights? The closer a weight is to zero, the smaller its impact, the bigger the weight, the bigger its impact. This brings us to feature selection through standardization.
-Predicting with the Standardized Coefficients
	-new_data=pd.DataFrame(data=[[1700,2],[1800,1]], columns=['SAT','Rand 1,2,3'])
	 new_data_scaled=scaler.transform(new_data)
	 reg.predict(new_data_scaled)
-Underfitting and Overfitting
	-Overfitting means our regression has focused on the particular dataset so much it has missed the point. In other words, our training has focused on the particular training set so much, it has "missed the point".
		-Overfitting refers to models that are so super good at modeling the data that they fit or at least come very near each observation. The problem is that the random noise is captured inside an overfitting model.
	-Underfitting means the model has not captured the underlying logic of the data. It doesn't know what to do and therefore provides an answer that is far from correct.
		-Underfitting models are clumsy and have a low accuracy. You will quickly realize that either there are no relationships to be found or you need a different model.
	-Underfitting is easy to spot, you have almost no accuracy whatsoever, overfitting is much harder though, as the accuracy of the model seems outstanding.
	-There is one popular solution to overfitting, we can split their initial data set into two: training and test. Splits like 90 percent training and 10 percent test or 80, 20 are common. It works like this, we create the regression on the training data, after we have the coefficients, we test the model on the test data by assessing the accuracy. The whole point is that the model has never seen the test data set, therefore it cannot overfit on it.
-Train - Test Split Explained
	-We train the model on the training data set and then check how well it behaves on the testing one. Ultimately, we are trying to avoid the scenario where the model learns to predict the training data very well, but fails miserably when given new samples.
	-import numpy as np
	 from sklearn.model_selection import train_test_split
	 a=np.arange(1,101)
	 a_train, a_test = train_test_split(a, test_size=0.2, shuffle=True)
	-Most of the time we prefer to shuffle (shuffle=True) the data. This removes time, dependencies, day of the week effects and so on.
	-The problem with train_test_split() is that, each time we split the data, we will get different training and testing data sets.  Your R-squared is likely to change with one or two percentage points just because of the split. But, if you are trying to improve your model with many tiny tweaks, each of which are bringing one or two percent of additional explanatory power, a different shuffle every time would prevent an objective assessment of the changes.
	-In the best case scenario, you would like to have shuffled data but shuffled in the same way every time. Fortunately, sklearn has a random_state argument.
		-a_train, a_test = train_test_split(a, test_size=0.2, random_state=42)
	-We can split more than one array at the same time using train_test_split() method
		-a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=356)




-Advanced Statistical Methods - Practical Example: Linear Regression

-Rows -> axis=0
-Columns -> axis=1
-Rule of Thumb: If you are removing <5% of the observations, you are free to just remove all observations that have missing values.
-Outliers are observations that lie on abnormal distance from other observations in the data, they will affect the regression dramatically and cause coefficients to be inflated, as the regression will try to place the line closer to those values. One way to deal with that seamlessly is to remove the top one percent of observations.
	-The simplest way to do that with code is through the quantile() method, it takes one argument the quintile. You can input values from 0 to 1.
-Multicollinearity
	-One of the best ways to check for multicollinearity is through VIF (Variance Inflation Factor).
	-VIF produces a measure which estimates how much larger the square root of the standard error of an estimate is, compared to a situation where the variable was completely uncorrelated with the other predictors.
	-VIF value varies from 1 to +infinity
	-VIF=1: no multicollinearity
	-Generally 1 < VIF < 5: perfectly OK
	- 5/6/10 < VIF: unacceptable -> Different people think different unacceptable value of VIF (5 or 6 or 10)
-Add Categorical data to regression using dummy variable
	-pd.get_dummies(df [,drop_first]) -> spots all categorical variables and creates dummies automatically
	-If we have n categories for a feature, we have to create (n-1) dummies, Why? 
		-If all other dummy variables (n-1) are zeroes, it's clear that the car is an Audi. If we include a separate variable called Audi, we will introduce multicollinearity to the regression as the Audi dummy would be perfectly determined by the other variables.
-Note that, it is not usually recommended to standardize dummy variables.
-A simple way to check the final result is to plot the predicted values of our regression against the observed values.
-The residual plot: The residuals are the differences between the targets and the predictions, a residual plot refers to the distribution of the residuals
-Inferences from summary table
	-Continuous variables
		-A positive weight shows that as a feature increases in value, So do the log price and price.
		-A negative weight shows that as a feature increases in value, the price of the car decreases.
	-Dummy variables
		-A positive weight shows that the respective category (Brand) is more expensive than the benchmark (Audi).
		-A negative weight shows that the respective category (Brand) is less expensive than the benchmark (Audi).



-------------------------------------------------------------
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from sklearn.linear_model import LinearRegression

raw_data=pd.read_csv('C:/Users/pakale/Downloads/104_Real_life_example.csv')
raw_data.head()
raw_data.describe(include='all')

#drop Model column as it contains large number of unique values
data=raw_data.drop(['Model'],axis=1)
data.describe(include='all')

#check null values
data.isnull().sum()
#Remove rows containing null values
data_no_mv=data.dropna(axis=0)
data_no_mv.describe(include='all')

#Plot PDF and Remove outliers
sns.distplot(data_no_mv['Price'])
q = data_no_mv['Price'].quantile(0.99)
data_1=data_no_mv[data_no_mv['Price']<q]
sns.distplot(data_1['Price'])
data_1.describe(include='all')

sns.distplot(data_1['Mileage'])
q = data_1['Mileage'].quantile(0.99)
data_2=data_1[data_1['Mileage']<q]
sns.distplot(data_2['Mileage'])
data_2.describe(include='all')

sns.distplot(data_2['EngineV'])
data_3=data_2[data_2['EngineV']<6.5]
sns.distplot(data_3['EngineV'])
data_3.describe(include='all')

sns.distplot(data_3['Year'])
q = data_3['Year'].quantile(0.01)
data_4=data_3[data_3['Year']>q]
sns.distplot(data_4['Year'])
data_4.describe(include='all')

#Reset index
data_cleaned = data_4.reset_index(drop=True)
data_cleaned.describe(include='all')

#Plot graphs to check if variation of Price is linear with respect to Year, EngineV, Mileage
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,3))
ax1.scatter(data_cleaned['Year'], data_cleaned['Price'])
ax1.set_title('Price and Year')
ax2.scatter(data_cleaned['EngineV'], data_cleaned['Price'])
ax2.set_title('Price and EngineV')
ax3.scatter(data_cleaned['Mileage'], data_cleaned['Price'])
ax3.set_title('Price and Mileage')

#Transform Price using log
log_price=np.log(data_cleaned['Price'])
data_cleaned['log_price']=log_price

#Check graphs after transformation
f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,3))
ax1.scatter(data_cleaned['Year'], data_cleaned['log_price'])
ax1.set_title('Log Price and Year')
ax2.scatter(data_cleaned['EngineV'], data_cleaned['log_price'])
ax2.set_title('Log Price and EngineV')
ax3.scatter(data_cleaned['Mileage'], data_cleaned['log_price'])
ax3.set_title('Log Price and Mileage')

#Drop Price column
data_cleaned=data_cleaned.drop(['Price'],axis=1)

#Detect multicollinearity
data_cleaned.columns.values
from statsmodels.stats.outliers_influence import variance_inflation_factor
variables = data_cleaned[['Mileage', 'Year', 'EngineV']]
vif=pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif["features"]=variables.columns
vif
# Remove column having vif>10
data_no_multicollinearity=data_cleaned.drop(['Year'], axis=1)
data_no_multicollinearity.describe(include='all')

#Add Categorical data to regression using dummy variable
data_with_dummies=pd.get_dummies(data_no_multicollinearity, drop_first=True)
data_with_dummies.head()

#Rearrange columns
data_with_dummies.columns.values
cols = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',
       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',
       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',
       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',
       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']
data_preprocessed = data_with_dummies[cols]
data_preprocessed.head()

#Declare inputs and targets
targets=data_preprocessed['log_price']
inputs=data_preprocessed.drop(['log_price'], axis=1)

#Scale the data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(inputs)
inputs_scaled = scaler.transform(inputs)

#Train test split - Split data into training and testing
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

#Create Regression
reg = LinearRegression()
reg.fit(x_train, y_train)

#Check result - Scatter plot between Predicted values vs actual values
y_hat = reg.predict(x_train)
plt.scatter(y_train,y_hat)
plt.xlabel('Target(y_train)', size=15)
plt.ylabel('Predictions(y_hat)', size=15)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()

#Check result - Residual plot (PDF) - the residuals are the differences between the targets and the predictions
sns.distplot(y_train - y_hat)
plt.title("Residual PDF", size=15)

#Compute R-Squared
reg.score(x_train,y_train)

#Bias and weights
reg.intercept_
reg.coef_
#Summary table
reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])
reg_summary["weights"] = reg.coef_
reg_summary

#Get unique values for a column
data_cleaned['Brand'].unique()

#Testing
y_hat_test=reg.predict(x_test)
plt.scatter(y_test,y_hat_test, alpha=0.2)
plt.xlabel('Target(y_test)', size=15)
plt.ylabel('Predictions(y_hat_test)', size=15)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()

#Compare result
df_pf=pd.DataFrame(np.exp(y_hat_test), columns=['Predictions'])
y_test=y_test.reset_index(drop=True)
df_pf['Target']=np.exp(y_test)
df_pf
df_pf['Residual']=df_pf['Target']-df_pf['Predictions']
df_pf['Difference%']=np.absolute((df_pf['Residual']/df_pf['Target'])*100)
df_pf.describe()
pd.options.display.max_rows=9999
pd.set_option('display.float_format', lambda x: '%0.2f' %x)
df_pf.sort_values(by=['Difference%'])
---------------------------------------------------------------------------



-Advanced Statistical Methods - Logistic Regression

-Introduction to Logistic Regression
	-A logistic regression implies that the possible outcomes are not numerical, but rather categorical.
	-In the same way that we included categorical predictors into a linear regression through dummies, we can predict categorical outcomes through a logistic regression.
	-Using linear regression, we can make a prediction about the price a customer would pay. With the logistic regression, we can make a much more fundamental forecast - will the customer buy at all!
-A Simple example of Logistic Regression in Python
	-reg_log=sm.Logit(y,x)
	 result_Log=reg_log.fit()

	 def f(x,b0,b1):
		 return np.array(np.exp(b0+b1*x) / (1 + np.exp(b0+b1*x)))
						
	 f_sorted=np.sort(f(x1,result_Log.params[0],result_Log.params[1]))
	 x_sorted=np.sort(np.array(x1))
						
	 plt.scatter(x1,y,color='C0')
	 plt.xlabel('SAT',fontsize=20)
	 plt.ylabel('Admitted',fontsize=20)
	 plt.plot(x_sorted,f_sorted,color='C8')
	 plt.show()
-Logistic vs Logit function
	-Logistic regression assumptions
		-Non-Linear
		-No endogeneity
		-Normality and homoscedasticity
		-No autocorrelation
		-No multicollinearity
	-Logistic regression Model
		-The logistic regression predicts the probability of an event occurring.
		-Logistic regression curve has S-shape, called logistic curve and is bounded by 0 and 1.
		-p(X) = e^(β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)) / (1 + e^(β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)) )
			-Exponential of a linear combination of inputs and coefficients divided by one plus the same exponential.
	-Logit regression Model
		-p(X) / (1 - p(X)) = e^(β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k))
			-The probability of the event occurring, divided by the probability of the event not occurring equals the exponential above.
		-p(X) / (1 - p(X)) called 'odds'
		-log(p(X) / (1 - p(X))) = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)
		-log(odds) = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)
-Building a Logistic Regression
	-In statsmodels, the maximum number of iterations is 35. After that, it will stop trying and you'll get a message like this: "Warning: Maximum number of iterations has been exceeded"
	-y=data['Admitted']
	 x1=data['SAT']
	 x=sm.add_constant(x1)
	 reg_log=sm.Logit(y,x)
	 results_Log=reg_log.fit()
	 results_Log.summary()
-Understanding Logistic Regression Tables
	-Method: MLE (Maximum Likelihood Estimation)
		-Likelihood function: 
			-It is a function which estimates how likely it is that the model at hand describes the real underlying relationship of the variables.
			-The bigger the likelihood function, the higher the probability that our model is correct.
		-MLE tries to maximize the likelihood function, that's why it is called maximum likelihood estimation.
	-Log-Likelihood
		-The value of the log likelihood is almost but not always negative and the bigger it is, the better.
	-LL-Null (Log likelihood-null)
		-The LL-Null is the log likelihood of a model which has no independent variables.
		-Actually the same y is the dependent variable of that model but the sole independent variable is an array of ones.
		-y = β(0) * 1
		-x0 = np.ones(168)
	-You may want to compare the Log-Likelihood of your model with the LL-null to see if your model has any explanatory power.
	-LLR test
		-Log-Likelihood ratio test
		-LLR test it is based on the LogLikelihood of the model and the LL-null. It measures if our model is statistically different from the LL-Null, that is, a useless model.
	-LLR p-value
		-Lower the value of LLR p-value (~0.000), more the significance of the model
	-Pseudo R-squared
		-Pseudo R-squared for logistic regression = McFadden's R-squared
		-According to McFadden's, a good Pseudo R-squared is somewhere between 0.2 and 0.4.
		-This measure is mostly useful for comparing variations of the same model. 
		-Different models will have completely different and incomparable Pseudo R-squares.
-What do the Odds actually mean?
	-log(p(X) / (1 - p(X))) = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)
	-log(odds) = β(0) + β(1)*x(1) + β(2)*x(2) + ... + β(k)*x(k)
	-Odds = p(X) / (1 - p(X))
	-log(Odds_2) - log(Odds_1) = β(1) * (x(2) - x(1))
	-log(Odds_2/Odds_1) = β(1) * (x(2) - x(1))
		-For unit change, that is (x(2) - x(1)) = 1
		log(Odds_2/Odds_1) = β(1)
		Odds_2/Odds_1 = e^β(1)
		Odds_2 = e^β(1) * Odds_1
	-For a unit change in a variable, the change in the odds equals the exponential of the coefficient.
		Δodds = e^b(k)
-Binary Predictors in a Logistic Regression
	-In the same way that we created dummies for a linear regression, we can use binary predictors in a logistic regression.
-Calculating the Accuracy of the model
	-np.set_printoptions(formatter={'float': lambda x:"{0:0.2f}".format(x)})
	 results_log.predict()
	 np.array(data['Admitted'])
	 #Confusion metrix: It's called confusion as it shows how confused our model was
	 cm_df=pd.DataFrame(results_log.pred_table())
	 cm_df.columns=['Predicted 0','Predicted 1']
	 cm_df.rename(index={0:'Actual 0', 1:'Actual 1'})
	 cm_df
	 #Accuracy of model
	 cm=np.array(cm_df)
	 accuracy_train=(cm[0,0]+cm[1,1])/cm.sum()
	 accuracy_train	
-Underfitting and Overfitting
-Testing the model
	-Steps:
		-Create model from train data: results_log
		-Remove target variable from test data: test_data=test.drop(['Admitted'], axis=1)
		-Align test data as target data: test_data=sm.add_constant(test_data)  # column sequence of test data should be same as training data x variable sequence
		-Get confusion metrix
			def confusion_matrix(data,actual_values,model):
				pred_values = model.predict(data)
				bins=np.array([0,0.5,1])
				cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]
				accuracy = (cm[0,0]+cm[1,1])/cm.sum()
				return cm, accuracy
			cn = confusion_matrix(test_data, test_actual, results_log)
			cn
	-Misclassification rate: Opposite of accuracy is the misclassification rate.
		cn_df=pd.DataFrame(cn[0])
		cn_df.columns=['Predicted 0','Predicted 1']
		cn_df.rename(index={0:'Actual 0', 1:'Actual 1'})
		misclassification_rate=(cm[0,1]+cm[1,0])/cm.sum()
		misclassification_rate




-Advanced Statistical Methods - Cluster Analysis

-Introduction to Cluster Analysis
	-Cluster analysis is a multivariate statistical technique, that group's observations on the basis some of their features or variables they are described by.
		-Observations in a data set can be divided into different groups and sometimes this is very useful.
	-Ex: Geographic proximity and language are two different features by which we can cluster the observations (countries).
	-The goal of clustering is to maximize the similarity of observations within a cluster and maximize the dissimilarity between clusters.
	-Cluster analysis include:
		-Clustering problems
		-How to perform cluster analysis?
		-How to find the optimal number of clusters?
		-How to identify appropriate features?
		-How to interpret results?
-Some examples of clusters
	-There are many applications of clustering like market segmentation, image segmentation etc.
	-Clustering is often used as a preliminary step of all types of analysis.
	-Clustering is a very useful technique for exploring and identifying patterns in the data. Cluster analysis is rarely the sole method used for drawing conclusions, but is a great starting point.
	-Clustering could be applied for object recognition and computer vision.
-Difference between classification and clustering
	-In the machine learning context, linear regression and logistic regression are called as regression and classification. They are both approaches used in supervised learning.
	-The defining traits of supervised learning are that we are dealing with labelled data. Labelled data means that we know the correct values or labels (inputs/outputs) prior to creating the model. Then we create a model which fits the data, as well as possible with the idea to deploy it on future data.
	-When we use cluster analysis, we don't have labels. In the context of machine learning, that's called unsupervised learning. We cluster the observations in different groups, but we have no clue if these clusters are: 1) the right number, 2) correct at all, and 3) useful whatsoever. Moreover, the output we get is something that we must name ourselves.
	-To Summarize:
		-Classification is about predicting an output category, given input data. We train the model on the training data and then use it to predict future outcomes.
		-Clustering is about grouping data points together based on similarities among them and difference from others.
-Math prerequisites
	-Distance between two data points (The Euclidean distance)
		-Say, the coordinates of the two points are (x1, y1) and (x2, y2), then the Euclidean distance is: SQRT( (x2-x1)^2 + (y2-y1)^2 )
		-In two dimensional space, the distance between two points, A and B is the square root of the sum of the squares of the differences of the two coordinates for each point.
		-Similarly, in three dimensional space, we have the square root of the sum of the squares of the differences, but this time we have all three coordinates.
		-In n dimensional space, we would have the same formula, but along each of the axes
			-If coordinates of A are (a1, a2, ..., an) and of B are (b1, b2, ..., bn) then
				d(A, B) = SQRT( (a1-b1)^2 + (a2-b2)^2 + ... + (an-bn)^2 )
		-When performing clustering, we would be finding the distance between clusters.
	-Centroid
		-A centroid is the mean position of a group of points.
		-In physics, it is known as the Center of Mass.
		-For two points, the centroid is the midpoint of the line which connects them.




-Advanced Statistical Methods - K-Means Clustering

-K-Means clustering
	-How do we perform clustering? There are different methods to identify clusters, the most popular one is K-Means clustering.
	-How K-Means works?
		-1) Choose the number of clusters
			-K in K-Means stands for number of clusters we are trying to identify
		-2) Specify the cluster seeds
			-Seed is basically a starting centroid. It is chosen at random or as specified by the data scientist based on prior knowledge about the data.
		-3) Assign each point to a centroid
			-Assign each point on the graph to a seed (centroid), which is done based on proximity.
		-4) Adjust the centroid
			-Calculate the centroid of the green points and the orange points. The green seed will move closer to the green points to become their centroid and the orange will do the same for the orange points.
		-5) Repeat the step 3 and 4 again
-A simple example of clustering
	-DataFrame.iloc(row indices, column indices) -> slices the data frame, given the rows and columns to be kept
	-We can obtain the predicted clusters for each observation using the sklearn.cluster.KMeans.fit_predict(x) method.
	 from sklearn.cluster import KMeans
	 ...
	 x=data.iloc[:, 1:3]  # Select all rows and 2nd & 3rd columns
	 kmeans=KMeans(3)    # create 3 clusters
	 kmeans.fit(x)
	 identified_cluster = kmeans.fit_predict(x)
	 data_with_clusters = data.copy()
	 data_with_clusters['Cluster'] = identified_cluster
	 data_with_clusters
	 plt.scatter(data_with_clusters['Longitude'], data_with_clusters['Latitude'], c=data_with_clusters['Cluster'], cmap='rainbow')
	 plt.xlim(-180,180)
	 plt.ylim(-90,90)
	 plt.show()
-Clustering categorical data
	-Map categorical feature to numeric value using map() method and then use it for form clusters.
-How to choose the number of clusters?
	-The most widely adopted criterion for selecting the number of clusters is the Elbow Method.
		-What's the rationale behind it? Clustering was about minimizing the distance between points in a cluster and maximizing the distance between clusters. It turns out that for K-Means, these two occur simultaneously. If we minimize the distance between points in a cluster, we are automatically maximizing the distance between clusters.
		-Distance in measured in sum of squared, called Within-Cluster Sum of Squares (WCSS).
		-Similar to SST, SST and SSR from regressions, WCSS is a measure developed within the ANOVA framework. If we minimise WCSS, we have reached the perfect clustering solution.
		-If we plot WCSS against the number of clusters, we get a graph that looks like an elbow, hence the name, Elbow Method.
		-The point is that the WCSS is a monotonously decreasing function, which is lower for a bigger number of clusters.
		-In the beginning, WCSS is declining extremely fast, at some point it reaches the elbow. Afterwards, we are not reaching a much better solution in terms of WCSS by increasing the number of clusters.
	-To get WCSS for number of clusters, k
		-wcss = kmeans.inertia_
	-Compute wcss for different value of number of clusters, k for you data, x
		wcss=[]
		for i in range(1, 7):
			kmeans=KMeans(8)
			kmeans.fit(x)
			wcss_iter=kmeans.inertia_
			wcss.append(wcss_iter)
		wcss
		num_clusters=range(1, 7)
		plt.plot(num_clusters, wcss)
		plt.title('The Elbow Graph')
		plt.xlabel('Number of clusters')
		plt.ylabel('WCSS')
-Pros and Cons of K-Means clustering
	-Pros
		-Simple to understand
		-Fast to cluster
		-Widely available
		-Easy to implement
		-Always yield a result
	-Cons (and its remedies)
		-We need to pick k -> The Elbow method
		-Sensitive to initialization -> k-means++
			-The idea is that a preliminary iterative algorithm is ran prior to K-Means to determine the most appropriate seeds for the clustering itself.
			-sklearn employskK-means++ by default. But if you are using a different package, remember that initialization matters.
		-Sensitive to outliers -> Remove outliers prior to clustering
			-If there is a single point that is too far away from the rest, it will always be placed in its own one point cluster.
			-Get rid of outliers prior to clustering. Alternatively, if you do the clustering and spot one point clusters, remove them and cluster again.
		-Produces spherical solutions
			-On a 2D plane, we would more often see clusters that look like circles rather than elliptic shapes. The reason for this is that we are using Euclidean distance from the centroid. This is also why outliers are such a big issue for k-means.
		-Standardization
-To Standardize or not to standardize?
	-The ultimate aim of standardization is to reduce the weight of higher numbers and increase that of lower ones.
	-If we don't standardize, the range of the values will serve as weights for each variable.
	-It is a good practice to standardise the data before clustering, especially for beginners.
	-As standardization is trying to put all variables on equal footing. In some cases, we don't need to do that. If we know that one variable is inherently more important than another, then standardization shouldn't be used.
-Relationship between Clustering and Regression
	-Clustering help us identify a Ommitted Variable Bias. You could think about clustering as a method for exploring the data and realizing that one or more significant variables have not been included in the analysis.
-Market Segmentation with Cluster Analysis
	-Whenever we cluster on the basis of a single feature, the result will look like if it was cut off by a vertical/horizontal line. That's one of the ways to spot if something fishy is going on.
	-sklearn.preprocessing.scale(x) -> scales (standardize with mean 0 and standard deviation of 1 by default) each variable (column) separately
		from sklearn import preprocessing
		x_scaled = preprocessing.scale(x)
		kmeans_new=KMeans(2)   #try with 3,4
		kmeans_new.fit(x_scaled)
		clusters_new = x.copy()
		clusters_new['cluster_pred']=kmeans_new.fit_predict(x_scaled)
		plt.scatter(clusters_new['Satisfaction'], clusters_new['Loyalty'], c=clusters_new['cluster_pred'], cmap='rainbow')
		plt.xlabel('Satisfaction')
		plt.ylabel('Loyalty')
		plt.show()
-How is clustering useful?
	-Types of analysis
		-Exploratory
			-Exploratory analysis involves getting acquainted with the data, searching for patterns and determining what methods may be useful to investigate further.
			-Techniques such as data visualization, descriptive statistics and clustering are great ways to get acquainted with the data without explicitly trying to explain anything.
		-Confirmatory
		-Explanatory
			-The aim of Confirmatory and Explanatory analysis is to explain a phenomenon, confirm a hypothesis, or validate some previous research. That's where we would normally use hypothesis testing and regression analysis.
	-Clustering can be used for all types of analysis, but most commonly it is used for exploratory analysis.
	-Clustering can also be used as a confirmation of past beliefs, maybe we knew the different clusters and just wanted to assign each observation to a different cluster.
	-The big advantage of clustering is that when the market segments are changing fundamentally, a clustering solution may show that the clusters no longer look in the same way. Thus, the market has changed



-Advanced Statistical Methods - Other Types of clustering

-Types of clustering
	-Flat
		-Method: K-Means
	-Hierarchical
		-Example: Taxonomy of the animal kingdom
		-Types of hierarchical clustering
			-Agglomerative (bottom-up)
				-We start from different dog and cat breeds, cluster them into dogs and cats respectively, and then we continue pairing up species until we reach the animal cluster.
				-Agglomerative and divisive clustering should reach similar results, but agglomerative is much easier to solve mathematically.
				-Agglomerative hierarchical clustering
					-In order to perform agglomerate of hierarchical clustering, we start with each case being its own cluster. There is a total of N-clusters.
					-Using some similarity measure like Euclidean distance, we group the two closest clusters together, reaching (N-1) cluster solution.
					-Then we repeat this procedure until all observations are in a single cluster.
					-The name for graph generated by this process in called Dendrogram. A line starts from each observation, then the two closest clusters are combined, then another two and so on, until we are left with a single cluster. Note that all cluster solutions are nested inside the Dendrogram.
			-Divisive (top-down)
				-With divisive clustering, we start from a situation where all observations are in the same cluster then we split this big cluster into two smaller ones. Then we continue with three, four or five and so on until each observation is a separate cluster.
				-Method: K-Means
-Dendrogram
	-The distance between the link shows similarity or better dis-similarity between features.
	-Pros
		-Hierarchical clustering shows all the possible linkages between clusters.
		-This helps us understand the data much, much better.
		-No need to preset the number of clusters (like with k-means).
		-There are many different methods to perform hierarchical clustering, the most famous of which is the "Ward" method.
	-Cons
		-Scalability: One thousand observations, and the program is extremely hard to be examined. It is extremely computationally expensive. The more observations there are, the slower it gets.
-Heatmaps
	-data=pd.read_csv('C:/Users/pakale/Downloads/Country_clusters_standardized.csv', index_col='Country')
	 x_scaled=data.copy()
	 x_scaled=x_scaled.drop(['Language'], axis=1)
	 sns.clustermap(x_scaled, cmap='mako')
	-Heat maps are a great way to explore the data. We could even cluster these observations just by looking at the colors.
	-Read more about heat map from documentation
	
	
	
-Mathematics - Linear algebra

-What is a Matrix?
	-A matrix is a collection of numbers ordered in rows and columns. Each of these values (numbers) is an element of the matrix.
	-In linear algebra, rows and columns are the two dimensions of the matrix.
	-A matrix can only contain numbers, symbols or expressions with the idea that the latter two are nothing more than a generalised representation of numbers.
	-If A is an m by n matrix, A(mxn) , thas means it has m rows and n columns.
	-The elements of A are denoted with a(ij), and two numbers indicating the respective element position in terms of row and column.
	-In most programming languages arrays start from 0 (with the notable exception of MATLAB)
-Scalars and Vectors
	-A matrix with one row and one column, it contains a single element, that's called a scalar. In fact, all numbers we know from algebra are referred to as scalars in linear algebra.
	-Vectors are very common objects in linear algebra, they sits somewhere between scalars and matrices as they have one dimension. It's single dimension is the number of rows it has.
	-Since scalar is basically a number, a vector is practically the simplest linear algebric object.
	-It is more common to view a matrix as a collection of vectors rather than a vector as a special case of a matrix( A(mx1) or A(1xn) ).
	-There are two types of vectors: row vectors and column vectors.
	-Matrices have two dimensions, mxn. Vectors have a single dimension, mx1. And scalars have no dimensions and are 1x1.
-Linear Algebra and Geometry
	-A scalar has no dimension, it's like a point. It has no direction or size.
	-Vector has one dimension, it's like a line. It has direction.
	-Any two dimensional plane can be represented by a matrix. This idea is very simple, yet extremely powerful.
	-https://academo.org/demos/3d-vector-plotter/
-Arrays in Python - A Convenient way to represent mtrices
	-v = np.array([5,-2, 4])
	-m = np.array([[5, 12, 6], [-3, 0, 14]])
	-type(m)
	-shape() -> returns shape (dimensions) of a variable
	-m.shape()
	-reshape() -> gives an array a new shape, without changing its data
	-v.reshape(1,3)  or v.reshape(3,1)
-What is a Tensor?
	-A scaler has the lowest dimensionality and is always 1x1, it can be thought of as a vector of length 1 or a 1x1 matrix.
	-Each element of vector is a scalar. The dimensions of a vector are nothing but mx1 or 1xm matrices.
	-Matrices are nothing more than a collection of vectors, the dimensions of a matrix are mxn. In other words, a matrix is a collection of n vectors of dimensions mx1 or m vectors of dimensions nx1.
	-Scalars, vectors and matrices are all Tensor of rank zero, one and two, respectively.
	-Tensor dimensions are kxmxn. It can be thought of as a collection of matrices.
	-m1 = np.array([[5, 12, 6], [-3, 0, 14]])
	 n1 = np.array([[9, 8, 7], [1, 3, -5]])
	 t = np.array([m1, n1])
	 t.shape
	-t1 = np.array([[[ 5, 12,  6], [-3,  0, 14]], [[ 9,  8,  7], [ 1,  3, -5]]])
-Addition and Subtraction of metrices
	-For addition or subtraction, the two matrices must have the same dimensions.
	-m1 = np.array([[5, 12, 6], [-3, 0, 14]])
	 n1 = np.array([[9, 8, 7], [1, 3, -5]]
	 m1+n1
	 m1-n1
-Errors when adding matrices
	-We can add scalars to matrices and vectors.
	-m = np.array([[5, 12, 6], [-3, 0, 14]])
	 m + 1
	-Mathematically, this operation is not allowed as the shapes are different, but in programming or at least in Python, it works.
-Transpose of a matrix
	-When we transpose a vector, we are not losing any information. The values are not changing or transforming, only their position is.
	-Transposing the same vector (object) twice yields the initial vector (object).
	-A 3x1 metrix transposed is a 1x3 matrix.
	-Transposing metrices turns all of its rows into columns and vice versa.
	-In terms of dimensions, when transposed, an mxn matrix becomes an nxm matrix.
	-Transposing matrices is nothing more than transposing a bunch of vectors.
	-a = np.array([[9, 8, 7], [1, 3, -5]])
	 a.T
	-Transposing a vector X will yield the same result. That's because in Python, one dimensional arrays don't really get transposed.
-Dot Product
	-Scalar multiplication
		-np.dot(5, 6)
	-Vector multiplication
		-Condition: Vectors must have same length
		-Types of vector multiplication
			-Outer product or tensor product
			-Dot product or inner product
				-[2 8 -4] . [1 -7 3] = [2*1 + 8*(-7) + (-4)*3] = [-66]
				-Multiplication of two vectors is a scalar, that is why we also call the dot product as scalar product.
				-The dot product is nothing more than the sum of the products of the corresponding elements.
				-x=np.array([2, 8, -4])
				 y=np.array([1, -7, 3])
				 np.dot(x, y)
				-When we multiply a vector by a scalar, we get a vector with the same length. Each element of vector is multiplied by that scalar.
-Dot product of Matrices
	-We can only multiply a matrix of dimensions (m x n) with a matrix of dimensions (n x k). Basically, the second dimension of the first matrix has to match the first dimension of the second matrix.
	-When multiplying an (m x n) Matrix with an (n x k) matrix, the output is an (m x k) matrix.
	-When we have a DOT product, we always multiply a row vector times a column vector.
	-The row vectors from the first matrix determine the row in the output matrix, in the same way, the column vectors from the second matrix determine the column of the respective value.
	-m1 = np.array([[5, 12, 6], [-3, 0, 14]])
	 n1 = np.array([[2, -1], [8, 0], [3, 0]])
	 np.dot(m1,n1)
-Why is Linear Algebra Useful?
	-There are very many applications of linear algebra in data science.
		-Vectorized code/array programming
			-Whenever we are using linear algebra to compute many values simultaneously, we call this array programming or vectorized code.
			-It is important to stress that array programming is much, much faster.
		-Image recognition
			-In the last few years, deep learning and deep neural networks in particular conquered image recognition. On the forefront are Convolutional Neural Networks (CNN).
			-What is the basic idea?  You can take a photo, feed it to the algorithm and classify it.
			-Example: 
				-MNIST dataset: classify handwritten digits
				-CIFAR 10: classify animals and vehicles
				-CIFAR 100: have 100 different classes of images
			-The problem is that we cannot just take a photo and give it to the computer. We must design a way to turn that photo into numbers in order to communicate the image to the computer. Here's where linear algebra comes in.
			-Each photo has some dimensions. Each pixel in a photo is basically a colored square. Give it enough pixels on a big enough zoom out causes our brain to perceive this as an image rather than a collection of squares.
			-A color image of 400 pixel can be represented by 3x400x400 tensor. This tensor contains three 400x400 matrices.
		-Dimentionality reduction
			-Eigenvalues and Eigenvectors
			-Linear algebra provides us with fast and efficient ways to transform matrix from (mx3) or the three variables are x, y and z into a new matrix, which is (mx2) or the two variables are u and v.
			-How does that relate to the real world, why does it make sense to do that? Very often, we have too many variables that are not so different. So we want to reduce the complexity of the problem by reducing the number of variables. That's where dimensionality, reduction techniques and linear algebra come in
	
	

-Deep Learning - Introduction to Neural Networks

-Introduction to Neural	Networks
	-Creating a machine learning algorithm means building a model that outputs correct information, given that we've provided input data.
	-Think of this model as a black box, we feed input and it delivers an output.
	-Training is a central concept in machine learning, as this is the process through which the model learns how to make sense of the input data. Once we have trained our model, we can simply feed it with data and obtain an output.
	-The basic logic behind training an algorithm involves four ingredients: 
		-Data
			-We must prepare a certain amount of data to train with. Usually this is historical data which is readily available.
		-Model
			-We need a model. The simplest model we can train is a linear model. That would mean to find some coefficients, multiply each variable with them and sum everything to get the output.
		-Objective function
			-We want output of the model to be as close to reality as possible. That's where the objective function comes in. 
			-Objective function estimates how correct the models outputs are on average. The entire machine learning framework boils down to optimizing this function.
		-Optimization algorithm
			-It consists of the mechanics through which we vary the parameters of the model to optimize the objective function.
	-The machine learning process is iterative, we feed data into the model and compare the accuracy through the objective function. Then we vary the models parameters and repeat the operation. When we reach a point after which we can no longer optimize or we don't need to, we would stop since we would have found a good enough solution to our problem.
-Training the model
	-The machine learning process is a kind of trial and error training. 
	-The training process is essentially a trial-and-error process, but each consequent trial is better than the previous one, as we have methods in place that give feedback to the algorithm
	-In case of coffee maker, the machine would try various combinations of grinding the coffee, heating the water and pouring the water.
	-A reasonable optimization algorithm would not try all combinations, as there are usually inexhaustibly many other options.
	-Machine learning is very powerful, it allows systems to learn on their own situations where humans cannot define a rigid set of rules for the computer to follow. Even if we can define a set of rules, an algorithm can probably provide a better one.
-Types of Machine learning
	-There are three major types of machine learning:
		-Supervised
			-Supervised learning refers to the case where we provide the algorithm with inputs and their corresponding desired outputs. Based on this information, it learns how to produce outputs as close to the ones we are looking for.
			-Supervised learning could be divided into additional subtypes:
				-Classification
					-Classification supervised learning models provide outputs which are categories such as cats or dogs.
				-Regression
					-In regression supervised learning models, the outputs will be of numerical type. For instance, predicting the euro dollar exchange rate will always give us a continuous number like 1.21 or 1.19.
		-Unsupervised
			-In unsupervised learning, we feed inputs, but there are no target outputs. This means we don't tell the algorithm exactly what our goal is. Instead, we ask it to find some sort of dependent's or underlying logic in the data provided.
			-Unsupervised learning is especially useful when our goal is to split a dataset into a certain number of categories, which we do not know prior to implementing it, that, by the way, is called clustering.
		-Reinforcement
			-With reinforcement learning, we would train a model to act in an environment based on the rewards it receives. 
			-It is much like training your pet and rewarding it with treats every time it achieves a goal, sit!, rollover or gives you a pun. In the same way, the machine learning algorithm could be taught how to play Super Mario by rewarding it for progressing with an increase in score.
-The linear model (Linear Algebric version)
	-Linear model, y = f(x) = xw + b
		-b = intercept = bias
		 x = input
		 w = coefficient = weight
	-The goal of the machine learning algorithm would be to find such values for "w" and "b", so the output of (xw + b) is as close to the observed values as possible.
-The linear model with multiple inputs
	-y = f(x) = xw + b
	-Here x and w will be vectors of size (1,n) and (n,1) respectively.
-The linear model with multiple inputs and multiple outputs
	-Y1 = X1 * W11 + X2 * W21 + B1
	 Y2 = X1 * W12 + X2 * W22 + B2
	-In general, if we have K inputs and M outputs, the number of weights would be (K * M). The number of Bias's is equal to the number of outputs M.
	-In general, the output matrix will be in (N * M), where N is the number of observations, an M is the number of output variables. The input matrix will be (N * K), where K is the number of input variables. The weights matrix remains the same as the weights don't change depending on the number of observations. The same applies to the Bias's.
		-This last bit is extremely important. It shows us we can feed as much data in our model as we want to, and it won't change, as each model is determined solely by the weights and the biases. This property will greatly help us when creating machine learning algorithms. We very only the values of the weights and the biases, but the logic of the model stays the same cool.
-Graphical representation of Simple Neural Networks
	-Data which can be classified using a linear model is called linearly separable.
-What is the Objective Function?
	-The objective function is the measure used to evaluate how well the model's outputs match the desired correct values.
	-Objective functions are generally split into two types: loss functions and reward functions.
		-Loss functions - 
			-Also also called cost functions.
			-The lower the loss function, the higher the level of accuracy of the model.
			-Most often we work with loss functions and intuitive example is a lost function that measures the error of prediction. We want to minimize the error of prediction, thus minimize the loss.
		-Reward functions
			-The higher the reward function, the higher the level of accuracy of the model.
			-Usually reward functions are used in reinforcement learning where the goal is to maximize a specific result.
	-When dealing with supervised learning, we normally encounter lost functions.
	-When dealing with reinforcement learning, we normally encounter reward functions.
-Common Objective Functions: L2-norm Loss
	-Supervised learning is divided into Regression and Classification. For each type of supervised learning, different loss function is used.
		-Regression supervised learning -> L2-norm loss function
		-Classification supervised learning -> Cross-Entropy loss function
	-The target (T) is essentially the desired value at which we are aiming. Generally, we want our output, y to be as close as possible to the target, T.
	-The outputs of a regression are continuous numbers. A commonly used, loss function is the squared-loss, also called L2-norm loss in the machine learning realm. The method for calculating is the least squares (OLS) method used in statistics.
		-L2-norm = Summation(i) ( y(i) - t(i) )^2
		-L2-norm = the sum of the square differences between the output values, y(i) and the targets, t(i).
		-The lower this sum is, the lower the error of prediction, therefore, the lower the cost function.
-Common Objective Functions: Cross-Entropy Loss
	-The most common lost function used for classification is cross entropy and it is given as below.
	-Cross-Entropy = L(y,t) = - Summation(i) ( t(i) * ln(y(i)) )
	-The lower the loss function or the cross-entropy in this case, the more accurate the model.
	-The lower the value of loss/cross-entropy function , better the accuracy of prediction.
	-With classification, target vectors consist of a bunch of zeros and a one which indicates the correct category.
	-Most regression and classification problems are solved by using L2-norm and Cross-Entropy loss functions, but there are other loss functions that can help us resolve a problem. We must emphasise that any function that holds the basic property of being higher for worse results and lower for better results, can be a lost function. We will often use this observation when coding.	
-Optimization Algorithm: 1-Parameter Gradient Descent
	-The actual optimization process happens when the optimization algorithm varies the models parameters until the loss function has been minimized. In the context of the linear model, this implies varying W and B.
	-The simplest and the most fundamental optimization algorithm is the Gradient Descent. 
	-The gradient is the multivariate generalization of the derivative concept.
		-f'(x) = ∇Fx(x1, x2, ..., xN)
	-Logic behind gradient descent
		-f(x) = 5*x^2 + 3*x + 4
		-f'(x) = 10*x + 3
		-Consider x(0) = 4
		-Then x(i+1) = x(i) - η * f'(x(i))
			-X(1) = X(0) - η * f'(x(0))
		-η(eta) -> The learning rate -> It is the rate at which the machine learning algorithm forgets old beliefs for new ones.
		-We choose the learning rate, η for each case.
		-Using the update rule, we can find X(2) = X(1) - η * f'(x(1)), X(3) = X(2) - η * f'(x(2)) and so on.
		-After conducting the update operation long enough, the values will eventually stop updating. That is the point at which we know we have reached the minimum of the function. This is because the first derivative of the function is zero (f'(x(i)) = 0) when we have reached the minimum.
		-So the update rule, [ x(i+1) = x(i) - η * f'(x(i)) ] will become [ x(i+1) = x(i) ], that is, the update rule will no longer update.
	-The speed of minimization depends on the η(eta).
	-Oscillation: It is the situation where for specific η(eta) value, we bounce around the minimum value, but we never reach it.
	-Generally, we want the learning rate, η(eta) to be "high enough" so we can reach the closest minimum after repeating the operation in a rational amount of time. At the same time, we want η(eta) to be "low enough", so we are sure we reached the minimum and don't oscillate around it.
	-Summary of gradient descent:
		-We can find the minimum value of a function through a trial and error method.
		-There is an update rule that allows us to cherry pick the trials, so we can reach the minimum faster. Each consequent trial is better than the previous one with a nice update rule.
		-Learning rate should be high enough so we don't iterate forever, and low enough so we don't oscillate forever.
		-Once we have converged, we should stop updating or we should break the loop. One way to know we have converged is when [ x(i+1) - x(i) ] = 0.001
-Optimization Algorithm: n-Parameter Gradient Descent
	-The N-parameter gradient descent updates many weights and biases. The 1-parameter GD still could have many inputs, outputs and targets, but related to a single weight.
	-Model: x(i)*w + b = y(i)
	-Loss/Cost/Error function for regression: L(y,t)/C(y,t)/E(y,t) = L2-norm / 2 = Summation(i) ( y(i) - t(i) )^2 / 2
		-Holding the general property to be lower for higher accuracy is a lost function, division by some constant changes nothing.
	-Optimization
		-The update rule x(i+1) = x(i) - η * f'(x(i)) for above model, y(i) becomes
			-w(i+1) = w(i) - η * ∇(w)L(y,t) -> For weights
			-b(i+1) = b() - η * ∇(b)L(y,t) -> For biases
		-We are trying to optimize the loss function regarding w and b.
			-Mathematically, the gradient with respect to weight of the loss function, ∇(w)L(y,t) = Summation(i) ∇(w) ( 1/2 * ( y(i) - t(i) )^2 )
			-Replacing y(i) = x(i)*w + b
			-∇(w)L(y,t) = Summation(i) ( x(i) * (y(i) - t(i)) )
			-δ(i) = y(i) - t(i)
			-∇(w)L(y,t) = Summation(i) ( x(i) * δ(i) )
			-The gradient of the loss function with respect to bias, ∇(b)L(y,t) = Summation(i) δ(i)



-Deep Learning - How to build a neural network from scratch with NumPy

-Basic Neural Network Example
	-Elements of model in supervised learning: inputs, weights, biases, outputs, targets
	-numpy.random.uniform(low, high, size) -> draws a random value from the interval (low, high), where each number has an equal chances to be selected. "size" is the output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.
	-numpy.column_stack() -> Take a sequence of 1-D arrays and stack them as columns to make a single 2-D array.
	-Example
	import numpy as np  # mathematical operations
	import matplotlib.pyplot as plt  # nice graphs
	import mpl_toolkits.mplot3d as Axes3D  # nice 3D graphs
	#Generate random input data to train on
	observations = 1000
	xs = np.random.uniform(low=-10, high=10, size=(observations,1))
	zs = np.random.uniform(low=-10, high=10, size=(observations,1))
	inputs = np.column_stack((xs, zs))
	inputs.shape
	#Create the targets we will aim at
	noise = np.random.uniform(low=-1, high=1, size=(observations,1))
	targets = 2 * xs - 3 * zs + 5 + noise
	targets.shape
	#Initialize variable
	init_range = 0.1 	# That will be the radius of the range we will use to initialize the weights and the biases.
						# Our initial weights and biases will be picked randomly from the interval -0.1 to 0.1.
	weights = np.random.uniform(-init_range, init_range, size=(2, 1)) # size=(2, 1) -> As we have 2 inputs and 1 output
	biases = np.random.uniform(-init_range, init_range, size=1) # size=1 -> As we have 1 output
	#Set a learning rate
	learning_rate = 0.02
	#Write the algorithm to train the model
	for i in range(1000):
		#Compute outputs for given weights and biases
		outputs = np.dot(inputs, weights) + biases 				# y = wx + b
		deltas = outputs - targets 					# delta = y - t
		#Calculates a loss function that compares the outputs to the targets.
		loss = np.sum(deltas ** 2) / 2 / observations 		# L2-norm/2 loss function = (summation (y-t)^2)/2 # / observations -> average loss per observation
			#This little improvement makes the learning independent of the number of observations. Instead of adjusting the learning rate, we adjust the loss that's valuable as the same learning rate should give a similar results for both one thousand and one million observations.
		#Prints the loss so we can later analyze it
		print(loss)
		#Update the weights and the biases following the gradient descent methodology
		deltas_scaled = deltas/observations
		weights = weights - learning_rate * np.dot(inputs.T, deltas_scaled)
		biases = biases - learning_rate * np.sum(deltas_scaled)
	#Print weights and biases
	print(weights, biases)
	#Plot outputs and targets
	plt.plot(outputs, targets)
	plt.xlabel('outputs')
	plt.ylabel('targets')
	plt.show()




-Deep learning - TensorFlow 2.0: Introduction

-How to install TensorFlow 2.0
	-Open "Anaconda Prompt"
	-Check conda environment: conda info --envs
	-Create new conda environment: conda create --name py3-TF2.0 python=3
	-Activate this environment: conda activate py3-TF2.0
	-Install TensorFlow: conda install tensorflow
	-Upgrade TensorFlow: pip install --upgrade tensorflow
	-To see kernel in Jupyter: pip install ipykernel
	-In Jupyter notebook
		import tensorflow as tf
		print(tf.__version__)
-TensorFlow outline and Comparison with other libraries
	-Scalars, vectors and matrices are all Tensors of rank 0, 1 and 2 respectively.
	-Tensorflow vs sklearn
		-Currently, Tensorflow is probably the leading library for neural networks, including deep neural networks, convolution neural networks and recurrent neural networks.
		-One of the biggest advantages of TenzerFlow is, it uses not only the CPU of the computer but also its GPU. This is crucial for the speed of the algorithms, as in this way, Tenzer Flow utilizes much more computing power. The best part is that this is done automatically. Recently, Google further this trend by introducing TPU or tensor processing units, which improves performance even further.
		-sklearn library is very powerful and widely adopted. However, it does not offer the same functionality as TenzerFlow regarding neural networks.
		-Having said that, we can make the opposite point for other fields of machine learning in the presence of problems such as K-Means clustering and random forests, sklearn could be a better fit, even though TenzerFlow started to make way. That is especially true when it comes to preprocessing.
-TensorFlow 1 vs TensorFlow 2
	-Tenzer Flow one is one of the most widely used deep learning packages that's largely due to its great versatility, which makes it the preferred choice of many practitioners. Unfortunately, it has one major drawback. It's very hard to learn and use. Not only is a method strange, but the whole logic of coding is unlike most libraries out there.
	-This led to the development and popularization of higher level packages such as Pie-Torch and Keras.
		-Keras is particularly interesting, as in 2017 it was integrated in the core Tenzer flow
		-Keras is conceived as an interface for TenzerFlow rather than a different library, making this integration even easier to digest and implement.
	-Tensorflow 2.0 instead of creating their own high level syntax, the developers chose to borrow that from Keras. Actually, this decision made sense as Keras was already widely adopted and people generally love it.
	-There are major advantages of TF2 over TF1 simplified API, no duplicate or deprecated functions and some new features in the core Tenzer flow.
	-TensorFlow woes eager execution or in other words, allowing standard Python rules of physics to apply to it rather than complex computational graphs you don't really want to work with.
-A note on TensorFlow syntax
	-TensorFlow is a deep learning library developed by Google. It allows us to construct fairly complicated models with little coding.
-Types of file format supporting TensorFlow
	-TensorFlow can't work with csv or excel file. We need a format that can store the information in Tensors. One solution to this problem is .npz files, that's basically NumPy's file type. It allows you to save ndrrays or n-dimensional arrays.
	-Tensors can be represented as multidimensional arrays. When we read an .npz file, the data is already organized in the desired way.
	-This is an important part of deep learning preprocessing. You are given data and a specific file format. Then you open it, pre-process it and finally save it into an .npz.
	-np.savez(file_name, arrays) -> saved n-dimentional arrays in .npz format (at the same location as .ipynb file), using a certain keyword (label) for each array
	-Generating .npz file
		observations = 1000
		xs = np.random.uniform(low=-10, high=10, size=(observations,1))
		zs = np.random.uniform(low=-10, high=10, size=(observations,1))
		generated_inputs = np.column_stack((xs, zs))
		noise = np.random.uniform(low=-1, high=1, size=(observations,1))
		generated_targets = 2 * xs - 3 * zs + 5 + noise
		np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
-Outlining the model with TensorFlow 2
	-tf.keras.Sequential() -> function that specifies how the model will be laid down ('stacks layers'). It takes as arguments the different layers we'd like to include in our algorithm.
	-Simple algorithm generally takes inputs, applies a single linear transformation and provides outputs. These linear combinations, together with the outputs, constitute the so-called output layer.
	-tf.keras.layers.Dense(output_size, kernel_initializer, bias_initializer) -> takes the input provided to the model and calculates the dot product of the inputs and the weights and adds the bias. We can also initialize weights (kernel) and biases but not mandatory.
	-model.compile(optimizer, loss) -> configures the model for training
		-optimizer/Optimization algorithm
			-Ex: SGD (Stochastic Gradient Descent) - is a generalisation of the gradient descent concept
			-https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
		-loss/loss function
			-L2-norm loss / number of observations = mean_squared_error
			-https://www.tensorflow.org/api_docs/python/tf/keras/losses
	-model.fit(inputs, targets) -> fits (trains) the model
	-Each iteration over the full dataset in machine learning is called an Epoch.
	-Reading data from .npz file and outlining the model
		training_data = np.load('TF_intro.npz')
		input_size = 2
		output_size = 1
		model = tf.keras.Sequential([ tf.keras.layers.Dense(output_size) ])
		model.compile(optimizer='sgd', loss='mean_squared_error')
		model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=0)
-Interpreting the Result and Extracting the weights and bias
	-verbose=0 -> stands for 'silent' or no output about the training is displayed
	-verbose=1 -> progress bar
	-verbose=2 -> one line per epoch
	-Extracting weights and biases
		weights = model.layers[0].get_weights()[0]
		biases = model.layers[0].get_weights()[1]
	-model.predict_on_batch(data) -> calculates the outputs given inputs
	-Extract the outputs (make predictions)
		model.predict_on_batch(training_data['inputs']).round(1)
		training_data['targets'].round(1) # compare with targets
	-Plot the result
		plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))
		plt.xlabel('outputs')
		plt.ylabel('targets')
		plt.show()
-Customizing a TensorFlow 2 model
	-In NumPy model we need to explicitly specify following, which are not mandatory in TensorFlow 2
		-Select the best way to initialize the weights
		-Choose the starting points of the weights and biases to be random numbers
			model = tf.keras.Sequential([
				tf.keras.layers.Dense(output_size,
				kernel_initializer=tf.random_uniform_initializer(minval=-0.1,maxval=0.1), 
				bias_initializer=tf.random_uniform_initializer(minval=-0.1,maxval=0.1))
			])
		-learning_rate
			custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)
			model.compile(optimizer=custom_optimizer, loss='mean_squared_error')
		-loss function
-Complete code with TensorFlow 2.0
	import numpy as np
	import matplotlib.pyplot as plt
	import tensorflow as tf
	#Creating random input and writing inputs to .npz file
	observations = 1000
	xs = np.random.uniform(low=-10, high=10, size=(observations,1))
	zs = np.random.uniform(low=-10, high=10, size=(observations,1))
	generated_inputs = np.column_stack((xs, zs))
	noise = np.random.uniform(low=-1, high=1, size=(observations,1))
	generated_targets = 2 * xs - 3 * zs + 5 + noise
	np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)
	#Reading data from .npz file and outlining the model
	training_data = np.load('TF_intro.npz')
	input_size = 2
	output_size = 1
	model = tf.keras.Sequential([ tf.keras.layers.Dense(output_size) ])
	model.compile(optimizer='sgd', loss='mean_squared_error')
	model.fit(training_data['inputs'], training_data['targets'], epochs=100, verbose=0)
	#Extracting weights and biases
	weights = model.layers[0].get_weights()[0]
	biases = model.layers[0].get_weights()[1]
	#Extract the outputs (make predictions)
	model.predict_on_batch(training_data['inputs']).round(1)
	training_data['targets'].round(1) # compare with targets
	#Plot the result
	plt.plot(np.squeeze(model.predict_on_batch(training_data['inputs'])), np.squeeze(training_data['targets']))
	plt.xlabel('outputs')
	plt.ylabel('targets')
	plt.show()
	
	
	
	
-Deep learning - Digging deeper into NNs: Introducing Deep Neural Networks

-What is a Layer?
	-Mixing linear combinations and nonlinearities allow us to model arbitrary functions or, in other words, functions with strange, unconventional shapes.
	-Basically our model changes from inputs that are linearly combined, resulting in outputs to inputs that are linearly combined and then go through some nonlinear transformation resulting in outputs.
	-A commonly used non-linearity is the sigmoid function
		σ(x) = 1 / (1 + e^(-x))
	-The initial linear combination and the added non-linearity form a Layer. The layer is the building block of neural networks. 
	-When we have more than one layer, we are talking about a deep neural network.
-What is a Deep Net?
	-First Layer = Input Layer (the data we have)
	-Last Layer = Output Layer (the layer with which we compare targets)
	-Hidden Layer = All the layer between input and output layers
	-Output of one layer can be used as input for another layer.
	-Stacking layers one after the other produces a deep network or a deep net.
	-The building blocks of the hidden layer are called hidden units or nodes.
	-In mathematical terms, if H is the Tensor related to the hidden layer, each hidden unit is an element of that Tensor. The number of hidden units in a hidden layer is often referred to as the width of the layer.
	-Usually, but not always, we stack layers with the same width so that the layer width is equal to the width of the entire network.
	-Depth refers to the number of hidden layers in a network.
	-When we create a machine learning algorithm, we choose it's width and depth. We refer to these values as hyperparameters, not paramaters.
		-The main difference between hyperparameters and parametrs is that the value of the parameters will be derived through optimization while hyper parameters are set by us before we start optimizing.
			-Hyperparameters: width, depth, learning rate
			-Paramaters: weights, biases
-Digging into a Deep Net
	-The first layer you see in NN is the input layer. Each circle represents a separate input. These inputs are the data we feed to train the model.
	-The non-linearity doesn't change the shape of the expression (x * w), it only changes its linearity.
	-To be even more specific, each weight has to index numbers, the first one indicates the input it is referring to, while the second one indicates the hidden unit it is referring to.
-Non-linearities and their purpose
	-Non-linearities are needed so we can break the linearity and represent more complicated relationships.
	-An important consequence of including non-linearities is the ability to stack layers. Stacking layers is the process of placing one layer after the other in a meaningful way. The point is that we cannot stack layers when we have only linear relationships.
	-The two consecutive linear transformations are equivalent to a single one. Even if we add one hundred layers, the problem would be simplified to a single transformation. That is the reason we need non-linearities.
	-Without stacking layers one after the other is meaningless and we will have no depth.With no depth, each and every problem will equal the simple linear example and many practitioners would tell you it was borderline machine learning.
	-To summarize: In order to have deep nets and find complex relationships through arbitrary functions, we need non-linearities.
-Activation Functions
	-In a machine learning context, non-linearities are also called activation functions. The activation functions are also called transfer functions because of their transformation properties.
	-Activation functions transform inputs into outputs of a different kind.
	-Commonly used activation functions:
		-Sigmoid/Logistic function
			-Formula: σ(a) = 1 / (1 + e^(-a))
			-Derivative: ∂σ(a)/∂a = σ(a) * (1 - σ(a))
			-Range: (0,1)
			-Once we have applied the sigmoid as an activator, all the outputs will be contained in the range from zero to one, so the output is somewhat standardized.
		-TanH/Hyperbolic tangent
			-Formula: tanh(a) = (e^a - e^-a)/(e^a + e^-a)
			-Derivative: ∂tanh(a)/∂a = 4/ (e^a + e^-a)^2
			-Range: (-1, 1)
		-ReLu/Rectified Linear Unit
			-Formula: relu(a) = max(0,a)
			-Derivative: ∂relu(a)/∂a = 0, if a<=0; 1 if a>0
			-Range: (0, infinity)
		-Softmax
			-Formula: σ(i)(a) = e^a(i)/Summation(j) e^a(j)
			-Derivative: ∂σ(i)(a)/∂a(j) = σ(i)(a) * (δ(ij) - σ(j)(a)), where δ(ij) is 1 if i=j, 0 otherwise
			-Range: (0,1)
	-Graphs of all these functions are monotonic, continuous and differentiable. These are important properties needed for the optimization process.
-Activation Functions: Softmax Activation
	-Formula: σ(i)(a) = e^a(i)/Summation(j) e^a(j)
	-The softmax function is equal to the exponential of the element at position i divided by the sum of the exponential of all elements of the vector.
	-While the other activation functions get an input value and transform it regardless of the other elements, the softmax considers the information about the whole set of numbers we have.
	-A key aspect of the softmax transformation is that the values it output's are in the range from zero to one  and there sum is exactly one. What else has such a property? -> Probabilities?
	-The softmax transformation transforms a bunch of arbitrarily large or small numbers (that come out of previous layers and fit them) into a valid probability distribution.
	-The softmax activation is often used as the activation of the final output layer in classification problems. So no matter what happens before, the final output of the algorithm is a probability distribution.
-Backpropagation
	-Backpropagation of output layer
		∂E/∂y(j) ∂y(j)/∂a(j) ∂a(j)/∂u(ij) = (y(j)-t(j)) (1-y(j)) x(i) = δ(j)*x(i)
	-Backpropagation of hidden layer
		∂L/∂w(ij) = δ(j)*x(i), where δ(j) = Summation(k) δ(k)*w(jk)*y(j)*(1-y(j))
	-The training process consists of updating parameters through the gradient descent for optimizing the objective function. In supervised learning, the process of optimization consisted of minimizing the loss. Our updates were directly related to the partial derivatives of the loss and indirectly related to the errors or deltas. The deltas are the differences between the targets and the outputs.
	-The procedure for calculating deltas for the hidden layers is called backpropagation of errors. Having these deltas allows us to vary parameters using the familiar update rule.
	-Forward propagation: Forward propagation is the process of pushing inputs through the net. At the end of each epoch, the obtained outputs are compared to the targets to form the errors. Then we back propagate through partial derivatives and change each parameter, so errors at the next epoch are minimized.
		-For the minimal example, the backpropagation consisted of a single step aligning the weights, given the errors we obtained. 
		-But when we have a deep net, we must update all the weights related to the input layer and all the hidden layers. We also introduced activation function so we have to update the weights accordingly, considering the used non-linearities and their derivatives. 
		-Finally, to update the weights, we must compare the outputs to the targets, this has done for each layer, but we have no targets for the hidden units. We don't know the errors. So how do we update the weights? That's what back propagation is all about. We must derive the appropriate updates as if we had targets.
		-The way academics solve this issue is through errors. The main point is that we can trace the contribution of each unit, hidden or not, to the error of the output.
-Backpropagation picture
	-Through backpropagation, the algorithm identifies which weights lead to which errors. Then it adjusts the weights that have a bigger contribution to the errors by more than the weights with a smaller contribution.
	-A big problem arises when we must also consider the activation functions, they introduce additional complexity to this process. Linnear contributions are easy, but non-linear ones are tougher.
	-Backpropagation is one of the biggest challenges for the speed of an algorithm.
	
	
	
-Deep Learning - Overfitting

-What is Overfitting?
	-Overfitting means our training has focused on the particular training set so much it has missed the point.
	-Underfitting means the model has not captured the underlying logic of the data. It doesn't know what to do and therefore provides an answer that is far from correct.
	-Underfitted models are clumsy. They have high costs in terms of high loss functions and their accuracy is low. You quickly realize that either there are no relationships to be found or you need a more complex model.
	-Overfitting refers to models that are so super good at modeling the training data that they fit or come very near each observation. The problem is that the random noise is captured inside an overfitting model.
-Underfitting and Overfitting for Classification
	-What is a well-trained model? It is somewhere between an underfitting and an overfitting model. This fine balance is often called the Bias-variance Tradeoff.
-What is Validation?
	-To prevent Overfitting, one must be able to identify it first. To spot overfitting, we first divide our available data set into three subsets: training, validation and test.
	-Training dataset train the model to its final form.
	-The validation dataset is the one that will help us detect and prevent Overfitting.
	-On average, the loss function calculated for the validation set should be the same as the one of the training set. This is logical as the training and validation sets were extracted from the same initial data set containing the same perceived dependencies.
	-Normally, we would perform this operation many times in the process of creating a good machine learning algorithm. The two lost functions we calculate are referred to as training loss and validation loss. And because the data in the training set is trained using the gradient descent, each subsequent loss will be lower or equal to the previous one. That's how gradient descent works by definition. So we are sure the trading loss is being minimized. That's where the validation loss comes in play.
	-At some point, the validation loss could start increasing, that's a red flag we are overfitting. We are getting better at predicting the training set, but we are moving away from the overall logic data. At this point, we should stop training the model.
	-It is extremely important that the model is not trained on validation samples. This will eliminate the whole purpose of the above mentioned process. The training set and the validation set should be separate without overlapping each other.
-Training, validation and test datasets
	-After we have trained the model and validated it, it is time to measure its predictive power. Logically, this is done by running the model on a new data set (test dataset) it hasn't seen before.
	-Generally we splits the original dataset into 80% training, 10% validation and 10% test or 70%, 20%, 10% are commonly used.
	-Steps involved in training, validation and test datasets process
		-1. Get a dataset
		-2. Split it into three parts
		-3. We train the model using the training data set
		-4. Every now and then, we validate the model by running it for the validation data set. What does every now and then mean? Usually we validate the dataset for every epoch, every time we adjust all weights and calculate the training loss, we validate. If the training loss and the validation loss go hand in hand, we carry on training the model. If the validation loss is increasing, we are overfitting, so we should stop.
		-5. Test the model with the test dataset. The accuracy you obtain at this stage is the accuracy of your machine learning algorithm.
-N-Fold Cross-Validation
	-What if we have a small data set, we can't afford to split it into three data sets as we will lose some of the underlying relationships or worse, we can have so little data left for training that the algorithm cannot learn anything. There is answer to this issue, and it's called N-Fold Cross-Validation.
	-This is a strategy that resembles the general one, but combines the train and validation datasets in a clever way. However, it still requires a test subset.
	-In data science, we often deal with ginormous data sets. "Ginormous" datasets have their own big problem - being so large they often have a lot of missing values. Such a data set is usually referred to as being "sparse". This introduces a whole new spectrum of issues.
	-We split the combined training and validation datasets into 10 subsets, so this is a 10-Fold cross-validation. 10 is also a most commonly used value.
	-We treat one subset as a validation set, while the other nine combined as a training set. During the first epoch, the first chunk of data serves as validation, then in the second epoch, the second chunk of data serves as validation and so on. In this way, for each epoch, we don't overlap training and validation as it should be. Moreover, we manage to use the whole data set except for the test part.
	-As with all good things, this comes at a price we have still trained on the validation set, which was not a good idea. It is less likely that the overfitting flag is raised and it is possible that we overfit it a bit.
	-The trade-off is between not having a model or having a model that's a bit overfitted. N-fold cross-validation solves the scarce data issue, but should by no means be used as the norm.
	-Whenever you can, divide your data into three parts: training, validation and test. Only if it doesn't manage to learn much because of data scarcity, you should try the N-fold cross-validation.
-Early Stopping and When to stop training
	-We train the model until the lost function is minimized. We can go on doing that forever, but at some point will overfit, that is why we introduced the validation dataset.
	-Early stopping
		-Early stopping is a technique to prevent overfitting. It is called early stopping as we want to stop early before we overfit.
		-Most common ways to do that are:
			-1. The simplest way is to train for a preset number of epochs
				-This solve the problem, but no guarantee that the minimum has been reached or past.
				-A high enough learning rate would even cause the loss to diverge to infinity.
			-2. Stop when the loss function updates become sufficiently small (0.001)
				-A common rule of thumb is to stop when the relative decrease in the lost function becomes less than 0.001 or 0.1%
				-This simple rule has two underlying ideas:
					-We are sure we won't stop before we have reached a minimum. That's because of the way gradient descent works. It will descend until a minimum is reached.
					-We want to save computing power by using as few iterations as possible. Once we have reached the minimum or diverged to infinity, we will stop there knowing that a gazillion more epochs won't change a thing. This saves us the trouble of iterating uselessly without updating anything.
				-Problem with preset number of epochs and this method is that it can lead to tremendous overfitting.
			-3. Validation set strategy
				-This is the simplest clever technique for early stopping that prevents overfitting.
				-If we plot "Training Loss" and "Validation loss" curves on the "Time vs Loss" plot, there will be a point when we start overfitting (the validation loss/cost will start increasing). This is the point where two functions (training and validation curve) begin diverging, that's our red flag, we should stop the algorithm before we do more damage to the model.
				-Cons: Sometimes, it is possible that the weights are barely moving and we still haven't started overfitting, in this case model will iterate uselessly.
		-So general rule is, Stop when the validation loss starts increasing or when the training loss becomes very small. That is combination of 2nd and 3rd method. 




-Deep Learning - Initialization

-What is initialization?
	-Initialization is the process in which we set the initial values of weights. This is important as an inappropriate initialization would cause an un-optimizable model.
	-Does it really matter what the initial weights are? Yes, we need random initial weights.
-Types of Simple Initializations
	-1. A simple approach would be to initialize weights randomly within a small range.
		-init_range = 0.1
		weights = np.random.uniform(-init_range, init_range, size=(2, 1))
		-This approach chooses the values randomly, but in a uniform manner, each one has the exact same probability of being chosen.
	-2. We could choose a normal initializer
		-We pick the numbers from a zero mean normal distribution. The chosen variance is arbitrary, but should be small. 
		-Since numbers follows the normal distribution, values closer to zero are much more likely to be chosen than other values.
		-Ex: Draw from a normal distribution with a mean 0 and a standard deviation of 0.1.
	-Problem:
		-Weights are used in linear combinations. Then the linear combinations are activated (using sigmoid function).
		-Activation functions take as inputs the linear combination of the units from the previous layer.
		-If the weights are too small, this will cause output of the sigmoid function to follow linear function. SO, the sigmoid would not apply in non-linearity, but a linearity to the linear combination. 
		-Conversely, if the weight values are too large or too small, the sigmoid is almost flat, which causes the output of the sigmoid to be only once or only Xeros respectively. A static output of activations minimizes the gradient, well, the algorithm is not really trained.
		So what we want is a wide range of inputs for the sigmoid. These inputs depend on the weights, so the weights will have to be initialized in a reasonable range.
-State-of-the-Art Method: Xavior/Glorot Initialization
	-"Understanding the difficulty of training deep feedforward neural networks" - paper published by Xavior Glorot.
	-The main idea is that the method used for randomization isn't so important, it is the number of outputs in the following layer that does. With the passing of each layer, the Xavior initialization is maintaining the variance in some bounds, so we can take full advantage of activation functions.
	-Uniform Xavior initialization
		-Draw each weight, w, from a random uniform distribution in [-x, x] for x = SQRT(6/(inputs+outputs))
	-Normal Xavior initialization
		-Draw each weight, w, from a normal distribution with a mean of 0 and a standard deviation σ = SQRT(2/(inputs+outputs))
			-inputs = number of inputs for the transformation
			-outputs = number of outputs for the transformation
	-Another thing to notice is that the number of inputs and outputs matters. 
		-Outputs are clear, that's where the activation function is going. So the higher number of outputs, the higher need to spread weights.
		-What about inputs? Well, optimization is done through back propagation, so when we back propagate, we would obviously have the same problem but in the opposite direction.
	-In TensorFlow, "Normal Xavior initialization" is the default initializer.
	



-Deep Learning - Digging into Gradient Descent and Learning Rate Schedules

-Stochastic Gradient Descent
	-Optimization refers to the algorithms we use to vary our models parameters.
	-Optimizer - Gradient Descent (GD)
		-GD iterates over the whole training set before updating the weights. Each update is very small, that's due to the whole concept of the gradient descent, driven by the small value of the learning rate.
		-There is simple solution to this very small weight update problem, called Stochastic Gradient Descent (SGD).
	-Optimizer - Stochastic Gradient Descent (SGD)
		-Instead of updating the weights once per epoch, it updates them in real time inside a single epoch.
		-The stochastic gradient descent is closely related to the concept of batching. 
		-Batching is the process of splitting data into N batches, often called mini batches. We update the weights after every batch instead of every epich.
		-Let's say we have 10000 training points,.If we choose a batch size of 1000, then we have 10 batches per epoch. So for every full iteration over the training data set, we would update the weights 10 times instead of one. This is by no means a new method. It is the same as the gradient descent, but much faster.
		-The SGD comes at a cost, it approximates things a bit, so we lose a bit of accuracy, but the tradeoff is worth it. That's confirmed by the fact that virtually everyone in the industry uses stochastic gradient descent, not gradient descent.
		-So why does this speed up the algorithm so drastically? There are a couple of reasons, but one of the finest is related to hardware. Splitting the training set into batches allows the CPU course or the GPU course to train on different batches in parallel. This gives an incredible speed boost, which is why practitioners rely on it.
		-Actually, stochastic gradient descent is when you update after every input, so your batch size is one. What we have been talking about was technically called "Mini-batch Gradient Descent". However, more often than not, practitioners refer to the mini-batch as stochastic gradient descent.
		-The plain gradient descent is called Batch GD, as it has a single batch.
-Problems with Gradient Descent
	-Loss function can have more than one local imposter (local minimum) and one global minimum.
	-Each local minimum is a suboptimal solution to the machine learning optimization. Gradient descent is prone to this issue. Often it falls into the closest minimum to the starting point rather than the global minimum. Of course, it depends on the learning rate as well.
	-A higher learning rate may miss the first local minimum and fall directly into the global valley. However, it is likely to oscillate and never reach it.
-Momentum
	-How to improve our chances of reaching the global minimum rather than getting stuck in a local one?
	-The GD and the SGD are good ways to train our models, we need not change them. We should simply extend them. The simplest extension we should apply is called momentum.
	-What is momentum? The faster the ball rolls, the higher as its momentum. A small dip in the grass would not stop the ball. It would rather continue rolling until it has reached a flat surface out of which it cannot go. The small dip is the local minimum, while the big valley is the global minimum.
	-How do we add momentum to the algorithm? The rules so far was w(i) = w - η * ∂L/∂w
		-∂L/∂w = the gradient of the loss, L with respect to  w
	-Including momentum, we will consider the speed with which we've been descending so far. For instance, if the ball is rolling fast, the momentum is high. Otherwise, the momentum is low. The best way to find out how fast the ball rolls is to check how fast it rolled a moment ago.
		-w(i) = w(t) - η * ∂L/∂w(t) - α * η * ∂L/∂w(t-1)
			-α = 0.9 is conventional, to adjust the previous update. α is hyperparameter and we can play around with it for better results.
-Learning rate Schedules, or How to choose the Optimal Learning Rate
	-Hyperparameters
		-width = number of hidden units
		-depth = number fo hidden layers
		-learning rate, η
	-Learning rate, η
		-It must be small enough, so we gently descend through the loss function instead of oscillating wildly around the minimum and never reaching it or diverging to infinity.
		-It also had to be big enough, so the optimization takes place in a reasonable amount of time.
	-A smart way to deal with choosing the proper learning rate is adopting a so-called "Learning Rate Schedule". Learning rate schedules get the best of both worlds, small enough and big enough.
	-Learning Rate Schedule process:
		-We start from a high initial learning rate, this leads to faster training in this way, we approach the minimum faster.
			-Simplest solution is setting a pre-determined piecewise, constant learning rate (0.1 -> 0.01 -> 0.001).
			-The exponential schedule, it smoothly reduces or decays the learning rate.
				η(0) = 0.1
				η(i) = η(0) * e^(-n/c)
					-where n = current epoch
					      c = some constant
					-There is no rule for the constant, c, but usually it should be int the same order of magnitude as the number of Epichs needed to minimize the loss.
						-For 100 epochs, 50 < c < 500
						-For 1000 epochs, 500 < c < 5000
						-For <100, usually c = 20 or 30
						-c is also a hyperparameter
		-Then we want to lower the rate gradually to avoid oscillations
		-Around the end of the training, we pick a very small learning rate to get an accurate solution
-Learning rate schedules visualized
	-A well selected learning rate, such as the one defined by the exponential schedule would minimize the loss much faster than a low learning rate. Moreover, it would do so more accurately than a high learning rate. Problem is, we don't know what this learning rate is for our particular data and model.
	-One way to establish a good learning rate is to plot the graph between "number of epochs and Loss" for a few learning rate values and pick the one that looks the most like the good curve.
-Adaptive Learning Rate Schedules (AdaGrad and RMSprop)
	-AdaGrad - Adaptive Gradient Algorithm
		-It dynamically varies the learning rate at each update and for every weight individually.
		-The original rule was:
			w(t+1) = w(t) - η * ∂L/∂w(t)
			w(t+1) - w(t) = - η * ∂L/∂w(t)
			Δw = -η * ∂L/∂w(t)
		-In case of AdaGrad
			Δw(i)(t) = - (η / (SQRT(G(i)(t)) + ε) )  * ∂L/∂w(i)(t)
				G = The adaptation magic
				G(i)(t) = G(i)(t-1) + ( ∂L/∂w(i)(t) )^2, with beginning point G(i)(0) = 0
				G is a monotonously increasing function.
				ε = small number we need to pick there because initially G(i)(0) = 0
		-Since we are dividing the learning rate, η by a monotonically increasing function, η divide by that function is obviously monotonically decreasing.
		-AdaGrad is smart, adaptive learning rate. 
			-Adaptive stands for the fact that the effective learning rate is based on the training itself.
			-It is not a pre-set learning schedule like the exponential one where all the eita values are calculated regardless of the training process.
			-Another very important point is that the adaptation is per weight, this means every individual weight in the whole network keeps track of its own G function to normalize its own steps. It's an important observation as different weights do not reach their optimal values simultaneously.
	-RMSprop - Root Mean Square Propagation
		-It is very similar to AdaGrad, the update rule is defined in the same way, but the G-function is a bit different.
			-Δw(i)(t) = - (η / (SQRT(G(i)(t)) + ε) )  * ∂L/∂w(i)(t)
			-G(i)(t) = β * G(i)(t-1) + (1-β) * ( ∂L/∂w(i)(t) )^2, with beginning point G(i)(0) = 0
			-New hyperparameter, β is a number between 0 to 1, the value of 0.9 is very typical.
			-The great implication of β, is that the function, G(i)(t) is no longer monotonically increasing. Hence, (η / (SQRT(G(i)(t)) + ε) ) is not monotonically decreasing. Empirical evidence shows that in this way the rate adapts much more efficiently.
-Adam (Adaptive Moment Estimation)
	-If we take the learning rate schedules and the momentum, we can reach a state of the hour optimization algorithm called the Adaptive Moment Estimation. It is the most advanced optimizer applied in practice.
	-AdaGrad and RMSprop does not include momentum. Adam steps on RMSprop and introduces momentum into the equation.
		Δw(i)(t) = - (η / (SQRT(G(i)(t)) + ε) )  * M(i)(t)
		M(i)(t) = α * M(i)(t-1) + (1-α) * ∂L/∂w(i)(t), where M(i)(0) = 0




-Deep Learning - Preprocessing

-Preprocessing introduction
	-Preprocessing refers to any manipulation we apply to the dataset before running it through the model.
	-What is the motivation for preprocessing?
		-Compatibility with the libraries we use
		-We may need to adjust inputs of different magnitude
		-Generalization: problems that seem different can often be solved by similar models, standardizing inputs of different problems allows us to reuse the exact same models. Sometimes there are cases when we can even reuse already train networks.
-Types of basic preprocessing
	-In the world of finance, we can transform relative changes into logarithms. Many statistical and mathematical methods take advantage of logarithms as they facilitate faster computation.
	-In machine learning, log transformations are not as common, but can increase the speed of learning.
	-Logarithms
		-Faster computation
		-Lower order of magnitude
		-Clearer relationships
		-Homogeneous variance
-Standardization
	-The most common problem when working with numerical data is about the difference in magnitudes, an easy fix for this issue is standardization.
	-Standardization = Feature scaling (Sometime also called normalization)
	-Standardization is the process of transforming the data into a standard scale. A very common way to approach this problem is by subtracting the mean and dividing by the standard deviation. In this way, regardless of the dataset, we will always obtain a distribution with a mean of 0 and standard deviation of 1.
	-Standardization ensure that linear combinations treat the two variables equally. Also, it is much easier to make sense of the data.
	-One of the concept of normalization is: Converting each sample into a unit length vector using the L1 or L2-norm.
	-PCA (Principal Components Analysis)
		-It is a dimension reduction technique often used when working with several variables, referring to the same bigger concept or latent variable.
		-Dimension reduction technique often used to combine several variables into bigger(latent) variable.
		-For example, if we have data about one's religion, voting history, participation in different associations and upbringing, we can combine these four to reflect his or her "attitude towards immigration". This new variable will normally be standardized in a range with a mean of 0 and a standard deviation of 1.
	-Whitening
		-Whitening is often performed after PCA and removes most of the underlying correlations between data points. Whitening can be useful when conceptually the data should be uncorrelated, but that's not reflected in the observations.
-Preprocessing Categorical Data
	-Categorical data refers to groups or categories. But the machine learning algorithm takes only numbers as values. Therefore, the question when working with categorical data is how to convert a category into a number so we can input it into a model or output it in the end. Obviously, a different number (Tensor) should be associated with each category.
	-So our question becomes how to encode categories in a way which will be useful for a machine learning algorithm? Two main ways are adopted:
		-Binary encoding
			-Binary encoding implies we should turn numbers into binary. The next step of the process is to divide these into different columns as if we were creating two new variables.
			-But there are some problems with binary encoding, so one-hot comes to rescue. 
		-One-hot encoding
			-It consists of creating as many columns as there are possible values.
			-There is one big problem with one high encoding, though, one encoding requires a lot of new variables.
			-For example, IKEA offers around 12000 products. Do we want to include 12000 columns in our inputs? Definitely not. If we used binary, the 12000 products would be represented by 16 columns only. This is exponentially lower than the 12000 columns we would need for one-hot encoding. In such cases, we must use binary, even though that would introduce some unjustified correlations between the products. Clearly, there is a trade off between binary and one-hot encoding, we would prefer one-hot when we have a few categories and binary when dealing with many categories.




-Deep Learning - Classifying on the MNIST Dataset

-MNIST: The Dataset
	-MNIST dataset consist of around 70000 images of handwritten digits.
	-Yann LeCun - Founding Father of CNN
-MNIST: How to Tackle the MNIST
	-Each image in the MNIST dataset is 28x28 pixel. It's on a gray scale. So we can think about the problem as a 28x28 matrix, where input values are from 0 to 255, 0 corresponds to purely black and 255 to purely white. A 28x28 photo will have 784 pixels.
	-The approach for deep field forward neural networks is to transform or flatten each image into a vector of length 784. So for each image, we would have 784 input's. Each input corresponds to the intensity of the color of the corresponding pixel.
	-Then we will linearly combine them and add a non-linearity to get the first hidden layer. For our example, we will build a model with two hidden layers. Two hidden layers are enough to produce a model with very good accuracy.
	-Finally, we will produce 10 output units in the output layer. The output will then be compared to the targets. It will use one-hot encoding for both the outputs and the target's. For example, the digit 0 will be represented by [1,0,0,0,0,0,0,0,0,0] vector, while the digit 5 by [0,0,0,0,0,1,0,0,0,0].
	-Since we would like to see the probability of a digit being rightfully labeled, we will use a softmax activation function for the output layer.
	-The MNIST action plan
		-Prepare our data and process it. Create training, validation and test datasets as well as select the batch size.
		-Outline the model and choose the activation functions
		-Set the appropriate advanced optimizers and the loss function
		-Make it learn (The algorithm will back propagate its way to accuracy at each epoch we will validate)
		-Test the accuracy of the model
-MNIST: Importing the relevant packages and loading the data
	-tfdf.load(name, with_info, as_supervised) 
		-name -> loads a dataset from TensorFlow datasets
		-as_supervised=True -> loads the data in a 2-tuple structure [input, target]
		-with_info=True -> provides a tuple containing info about version, features, number of samples of the dataset
-MNIST: Preprocess the data - Create a validation set and scale it	
	-Extract training and testing dataset from complete dataset
		mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']
	-tf.cast(x, dtype) -> casts a variable into a given type
	-dataset.map(*function*) -> applies a custom transformation to a given dataset. If takes as input a function which determines the transformation.
-MNIST: Preprocess the data - Shuffle and Batch	
	-When shuffling, we are basically keeping the same information, but in a different order.
	-BUFFER_SIZE parameter is used in cases when we are dealing with enormous data sets. In such cases, we can't shuffle the whole data set in one go because we can't possibly fit it all in the memory of the computer.
	-dataset.batch(batch_size) -> a method that combines the consecutive elements of a dataset into batches
	-Batching was useful in updating weights only once per batch, which is like 100 samples rather than at every sample, hence reducing noise in the training updates.
	-iter() -> creates an object that can be iterated one element at a time 
	-next() -> loads the next element of an iterable object
-MNIST: Outline the model
	-tf.keras.Sequential() -> function that is laying down the model (used to 'stack layers')
	-tf.keras.layers.Flatten(original_shape) -> transforms (flatten) a tensor into a vector
-MNIST: Select the loss and the optimizer
-MNIST: Learning/Training
	-What happen inside an epoch?
		-At the beginning of each epoch, the trading loss will be set to 0.
		-The algorithm will iterate over a preset number of batches, all extracted from the train_data.
		-The weights and biases will be updated as many times as there are batches.
		-We'll get a value for the loss function, indicating how the training is going.
		-We'll also see a training accuracy
		-At the end of the epoch, the algorithm will forward propagate the whole validation dataset in a single batch through the optimized model and calculate the validation accuracy.
		-When we reach the maximum number of epochs, the training will be over.
-MNIST: Testing the model
	-model.evaluate() -> returns the loss value and metrics values for the model in 'test mode'
	
-Complete code	
#Import relevant packages
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfdf
#Load TensorFlow dataset
mnist_dataset, mnist_info = tfdf.load(name='mnist', with_info=True, as_supervised=True) 
#Extract training and testing dataset
mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']
#Extract validation sample count (10% of training data) from training samples
num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples
num_validation_samples = tf.cast(num_validation_samples, tf.int64)
#Extract test samples count
num_test_samples = 0.1 * mnist_info.splits['test'].num_examples
num_test_samples = tf.cast(num_test_samples, tf.int64)
#Scale function to scale the data so that result will be numerically stable (i.e. inputs between 0 and 1)
def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255.0
    return image, label
#Scale the data
scaled_train_and_validation_data = mnist_train.map(scale)
test_data = mnist_test.map(scale)
#Shuffle training and validation dataset
BUFFER_SIZE = 10000
shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)
#Retrieve validation and test dataset from shuffled dataset
validation_data = shuffled_train_and_validation_data.take(num_validation_samples)
train_data = shuffled_train_and_validation_data.skip(num_validation_samples)
#Create batches of train dataset
batch_size = 100
train_data = train_data.batch(batch_size)
#No matching needed for validation or test data, so create single batch for them
validation_data = validation_data.batch(num_validation_samples)
test_data = test_data.batch(num_test_samples)
#Separate out validation input and targets from validation_data as it contains 2-tuple structure [input, target]
validation_inputs, validation_targets = next(iter(validation_data))
#Outline the model
input_size = 784
output_size = 10
hidden_layer_size = 50   # Modify this to 100 and retest
model = tf.keras.Sequential([
			tf.keras.layers.Flatten(input_shape=(28,28,1)),
			tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
			tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  #Try to add more hidden layers to model
			tf.keras.layers.Dense(output_size, activation='softmax')
		])
#Specify the optimizer and loss function for model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
	#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
	#model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
#Training the model
NUM_EPOCHS = 5
model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)
#Testing the model
test_loss, test_accuracy = model.evaluate(test_data)
print('Test loss: {0:.2f}, Test accuracy: {1:.2f}'.format(test_loss, test_accuracy*100.))



-Deep Learning - Business Case Example

-Preprocess the data
	-Balance the dataset
		-If we have three classes, cats, dogs and horses; balancing the data set would imply picking a dataset where each class amounts to approximately one third of the dataset.
		-sklearn capabilities can be is used for standardizing the datasets. Almost always we standardize all inputs as the quality of the algorithm improve significantly. Without standardizing the inputs, we reach 10% less accuracy for the model.
		-targets_all.shape[0] -> The shape of target_all on axis=0, is basically the lenght of the vector (number of all targets)
		-np.delete(array, obj_to_delete, axis) -> method that deletes an object along an axis
		-preprocessing.scale(X) -> standardize an array along each variable
		-np.arange([start],stop) -> is a method that returns a evenly spaced values within a given interval 
		-np.random.shuffle(X) -> is a method that shuffles the numbers in a given sequence
	-Divide the dataset into train, validation and test -> prevent overfitting
	-Save the data in a tensor friendly format -> .npz format
	-model.fit(callback=EarlyStopping) => callbacks are utilties/functions that are called at certain point during model training
		-EarlyStopping -> Stop training when a monitored quantity has stopped improving
		-early_stopping = tf.keras.callbacks.EarlyStopping(patience)
			-By default, this object will monitor the validation loss and stop the traning process the first time the validation loss starts increasing 
			-'patience' -> let us decide how many consecutive increases we can tolerate

-Preprocessing of data and conversion from .csv to .npz
#Import packages
import numpy as np
from sklearn import preprocessing
#load the data from csv
raw_csv_data = np.loadtxt('C:/Users/pakale/Downloads/Audiobooks_data.csv', delimiter=',')
unscaled_inputs_all = raw_csv_data[:,1:-1]
targets_all = raw_csv_data[:,-1]
#Balance the dataset
num_one_targets = int(np.sum(targets_all))
zero_targets_counter = 0
indices_to_remove = []

for i in range(targets_all.shape[0]):
    if targets_all[i]==0:
        zero_targets_counter+=1
        if zero_targets_counter>num_one_targets:
            indices_to_remove.append(i)
            
unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)
targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)
#Standardize the inputs
scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)
#Shuffle the data (inputs and targets)
shuffled_indices = np.arange(scaled_inputs.shape[0])
np.random.shuffle(shuffled_indices)

shuffled_inputs = scaled_inputs[shuffled_indices]
shuffled_targets = targets_equal_priors[shuffled_indices]
#Split the dataset into train, validation and test
samples_count = scaled_inputs.shape[0]

train_samples_count = int(0.8*samples_count)
validation_samples_count = int(0.1*samples_count)
test_samples_count = samples_count - train_samples_count - validation_samples_count

train_inputs = shuffled_inputs[:train_samples_count]
train_targets = shuffled_targets[:train_samples_count]

validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]
validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]

test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]
test_targets = shuffled_targets[train_samples_count+validation_samples_count:]

print(np.sum(train_targets), train_samples_count, np.sum(train_targets)/train_samples_count)
print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets)/validation_samples_count)
print(np.sum(test_targets), test_samples_count, np.sum(test_targets)/test_samples_count)
#Save the datasets in .npz format
np.savez()

-Create machine learning algorithm
#Import packages
import numpy as np
import tensorflow as tf
#Load the data
npz = np.load('Audiobooks_data_train.npz')
train_inputs = npz['inputs'].astype(float)
train_targets = npz['targets'].astype(int)

npz = np.load('Audiobooks_data_validation.npz')
validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)

npz = np.load('Audiobooks_data_test.npz')
test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)
#Outline the model
input_size = 10
output_size = 2
hidden_layer_size = 50

model = tf.keras.Sequential([
            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),
            tf.keras.layers.Dense(output_size, activation='softmax')
        ])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

batch_size = 100
max_epochs = 100

early_stopping = tf.keras.callbacks.EarlyStopping()

model.fit(train_inputs, 
          train_targets, 
          batch_size=batch_size, 
          epochs=max_epochs, 
		  callbacks=[early_stopping],
          validation_data=(validation_inputs, validation_targets), 
          verbose=2)
#Test the model
test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)
print('Test loss: {0:.2f}, Test accuracy: {1:.2f}'.format(test_loss, test_accuracy*100.))





-Deep Learning - Conclusion

-What's further out there in terms of ML
	-CNNs - Convolutional Neural Networks
	-RNNs - Recurrent Neural Networks
-CNNs - Convolutional Neural Networks
	-Lets consider MNIST dataset. 
	-In simple feed forward neural network:
		-We flatten the images into a vector of length 784 (28x28). That way, however, we lost the spatial information of every pixels neighborhood of pixels.
		-For example, 28th pixel and 29th pixel are next to each other in the vector, but in the picture they are actually quite far away.
		-Since we always start by taking a linear combination of the inputs, it will search for particular digits in particular places. For instance, a 7 written to the top left corner is very different from a 7 written in the bottom right corner.
	-A convolutional neural network solves this problem by dealing with the original 28x28 photos without flattening them. Instead, it applies tiny, say 5x5, so-called "kernels" to every possible position of the image. Kernels are like weights.
		-So if we start from the top and continue down, we can count a total of 24, 5x5 squares, the same applies if we go from the left to the right side. Therefore, the next layer is 24x24, which is the number of 5x5 sub-matrices. This is called "Convolution".
		-A layer that we get is called the "convolutional layer". The number of kernels you choose is a hyperparameter and you are not restricted to a single kernel.
		-Apart from convolution, there is another main step in CNNs, "Pooling". Most commonly we would divide these 24x24 squares into multiple 2x2 squares without overlapping, and we'll take the largest number in the 2x2 matrix because we assume it is the strongest detail. That's how we reduce the dimensionality of the problem.
		-What makes the issue slightly more complicated, though, is that most photos have colors, which implies that images have height and width, but also color or depth. This is a third dimension with three variables, according to the scheme, red, green and blue.
		-Thus our original photo was not of shape 28x28x3 but rather 12x12x3. That's also where the whole Tensas approach fits perfectly.
		-So the initial 28x28 matrix was converted in a 28x28x3 tensor which was then reduced to a 24x24x3 tensor, depending on the number of kernels we chose to use.
		-If we convolute and pool for long enough, we can reduce the dimensions to a vector containing one-hot encoded categories like Dog, Cat, Horse and so on. 
	-CNN's are mainly use an image recognition due to their two major advantages:
		-Spatial proximities are preserved: It matters where in the photo we find a certain detail.
		-A certain detail (such as a human eye) is looked for everywhere in the photo: Thus, a face can be recognized everywhere on the image.
	-These two advantages make CNN's predictive power much higher than that of NN, especially when it comes to image related problems. Instances are robot vision, self-driving cars, Facebook tagging and Apple's face recognition for unlocking the iPhone.
	-A recent application of CNN for a non-image related problem was the Google assistant technology developed by Deep Mind. This is a competitor product to Siri, Alexa and Cortana. They managed to apply CNN's in a way that converts artificial sound in a more human-like shape than ever.
	-Well, currently image related problems are mostly used by companies such as Google, Tesla, Apple, Microsoft, Amazon and tech startups. The applications of CNN so far seem more or less out of reach for most people.
-RNNs - Recurrent Neural Networks	
	-While CNN's are great for modeling and dealing with visual data, RNN's were specifically designed for sequential data. Examples are trading signals, stocks, bonds and forex or music where there is a certain flow. Speech recognition and handwriting recognition are also instances of Arnon's, as each consequent word often depends on what you said before.
	-It is probable that the suggestive keyboard on your phone was developed through an RNN.
	-Similar to CNNs, RNNs are developed on the top of NN. The specific feature of RNN's is that they have memory.
	-Procedure:
		-We have an input layer, a hidden layer and an output layer. We forward propagate, but we keep the information from the hidden layer.
		-For the second sample, we forward propagate again. But the hidden layer for the second sample is a function of the input layer, the updated weights and the hidden layer from the previous iteration. Essentially, these are connections between the hidden units and their own values for the previous input's.
		-In NN, every input-target pair was independent. With this new structure and output is formed by the current input and the hidden units values from previous forward propagations. Therefore, we must learn their weights as well.
		-If you think about RNN, it's basically a Deep NN, the net, though, is not just deep but extremely deep. This is called "unrolling" the RNN. If we unroll the added time dependencies, we will end up with a deep feed forward neural network.
	-While RNN seem very easy, they are computationally expensive as they are extremely deep.
	-Due to the high number of layers involved and their interconnection over time, one must have a very focused approach and a solid understanding of deep NN's  before diving into RNN's.	
-An overview of non-NN approaches
	-A discriminative model is one that uses an input and then provides a probability of whether an output is correct.
	-Another instance of discriminative models are "random forests". They are based on decision trees, and this is why they are called forests. The main idea is that decision trees are not so good at classifying as they tend to overfit a lot. A random forest takes many decision trees and makes the point that many bad classifiers equal a good classifier. That said, random forests are mainly used for classification.
	-There are also "Generative models" that don't give an output, Y given X? The target is actually the joint probability distribution of X and Y, which carries more information. It goes from inputs to outputs and then from outputs to inputs. That's useful for problems such as translation's.
		-If you have an English to Mandarin translator, you'll probably want it to work for Mandarin to English as well, or at least you are going to check if 	it's working.
		-Example: 
			-Hidden Markov Models: Hidden Markov models assume that the problem at hand is a Markov process. Broadly speaking, this implies we can predict the future based on the present, just as well as if we had the whole history of the process.
			-Bayesian network: These models take into account prior probabilities. The difference between a neural network and Bayesian network is that with Bayesian networks, probabilities of an event occurring are used as the models inputs in a neural network. In a NN, each input alone doesn't mean much, but a model trained on many, many inputs gives amazing insights.
				-P(H|E) = (P(E|H) * P(H)) / P(E)
				-Posterial probability of 'H' given the evidence, 'E' is the likelihood of the evidence 'E' if the hypothesis 'H' is true multiplier by prior probability, P(H), divided by the prior probability that the evidence itself is true.
				-Bayesian networks are quite useful when we have some uncertainty, for example, in medicine. 
				-A person may have a certain disease with some symptoms manifested, while another may have the same disease, but with completely different symptoms manifested. This is a situation where we have conflicting information, a neural network would be confused as there would be no trend to be found between the two patients, while the Bayesian network would assume that such a case is not unusual.
	-NNs are a great place to start exploring the machine learning universe. Moreover, it seems like they are advancing at a much faster pace than other types of machine learning. It's worth saying that they are so fascinatingly good at predicting that we humans are not quite sure why they beat every other type of analysis we've invented so far. Research is making way, but so far no one truly knows why these structures solve problems with such high accuracy.




-Software Integration

-What are data, servers, clients, requests and responses?
	-Data means information stored in the form of symbols- 1s and 0s, other digits, letters, special characters, etc. Thus it can be collected, measured, analyzed and processed further.
	-Data today is stored practically in databases. The latter represent an electronic collection of data or a structure filled with information if you prefer, organized in a way that is easy to access, manage and update. And where our database is stored in database servers.
	-A server is a combination of hardware and software responsible for storing, managing and processing large amounts of data.
	-The most common types are the Web servers, database servers and FTP (free transfer protocol) servers, these facilitate the transfer of Web pages, database queries and files, respectively.
-What are data connectivity, apis and endpoints?
	-Data connectivity: It regards the ability to connect clients and servers, securing the swift and voluminous transfer of information between them.
	-How can one connect information from multiple servers simultaneously? How is this data delivered to the clients? The simple answer is by using APIs (Application programming interfaces)
	-An API is a contract allowing software to share data with each other. Otherwise, in simple words, it lets devices and software applications communicate in real time.
	-APIs are a collection of endpoints to which developers can attach and then extract specific information that can be used by those who are working with the apps.
	-Data asset: Is just data that is expected to have some value in the future. Ex: intellectual property(patents, songs), databases, websites, code etc. Therefore, data assets are any forms of data or software related to the processing of information.
	-An application often called App refers to a program designed to perform a specific set of operations for the end user, be it a person or another application. Instances of apps are web browsers, video editing programs, database management software and so on. Applications have a specific purpose and are able to carry out specific tasks they have been designed for.
	-Programming programming simply refers to the function the app performs, in other words, the process of converting inputs into outputs.
	-An API is namely the technology necessary to process requests from the clients to the servers and in response brings the requested data from the server. So an API acts pretty much like a messenger.
-Software Integration
	-Software Integration is a system or an architecture composed of a few different software products, various programming languages or other pieces of software which can communicate with each other via APIs or a common API.
	-Software Integration refers to a situation where multiple software products can be set up to work as one tool.
	-The idea here is to enhance the functionality and the performance of an already existing application, thus solving a certain business problem quicker and more efficiently.
	-Drivers: 



-Case Study - Absenteeism:
	-Absenteeism: The absence from work during normal working hours resulting in temporary incapacity to execute regular working activity.
		-Based on what information should we predict whether an employee is expected to be absent or not?
		-How would we measure absenteeism?
	-Purpose of business exercise: whether a person presenting certain characteristics is expected to be away from work at some point in time or not?
	-We want to know for how many working hours an employee could be away from work based on information such as how far they live from their workplace, how many children and pets they have, do they have higher education and so on.
	-Dataset: prediction of absenteeism at work
	-Data preprocessing: It is a group of operations that will convert your raw data into a format that is easier to understand and hence useful for further processing and analysis. This step attempts to fix the problems that can inevitably occur with data gathering. Furthermore, it helps organize your information in a suitable and practical way before you do your analysis and start making predictions.
-Introduction to terms with multiple meaning
	-In mathematics, a variable is a symbol or a letter, that stands for a number we don't know yet (x, y).
	-In data analytics/economics, a variable is a characteristic or a quantity that may change its value over time under different circumstances.
	-In computer programming, a variable acts like a storage location, which contains a certain amount of information.
	-The data science community calls the variables used in a data analytics context features/attributes/inputs.
	-DataFrame in Python is nothing but raw data table in data analytics.
	-Vector/Matrics in mathematics are represented by arrays in Python.
-What is regression analysis?
	-A popular tool in data analytics, machine learning, advanced statistics, and econometrics, is regression analysis.
	-Regression analysis is an equation which on one side has a variable, called a dependent variable, because its value will depend on the values of all variables you see on the other side. The variables on the right side are all independent, or explanatory. Their role is to explain the value of the dependent variable.
	-The dependent variable can also be called a target, while the independent variables can be called predictors. 
	-BA or data scientists, will call the explanatory/independent variables features.
	-Logistic regression is a type of a regression model whose dependent variable is binary. That is, the latter can assume one of two values – 0 or 1, True or False, Yes or No. Therefore, considering the values of all our features, we want to be able to predict whether the dependent variable will take the value of 0 or 1.
-Obtaining dummies from a single feature
	-Quantitative analysis: Add numerical meaning to categorical nominal values.
	-Dummy variables: In regression analysis, a dummy variable is an explanatory binary variable that equals 1 if a certain categorical effect is present and that equals 0 if that same effect is absent. Therefore, we want to have a column where the values of 1 will appear in case an individual has been absent because of reason number 1 and 0 if she was absent because of another reason.
	-pd.get_dummies(df['Reason for Absence']) -> converts categorical variable into dummy variables
-Classifying the various reasons for absence
	-In a regression analysis, the process of reorganizing a certain type of variables into groups is called classification.
-Which methodology to use for particular problem:
	-Logistic Regression
	-Random Forest
	-Neural Network
-Note
	-Whenever we apply sklearn function on pandas DataFrame, result is ndarray.
-Interpreting the coefficients
	-The weights show how we weigh a certain input, the closer they are to the 0, smaller the weight. And alternatively, the further away from 0, no matter if positive or negative, the bigger the weight of this feature.
	-There are a coefficient values and standardized coefficient values. The standardized coefficients are basically the coefficient values of a regression where all variables have been standardized.
	-Whenever we are dealing with a logistic regression, the coefficients we are predicting are the so-called log-odds. This is a consequence of the choice of model. Logistic regressions by default are nothing but a linear function predicting long odds. These log odds are later transformed into Os and 1s.
	-Odds_ratio -> The exponential of these coefficients
	-If a coefficient is around 0 or its Odds_ratio is close to 1, this means, the corresponding feature is not particularly important.
		-The reasoning in terms of weights is that a weight of 0 implies that no matter the feature value, we will multiply it by 0 and the whole result will be 0.
		-The meaning in terms of Odds_ratio: For 1 unit change in the standardized feature, the odds increase by a multiple equal to the odds ratio. So if the odds ratio was 1, then the odds don't change at all.
-Backward elimination
	-The idea is that we can simplify our model by removing all features which have close to no contribution to the model.
	-Usually when we have the p-values of variables, we get rid of all coefficients with p-values > 0.05.
-Note
	-Model: The model refers to the analytical tool applied to solve the business problem.
	-Module: Is a software component containing the code that will help us execute the model.
	
	

##Preprocessing the 'Absenteeism_data' 

# Pandas - allow you to work with panel data
# Pandas possesses various tools for handling data in tabular format (a DataFrame)
import pandas as pd
#Read csv file
raw_csv_data = pd.read_csv('C:/Users/pakale/Downloads/Absenteeism_data.csv')
# You should always make a copy of your initial dataset
# df - the naming convention for DataFrames in Python
df = raw_csv_data.copy()
df
# See all the columns and rows in dataset
pd.options.display.max_columns = None
pd.options.display.max_rows = None
display(df)
#df.info() - prints the concise summary of DataFrame
df.info()
#DataFrame.drop() -> Drop specified rows or columns
df = df.drop(['ID'], axis=1)
#Analyze "Reason for Absence" column
df['Reason for Absence']
df['Reason for Absence'].min()
df['Reason for Absence'].max()
df['Reason for Absence'].unique()
len(df['Reason for Absence'].unique())     #df['Reason for Absence'].nunique()
sorted(df['Reason for Absence'].unique())
df['Reason for Absence'].value_counts()
#Converts categorical variable into dummy variables
reason_columns = pd.get_dummies(df['Reason for Absence'])
reason_columns['check'] = reason_columns.sum(axis=1)
reason_columns['check'].sum(axis=0)
reason_columns['check'].unique()
reason_columns = reason_columns.drop(['check'], axis=1)
#To avoid potential multicollinearity issues in analysis, drop column 0 from reason_columns
reason_columns = pd.get_dummies(df['Reason for Absence'], drop_first=True)
#Group the "Reason for Absence" column as it contains 28 different values (columns) which is too much
df.columns.values
df = df.drop(['Reason for Absence'], axis=1)
reason_columns.columns.values
reason_columns.loc[:,'1':'14']
reason_type_1 = reason_columns.loc[:,1:14].max(axis=1)
reason_type_2 = reason_columns.loc[:,15:17].max(axis=1)
reason_type_3 = reason_columns.loc[:,18:21].max(axis=1)
reason_type_4 = reason_columns.loc[:,22:].max(axis=1)
df = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis=1)
#Renaming the columns
df.columns.values
column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',
       'Daily Work Load Average', 'Body Mass Index', 'Education',
       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4']
df.columns = column_names
df.head()
#Reorder columns in DataFrame
column_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Date', 'Transportation Expense', 
		'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index', 'Education',
        'Children', 'Pets', 'Absenteeism Time in Hours']
df = df[column_names_reordered]
df.head()
#Create a checkpoint - Create a copy of the current state of the DataFrame df
df_reason_mod = df.copy()
#Convert 'Date' column of String type to timestamp type 
df_reason_mod['Date']
type(df_reason_mod['Date'])
type(df_reason_mod['Date'][0])
df_reason_mod['Date'] = pd.to_datetime(df_reason_mod['Date'], format='%d/%m/%Y')
df_reason_mod['Date']
type(df_reason_mod['Date'][0])
#Extract month value (1-12)
df_reason_mod['Date'][0]
df_reason_mod.shape[0]
list_months = []
for i in range(df_reason_mod.shape[0]):
    list_months.append(df_reason_mod['Date'][i].month)
list_months
len(list_months)
df_reason_mod['Month Value'] = list_months
df_reason_mod.head()
#Extract the day of the week(0-6)
df_reason_mod['Date'][699].weekday()
def date_to_weekday(date_value):
    return date_value.weekday()
df_reason_mod['Day of the week'] = df_reason_mod['Date'].apply(date_to_weekday)
df_reason_mod.head()
#Drop 'Date' column and Reorder columns
df_reason_mod = df_reason_mod.drop(['Date'], axis=1)
df_reason_mod.columns.values
column_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4', 'Month Value', 'Day of the week', 
        'Transportation Expense', 'Distance to Work', 'Age', 'Daily Work Load Average', 'Body Mass Index', 
        'Education', 'Children', 'Pets', 'Absenteeism Time in Hours']
df_reason_mod = df_reason_mod[column_names_reordered]
df_reason_date_mod = df_reason_mod.copy()
#Analyze 'Education' column and re-map it
df_reason_date_mod['Education'].unique()
df_reason_date_mod['Education'].value_counts()
df_reason_date_mod['Education'] = df_reason_date_mod['Education'].map({1:0, 2:1, 3:1, 4:1})
df_reason_date_mod['Education'].value_counts()
df_reason_date_mod['Education'].unique()
#Take copy of dataframe and name it to df_preprocessed
df_preprocessed = df_reason_date_mod.copy()
df_preprocessed
#Export dataframe as csv file
df_preprocessed.to_csv('Absenteeism_preprocessed.csv', index=False)


##Applying ML to create the 'Absenteeism_module'

##Logistic Regression

#Import relevant libraries
import pandas as pd
import numpy as np
#Load the data
data_preprocessed = pd.read_csv('Absenteeism_preprocessed.csv')
#Logistic regression is a type of classification.
#The approach we will use here is to create two classes, one representing people who have been excessively absent and another which represents people that haven't.
data_preprocessed['Absenteeism Time in Hours'].median()
targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > 
                   data_preprocessed['Absenteeism Time in Hours'].median(), 1, 0)
data_preprocessed['Excessive Absenteeism'] = targets
targets.sum()/targets.shape[0]
data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours'], axis=1)
data_with_targets is data_preprocessed
#Select inputs for the regression
data_with_targets.shape
unscaled_input = data_with_targets.iloc[:,:-1]
#Standardizing the inputs
from sklearn.preprocessing import StandardScaler
absenteeism_scaler = StandardScaler()
absenteeism_scaler.fit(unscaled_input)
scaled_input = absenteeism_scaler.transform(unscaled_input)
#Splitting the data for training and testing - to avoid overfitting
#Dats shuffling - to remove all types of dependencies that come from the order of the data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_input, targets, train_size=0.8, shuffle=True, random_state=235)
x_train.shape, y_train.shape
x_test.shape, y_test.shape 
#Training the model and checking accuracy
from sklearn.linear_model import LogisticRegression
from sklearn import metrics 
reg = LogisticRegression()
reg.fit(x_train, y_train) #fit the model according to training data
reg.score(x_train, y_train)
#Manually check the accuracy
model_outputs = reg.predict(x_train)
model_outputs == y_train
accuracy = np.sum(model_outputs == y_train) / model_outputs.shape[0]
#Creating summary table with coefficients/weights and intercept/bias
reg.intercept_
reg.coef_
feature_name = unscaled_input.columns.values
summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
summary_table['Coefficients'] = np.transpose(reg.coef_)
summary_table
#Add intercept in summary table - in first row
summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table
#Add Odds_ratio to summary table
summary_table['Odds_ratio'] = np.exp(summary_table.Coefficients)
summary_table.sort_values('Odds_ratio', ascending=False)



#When we standardize the inputs, we also standardize the dummies. This is bad practice because when we standardize, we lose the whole interprete ability of a dummy.
#So we update/correct our code from step "#Standardizing the inputs"

#Standardizing the inputs
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler

class CustomScaler(BaseEstimator,TransformerMixin): 

    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.copy = copy
        self.with_mean = with_mean
        self.with_std = with_std
        self.mean_ = None
        self.var_ = None
    
    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.mean(X[self.columns])
        self.var_ = np.var(X[self.columns])
        return self
    
    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]
		
unscaled_input.columns.values
columns_to_scale =['Month Value', 'Day of the week', 'Transportation Expense', 'Distance to Work',
       'Age', 'Daily Work Load Average', 'Body Mass Index', 'Children', 'Pets'] 
absenteeism_scaler = CustomScaler(columns_to_scale)
absenteeism_scaler.fit(unscaled_input)
scaled_inputs = absenteeism_scaler.transform(unscaled_input)
scaled_inputs
#Splitting the data for training and testing - to avoid overfitting
#Dats shuffling - to remove all types of dependencies that come from the order of the data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_input, targets, train_size=0.8, shuffle=True, random_state=235)
x_train.shape, y_train.shape
x_test.shape, y_test.shape 
#Training the model and checking accuracy
from sklearn.linear_model import LogisticRegression
from sklearn import metrics 
reg = LogisticRegression()
reg.fit(x_train, y_train) #fit the model according to training data
reg.score(x_train, y_train)
#Manually check the accuracy
model_outputs = reg.predict(x_train)
model_outputs == y_train
accuracy = np.sum(model_outputs == y_train) / model_outputs.shape[0]
#Creating summary table with coefficients/weights and intercept/bias
reg.intercept_
reg.coef_
feature_name = unscaled_input.columns.values
summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
summary_table['Coefficients'] = np.transpose(reg.coef_)
summary_table
#Add intercept in summary table - in first row
summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table
#Add Odds_ratio to summary table
summary_table['Odds_ratio'] = np.exp(summary_table.Coefficients)
summary_table.sort_values('Odds_ratio', ascending=False)



#From this we can say that as 'Odds_ratio' for 'Daily Work Load Average', 'Distance to Work', 'Day of the week' are need to 1, they have smallest impact and they barely impact our model
#Backward elimination: eliminate not impacting columns
data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours','Daily Work Load Average', 'Distance to Work', 'Day of the week'], axis=1)
data_with_targets.shape

unscaled_input = data_with_targets.iloc[:,:-1]

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler

class CustomScaler(BaseEstimator,TransformerMixin): 

    def __init__(self,columns,copy=True,with_mean=True,with_std=True):
        self.scaler = StandardScaler(copy,with_mean,with_std)
        self.columns = columns
        self.copy = copy
        self.with_mean = with_mean
        self.with_std = with_std
        self.mean_ = None
        self.var_ = None
    
    def fit(self, X, y=None):
        self.scaler.fit(X[self.columns], y)
        self.mean_ = np.mean(X[self.columns])
        self.var_ = np.var(X[self.columns])
        return self
    
    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)
        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]

unscaled_input.columns.values
columns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']
columns_to_scale = [x for x in unscaled_input.columns.values if x not in columns_to_omit]

absenteeism_scaler = CustomScaler(columns_to_scale)
absenteeism_scaler.fit(unscaled_input)
scaled_input = absenteeism_scaler.transform(unscaled_input)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_input, targets, train_size=0.8, shuffle=True, random_state=235)

from sklearn.linear_model import LogisticRegression
from sklearn import metrics 
reg = LogisticRegression()
reg.fit(x_train, y_train) #fit the model according to training data
reg.score(x_train, y_train)

model_outputs = reg.predict(x_train)
accuracy = np.sum(model_outputs == y_train) / model_outputs.shape[0]

feature_name = unscaled_input.columns.values
summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
summary_table['Coefficients'] = np.transpose(reg.coef_)
summary_table

summary_table.index = summary_table.index + 1
summary_table.loc[0] = ['Intercept', reg.intercept_[0]]
summary_table = summary_table.sort_index()
summary_table

summary_table['Odds_ratio'] = np.exp(summary_table.Coefficients)
summary_table.sort_values('Odds_ratio', ascending=False)


#Testing the model we created
reg.score(x_test,y_test)
#sklearn.linear_model.LogisticRegression.predict_proba(x) -> returns the probability estimates for all possible outputs (classes)
predicted_proba = reg.predict_proba(x_test)
predicted_proba
#1st column show probability of out being 0 and 2nd column show probability of out being 1
probability_of_excessive_absenteeism = predicted_proba[:,1]
probability_of_excessive_absenteeism

#Save the model and preparing it for deployment
#There are several popular ways to save (and finalize) a model. To name some, you can use Joblib (a part of the SciPy ecosystem), and JSON.
#pickle[module] -> is a Python module used to convert a Python object into a character stream (string of characters)
import pickle
with open('model', 'wb') as file:
    pickle.dump(reg, file)
with open('scaler', 'wb') as file:
    pickle.dump(absenteeism_scaler, file)


#Load and use absenteeism module
from absenteeism_module import *
model = absenteeism_model('model','scaler')

#pd.read_csv('Absenteeism_new_data.csv')
model.load_and_clean_data('Absenteeism_new_data.csv')

model.predicted_outputs()
#Exporting predicted result to a csv file
model.predicted_outputs().to_csv('Absenteeism_predictions.csv', index = False)





-Appendix - Additional Python Tools

-Using the .format() method
	-.format() method is applicable to string values only
		time_horizone = 1, 3, 13
		products = ['Product A', 'Product B']
		'Expected sales for a period of {} month(s) for product(s) {}:'.format(time_horizone[2], products[1])
		'Expected sales for a period of {0} month(s) for product(s) {1}:'.format(time_horizone[2], products[1])
		'Expected sales for a period of {t_hor} month(s) for product(s) {prod}:'.format(t_hor = time_horizone[2], prod = products[1])
		'Expected sales for a period of {t_hor} month(s) for product(s) {prod[1]}:'.format(t_hor = 12, prod = ['Product A', 'Product B'])
		'Expected sales for a period of {t_hor} month(s) for product(s) {prod[0]}: ${sales}'.format(t_hor = time_horizone[2], prod = products, sales = 1000)
		'Expected sales for a period of {0} month(s) for product(s) {prod[0]}: ${sales}'.format(time_horizone[2], prod = products, sales = 1000)
-Iterating over Range objects
	tuple = (4, 5, 8, 7)
	list = [10.5, 20.4, 30.6]
	s = 'abcde'
	for i in tuple:
		print(i, end=" ")
	for i in list:
		print(i, end=" ")
	for i in s:
		print(i, end=" ")
	for i in range(5):
		print(i, end=" ")
-Introduction to nested for loop
	-for i in ['Product A', 'Product B']:
		for j in range(5):
			print([i, j])
	-products = ['Product A', 'Product B']
	time_horizones = (1, 3, 12)
	prod_sales = [1000, 1200, 1400, 1500]
	for prod in products:
		for t_hor in time_horizones:
			for sale in prod_sales:
				print('Expected sales for a period of {0} month(s) for product(s) {1}: ${sales}'.format(t_hor, prod, sales=t_hor*sale))
-List comprehensions
	-numbers = [1, 13, 4, 5, 63, 100]
	 new_numbers = [n * 2 for n in numbers]
	 new_numbers
	-list_comprehension_1 = [i+j for i in range(2) for j in range(5)]
	-list_comprehension_2 = [[i+j for i in range(2)] for j in range(5)]
	-list(range(1,11))
	-list_comprehension_3 = [num ** 3 for num in range(1, 11) if num % 2 != 0]
	-[num ** 3 if num % 2 != 0 else 'even' for num in range(1, 11)]
	-List comprehensions are a very powerful tool because it can be applied to a very wide range of cases and can deliver many types of output.
	-List Comprehensions are a fantastic example of high quality code. They make writing Python a lot more intuitive and resemble spoken English to a great extent.
	-While programming, you will always be faced with the tradeoff between writing efficient code versus code that will perform better in terms of speed and precision of delivery. On average, list comprehensions, require more memory and run more slowly. Therefore, while aiming to optimize your code, given these two constraints, in some situations, you might be better off choosing among other alternatives, such as nested loops.
-Anonymous (lambda) functions
	-Normal function
		def raise_to_the_power_of_2(x):
			return x ** 2
		raise_to_the_power_of_2(4)
	-raise_to_the_power_of_2_lambda = lambda x: x ** 2
	 raise_to_the_power_of_2_lambda(4)
	-(lambda x: x/2) (11)
	-(lambda x: (2 + 5 * x ** 4) ** 2 / (x + 3) ** 3) (2)
	-sum_xy = lambda x, y: x + y
	 sum_xy(3,5)
	-sum_xy = lambda x, y: x + y(x)
	 sum_xy(3, lambda x: x ** 3)
	-Lambda functions can have one or many parameters, but can contain a single expression only.
	-Lambda functions allow you to write just one line of code to include a simple functionality in a more complex expression.
	-Unlike regular functions, lambda functions can only be applied to the larger expression they have been written in. They cannot be referenced elsewhere in your work.




-Appendix - pandas fundamentals

-Introduction to pandas series
	-import pandas as pd
	-The series object is basically a single-column data or a set of values that correspond to a single variable.
	-Convert list to series
		-products = ['A','B','C','D']
		 product_categories = pd.Series(products)
		 type(product_categories)
	-A Series object stores an index, the data and the type of the data.
	-The Pandas Series object corresponds to the one-dimensional NumPy array structure.
		-import numpy as np
		 array_a = np.array([10, 20, 30, 40])
		 type(array_a)
		 series_a = pd.Series([10, 20, 30, 40])
		 type(series_a)
	-The Pandas series object is something like a powerful version of the Python list or an enhanced version of the numpy array.
-Working with methods in Python
	-Function and Method are very similar because when provided with some initial data, they can make specific operations with it and return an output.
	-However, a function is an independent entity, in the sense that, it is not associated with an object by construction. On the contrary, a method from a given package is generally applied to an object of a certain class. More precisely, when called or invoked, a method can have access to the objects data and can also manipulate the object's state. For this reason, since we can't use a method unless there's an object to associate it with, different libraries contain their own sets of methods. Thus, they can be applied to the types of objects associated with these libraries only.
	-For instance, there is a certain group of methods and pandas that are specific to the series object. They can't be used on data frames or objects belonging to other packages
	-start_date_deposites = pd.Series({
                '7/4/2014' : 2000,
                '10/28/2017' : 1000,
                '4/19/2013' : 4000,
                '3/20/2019' : 3000,
	 })
	 start_date_deposites.sum()
	 start_date_deposites.min()
	 start_date_deposites.max()
	 start_date_deposites.idxmin()
	 start_date_deposites.idxmax()
	 start_date_deposites.head()
	 start_date_deposites.tail()
-Parameters and arguments in Pandas
	-start_date_deposites.head(3)
	-In programming terms, the option to set the number of rows is called a parameter of the .head(3) method. Our choice of that number in a given situation is referred to as an argument.
	-A parameter of a python method or function always has a name.
	-The benefit of using parameter names explicitly and in the right order is to inform the reader exactly what parameters you're providing the arguments for. Thus, we can avoid any ambiguity in what the supplied arguments are supposed to be used for.
-Using .unique() and .nunique() method
	-Read data from csv: location_data = pd.read_csv('location.csv', squeeze=True)
	-location_data.describe()
	-len(location_data)
	-location_data.nunique()
	-type(location_data.nunique())
	-location_data.unique()
	-type(location_data.unique())
-Using  .sort_values()
	-numbers = pd.Series([15, 1000, 74, 44, 300])
	 numbers.sort_values()
	 numbers.sort_values(ascending=False)
-Introduction to Pandas DataFrames
	-Series: Single column data
		-Corresponds to single variable. 
	-DataFrame: Multi-column data
		-Each column represents a different variable of different type. So information is potentially heterogeneous.
		-We aim for having information of same type within a certain column.
	-The DataFrame is nothing but a collection of multiple series objects. This means that as a structure, the series corresponds to a single column of a data frame.
	-You can refer to the series object as a 1-D data structure because it contains values along a single axis representing a certain number of rows. The DataFrame instead is a 2-D structure in the sense that its data has been organized not only in the rows but also in the columns. Thus, it represents a tabular structure and is the closest Python analogue we can have to a standard 2-D dataset, so to speak, or a spreadsheet.
	-A data frame can be regarded as a collection of series in the same way in which an Excel spreadsheet is a collection of multiple columns. That's why a DataFrame can both row and column labels.
	-Example DataFrame methods/functions
		array_a = np.array([[3,2,1],[6,4,5]])
		pd.DataFrame(array_a)
		pd.DataFrame(array_a, columns=['Column 1', 'Column 2', 'Column 3'])
		df = pd.DataFrame(array_a, columns=['Column 1', 'Column 2', 'Column 3'], index=['Row 1', 'Row 2'])
		df.index
		df.columns
		df.axes
		df.dtypes
		df.values
		df.to_numpy()
		df.shape
-Data selection in pandas DataFrame
	-Data selection or subset selection in the pandas DataFrame means extracting elements, rows, columns or subsets from such an object. Data selection allows us to work on just a portion of a dataset.
	-Python is case-sensitive!
	-Example:
		data = pd.read_csv('leading-company.csv', index_col='StringID')
		leading_co_data = data.copy()
		leading_co_data.Product
		leading_co_data.Location
		leading_co_data['Product']
		leading_co_data['Location']
		type(leading_co_data['Location']) #Series
		leading_co_data[['Location']]
		type(leading_co_data[['Location']]) #DataFrame - double []
		leading_co_data[['Location','Product']].head()
-pandas DataFrames - Indexing with .iloc[]
	-DataFrame.iloc[] -> Purely integer-location based indexing for selection by position.
	-Example:
		data = pd.read_csv('leading-company.csv', index_col='StringID')
		leading_co_data = data.copy()
		leading_co_data.iloc[1]  -> return 2nd row
		leading_co_data.iloc[1, 3] -> return 4rth column of 2nd row
		leading_co_data.iloc[1,:] -> return all columns of 1st row
		leading_co_data.iloc[:,3] -> return all rows of 4rth column
		leading_co_data.iloc[[1,3],:] -> return 2nd and 3rd row
		leading_co_data.iloc[:,[1,3]] -> return 2nd and 3rd columns
-pandas DataFrames - Indexing with .loc[]
	-DataFrame.loc[] -> Allows us to sub-select information from a DataFrame by referring to its index labels.
	-Example:
		data = pd.read_csv('leading-company.csv', index_col='StringID')
		leading_co_data = data.copy()
		leading_co_data.loc['LoanID_3']
		leading_co_data.loc['LoanID_3', :]
		leading_co_data.loc['LoanID_3', 'Region']
		leading_co_data.loc[:,'Location']
		
	
	
















